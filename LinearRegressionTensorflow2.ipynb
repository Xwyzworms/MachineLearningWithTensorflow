{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "particular-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.2\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "present-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =0.001\n",
    "steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "arranged-grave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "X = np.random.randn(100)\n",
    "Y = X + np.random.randn(100) * 0.9\n",
    "plt.plot(X,Y,\"o\")\n",
    "\n",
    "weight = tf.Variable(np.random.randn(),name=\"weight\")\n",
    "bias = tf.Variable(np.random.randn(),name=\"bias\")\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "def lin_reg(data):\n",
    "    \"\"\" calculate Lin Reg\"\"\"\n",
    "    return tf.convert_to_tensor(weight * data + bias,dtype=tf.float32)\n",
    "\n",
    "def cost_mean_square(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the mean squared error\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean( tf.square(y_true - y_pred ))\n",
    "\n",
    "def run_optimizer():\n",
    "    \"\"\"\n",
    "    Apply the Optimizer and do backward propagation\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as fufTape:\n",
    "        y_pred = lin_reg(X)\n",
    "        loss = cost_mean_square(Y,y_pred)\n",
    "    gradients = fufTape.gradient(loss,[weight,bias])\n",
    "    optimizer.apply_gradients(zip(gradients,[weight,bias]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "norman-russell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 0 , Loss : 2.7272489070892334, Weights 0.1347002238035202, bias : 1.2006006240844727  \n",
      "Steps : 1 , Loss : 2.7202446460723877, Weights 0.13626034557819366, bias : 1.1984614133834839  \n",
      "Steps : 2 , Loss : 2.713266372680664, Weights 0.1378178745508194, bias : 1.1963262557983398  \n",
      "Steps : 3 , Loss : 2.706313133239746, Weights 0.1393728107213974, bias : 1.1941951513290405  \n",
      "Steps : 4 , Loss : 2.6993846893310547, Weights 0.14092515408992767, bias : 1.1920679807662964  \n",
      "Steps : 5 , Loss : 2.692481756210327, Weights 0.14247490465641022, bias : 1.189944863319397  \n",
      "Steps : 6 , Loss : 2.685603380203247, Weights 0.14402207732200623, bias : 1.1878256797790527  \n",
      "Steps : 7 , Loss : 2.6787500381469727, Weights 0.1455666720867157, bias : 1.1857105493545532  \n",
      "Steps : 8 , Loss : 2.671921491622925, Weights 0.14710870385169983, bias : 1.1835993528366089  \n",
      "Steps : 9 , Loss : 2.6651175022125244, Weights 0.14864815771579742, bias : 1.1814922094345093  \n",
      "Steps : 10 , Loss : 2.6583380699157715, Weights 0.15018504858016968, bias : 1.1793889999389648  \n",
      "Steps : 11 , Loss : 2.651582956314087, Weights 0.1517193764448166, bias : 1.1772897243499756  \n",
      "Steps : 12 , Loss : 2.64485239982605, Weights 0.15325115621089935, bias : 1.1751943826675415  \n",
      "Steps : 13 , Loss : 2.63814640045166, Weights 0.15478038787841797, bias : 1.1731030941009521  \n",
      "Steps : 14 , Loss : 2.6314642429351807, Weights 0.15630707144737244, bias : 1.171015739440918  \n",
      "Steps : 15 , Loss : 2.6248068809509277, Weights 0.15783120691776276, bias : 1.168932318687439  \n",
      "Steps : 16 , Loss : 2.618173122406006, Weights 0.15935280919075012, bias : 1.1668528318405151  \n",
      "Steps : 17 , Loss : 2.611563205718994, Weights 0.16087187826633453, bias : 1.164777159690857  \n",
      "Steps : 18 , Loss : 2.6049773693084717, Weights 0.162388414144516, bias : 1.162705421447754  \n",
      "Steps : 19 , Loss : 2.598414897918701, Weights 0.1639024168252945, bias : 1.160637617111206  \n",
      "Steps : 20 , Loss : 2.591876745223999, Weights 0.16541390120983124, bias : 1.1585737466812134  \n",
      "Steps : 21 , Loss : 2.585361957550049, Weights 0.16692286729812622, bias : 1.1565136909484863  \n",
      "Steps : 22 , Loss : 2.5788707733154297, Weights 0.16842931509017944, bias : 1.1544575691223145  \n",
      "Steps : 23 , Loss : 2.5724029541015625, Weights 0.1699332594871521, bias : 1.1524052619934082  \n",
      "Steps : 24 , Loss : 2.565958261489868, Weights 0.1714347004890442, bias : 1.1503568887710571  \n",
      "Steps : 25 , Loss : 2.559536933898926, Weights 0.1729336380958557, bias : 1.1483123302459717  \n",
      "Steps : 26 , Loss : 2.5531394481658936, Weights 0.17443007230758667, bias : 1.1462715864181519  \n",
      "Steps : 27 , Loss : 2.5467641353607178, Weights 0.17592401802539825, bias : 1.1442347764968872  \n",
      "Steps : 28 , Loss : 2.540412187576294, Weights 0.17741547524929047, bias : 1.1422017812728882  \n",
      "Steps : 29 , Loss : 2.534083366394043, Weights 0.1789044439792633, bias : 1.1401726007461548  \n",
      "Steps : 30 , Loss : 2.527777671813965, Weights 0.18039092421531677, bias : 1.138147234916687  \n",
      "Steps : 31 , Loss : 2.5214943885803223, Weights 0.18187493085861206, bias : 1.1361256837844849  \n",
      "Steps : 32 , Loss : 2.5152337551116943, Weights 0.18335646390914917, bias : 1.1341079473495483  \n",
      "Steps : 33 , Loss : 2.508995771408081, Weights 0.1848355382680893, bias : 1.1320940256118774  \n",
      "Steps : 34 , Loss : 2.5027801990509033, Weights 0.18631213903427124, bias : 1.1300839185714722  \n",
      "Steps : 35 , Loss : 2.4965875148773193, Weights 0.1877862811088562, bias : 1.128077507019043  \n",
      "Steps : 36 , Loss : 2.4904165267944336, Weights 0.18925796449184418, bias : 1.1260749101638794  \n",
      "Steps : 37 , Loss : 2.4842684268951416, Weights 0.19072718918323517, bias : 1.1240761280059814  \n",
      "Steps : 38 , Loss : 2.478142023086548, Weights 0.19219397008419037, bias : 1.1220810413360596  \n",
      "Steps : 39 , Loss : 2.4720380306243896, Weights 0.19365830719470978, bias : 1.1200897693634033  \n",
      "Steps : 40 , Loss : 2.465956211090088, Weights 0.1951202005147934, bias : 1.1181021928787231  \n",
      "Steps : 41 , Loss : 2.4598960876464844, Weights 0.19657966494560242, bias : 1.1161184310913086  \n",
      "Steps : 42 , Loss : 2.4538581371307373, Weights 0.19803668558597565, bias : 1.1141383647918701  \n",
      "Steps : 43 , Loss : 2.4478418827056885, Weights 0.19949127733707428, bias : 1.1121619939804077  \n",
      "Steps : 44 , Loss : 2.4418468475341797, Weights 0.2009434551000595, bias : 1.1101893186569214  \n",
      "Steps : 45 , Loss : 2.4358739852905273, Weights 0.20239320397377014, bias : 1.1082204580307007  \n",
      "Steps : 46 , Loss : 2.429922580718994, Weights 0.20384053885936737, bias : 1.106255292892456  \n",
      "Steps : 47 , Loss : 2.423992872238159, Weights 0.2052854597568512, bias : 1.1042938232421875  \n",
      "Steps : 48 , Loss : 2.4180843830108643, Weights 0.20672796666622162, bias : 1.102336049079895  \n",
      "Steps : 49 , Loss : 2.4121973514556885, Weights 0.20816807448863983, bias : 1.1003819704055786  \n",
      "Steps : 50 , Loss : 2.4063315391540527, Weights 0.20960578322410583, bias : 1.0984315872192383  \n",
      "Steps : 51 , Loss : 2.400486707687378, Weights 0.21104109287261963, bias : 1.0964847803115845  \n",
      "Steps : 52 , Loss : 2.394663095474243, Weights 0.2124740034341812, bias : 1.0945416688919067  \n",
      "Steps : 53 , Loss : 2.3888602256774902, Weights 0.21390452980995178, bias : 1.092602252960205  \n",
      "Steps : 54 , Loss : 2.3830788135528564, Weights 0.21533267199993134, bias : 1.09066641330719  \n",
      "Steps : 55 , Loss : 2.3773179054260254, Weights 0.21675843000411987, bias : 1.0887342691421509  \n",
      "Steps : 56 , Loss : 2.371577739715576, Weights 0.2181818038225174, bias : 1.0868057012557983  \n",
      "Steps : 57 , Loss : 2.365858554840088, Weights 0.2196028083562851, bias : 1.0848808288574219  \n",
      "Steps : 58 , Loss : 2.3601598739624023, Weights 0.22102144360542297, bias : 1.082959532737732  \n",
      "Steps : 59 , Loss : 2.3544819355010986, Weights 0.22243770956993103, bias : 1.081041932106018  \n",
      "Steps : 60 , Loss : 2.3488245010375977, Weights 0.22385162115097046, bias : 1.0791279077529907  \n",
      "Steps : 61 , Loss : 2.3431873321533203, Weights 0.22526316344738007, bias : 1.07721745967865  \n",
      "Steps : 62 , Loss : 2.3375704288482666, Weights 0.22667235136032104, bias : 1.0753105878829956  \n",
      "Steps : 63 , Loss : 2.3319740295410156, Weights 0.2280791997909546, bias : 1.0734072923660278  \n",
      "Steps : 64 , Loss : 2.3263978958129883, Weights 0.2294836938381195, bias : 1.0715076923370361  \n",
      "Steps : 65 , Loss : 2.3208420276641846, Weights 0.230885848402977, bias : 1.069611668586731  \n",
      "Steps : 66 , Loss : 2.315305709838867, Weights 0.23228566348552704, bias : 1.0677191019058228  \n",
      "Steps : 67 , Loss : 2.3097896575927734, Weights 0.23368313908576965, bias : 1.065830111503601  \n",
      "Steps : 68 , Loss : 2.304293394088745, Weights 0.23507829010486603, bias : 1.063944697380066  \n",
      "Steps : 69 , Loss : 2.2988169193267822, Weights 0.23647111654281616, bias : 1.0620628595352173  \n",
      "Steps : 70 , Loss : 2.2933602333068848, Weights 0.23786161839962006, bias : 1.0601845979690552  \n",
      "Steps : 71 , Loss : 2.2879233360290527, Weights 0.2392497956752777, bias : 1.05830979347229  \n",
      "Steps : 72 , Loss : 2.282506227493286, Weights 0.24063566327095032, bias : 1.0564385652542114  \n",
      "Steps : 73 , Loss : 2.2771084308624268, Weights 0.24201920628547668, bias : 1.0545709133148193  \n",
      "Steps : 74 , Loss : 2.271730422973633, Weights 0.2434004545211792, bias : 1.0527067184448242  \n",
      "Steps : 75 , Loss : 2.266371726989746, Weights 0.24477939307689667, bias : 1.050845980644226  \n",
      "Steps : 76 , Loss : 2.2610323429107666, Weights 0.2461560219526291, bias : 1.0489888191223145  \n",
      "Steps : 77 , Loss : 2.2557120323181152, Weights 0.24753035604953766, bias : 1.0471351146697998  \n",
      "Steps : 78 , Loss : 2.250411033630371, Weights 0.24890239536762238, bias : 1.0452848672866821  \n",
      "Steps : 79 , Loss : 2.245129346847534, Weights 0.25027215480804443, bias : 1.043438196182251  \n",
      "Steps : 80 , Loss : 2.2398667335510254, Weights 0.25163963437080383, bias : 1.0415949821472168  \n",
      "Steps : 81 , Loss : 2.2346231937408447, Weights 0.2530048191547394, bias : 1.0397552251815796  \n",
      "Steps : 82 , Loss : 2.229398250579834, Weights 0.25436773896217346, bias : 1.0379189252853394  \n",
      "Steps : 83 , Loss : 2.2241921424865723, Weights 0.2557283639907837, bias : 1.036086082458496  \n",
      "Steps : 84 , Loss : 2.2190051078796387, Weights 0.25708672404289246, bias : 1.0342566967010498  \n",
      "Steps : 85 , Loss : 2.213836431503296, Weights 0.25844281911849976, bias : 1.0324307680130005  \n",
      "Steps : 86 , Loss : 2.2086870670318604, Weights 0.2597966492176056, bias : 1.0306082963943481  \n",
      "Steps : 87 , Loss : 2.2035558223724365, Weights 0.26114821434020996, bias : 1.0287891626358032  \n",
      "Steps : 88 , Loss : 2.1984429359436035, Weights 0.26249754428863525, bias : 1.0269734859466553  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 89 , Loss : 2.1933486461639404, Weights 0.2638446092605591, bias : 1.0251612663269043  \n",
      "Steps : 90 , Loss : 2.1882729530334473, Weights 0.26518943905830383, bias : 1.0233523845672607  \n",
      "Steps : 91 , Loss : 2.183215379714966, Weights 0.2665320038795471, bias : 1.0215469598770142  \n",
      "Steps : 92 , Loss : 2.178175926208496, Weights 0.26787233352661133, bias : 1.019744873046875  \n",
      "Steps : 93 , Loss : 2.173154830932617, Weights 0.26921042799949646, bias : 1.0179462432861328  \n",
      "Steps : 94 , Loss : 2.16815185546875, Weights 0.2705462872982025, bias : 1.016150951385498  \n",
      "Steps : 95 , Loss : 2.1631669998168945, Weights 0.2718799114227295, bias : 1.0143591165542603  \n",
      "Steps : 96 , Loss : 2.158200263977051, Weights 0.2732113003730774, bias : 1.0125706195831299  \n",
      "Steps : 97 , Loss : 2.1532514095306396, Weights 0.2745404839515686, bias : 1.010785460472107  \n",
      "Steps : 98 , Loss : 2.148319959640503, Weights 0.27586743235588074, bias : 1.0090036392211914  \n",
      "Steps : 99 , Loss : 2.143406629562378, Weights 0.2771921753883362, bias : 1.0072251558303833  \n",
      "Steps : 100 , Loss : 2.1385107040405273, Weights 0.27851471304893494, bias : 1.0054500102996826  \n",
      "Steps : 101 , Loss : 2.1336328983306885, Weights 0.279835045337677, bias : 1.0036782026290894  \n",
      "Steps : 102 , Loss : 2.128772497177124, Weights 0.2811531722545624, bias : 1.0019097328186035  \n",
      "Steps : 103 , Loss : 2.123929262161255, Weights 0.28246909379959106, bias : 1.000144600868225  \n",
      "Steps : 104 , Loss : 2.1191043853759766, Weights 0.28378280997276306, bias : 0.9983828067779541  \n",
      "Steps : 105 , Loss : 2.1142961978912354, Weights 0.28509435057640076, bias : 0.9966243505477905  \n",
      "Steps : 106 , Loss : 2.1095056533813477, Weights 0.28640368580818176, bias : 0.9948692321777344  \n",
      "Steps : 107 , Loss : 2.1047325134277344, Weights 0.28771084547042847, bias : 0.9931173920631409  \n",
      "Steps : 108 , Loss : 2.0999763011932373, Weights 0.2890157997608185, bias : 0.99136883020401  \n",
      "Steps : 109 , Loss : 2.0952374935150146, Weights 0.2903185784816742, bias : 0.9896235466003418  \n",
      "Steps : 110 , Loss : 2.090515613555908, Weights 0.2916191816329956, bias : 0.987881600856781  \n",
      "Steps : 111 , Loss : 2.085810899734497, Weights 0.2929176092147827, bias : 0.9861429333686829  \n",
      "Steps : 112 , Loss : 2.0811233520507812, Weights 0.2942138910293579, bias : 0.9844074845314026  \n",
      "Steps : 113 , Loss : 2.0764522552490234, Weights 0.2955079972743988, bias : 0.982675313949585  \n",
      "Steps : 114 , Loss : 2.071798324584961, Weights 0.2967999279499054, bias : 0.98094642162323  \n",
      "Steps : 115 , Loss : 2.0671608448028564, Weights 0.2980897128582001, bias : 0.9792207479476929  \n",
      "Steps : 116 , Loss : 2.0625405311584473, Weights 0.29937735199928284, bias : 0.9774983525276184  \n",
      "Steps : 117 , Loss : 2.057936668395996, Weights 0.3006628453731537, bias : 0.9757791757583618  \n",
      "Steps : 118 , Loss : 2.053349494934082, Weights 0.3019461929798126, bias : 0.9740632772445679  \n",
      "Steps : 119 , Loss : 2.048779010772705, Weights 0.30322739481925964, bias : 0.9723505973815918  \n",
      "Steps : 120 , Loss : 2.044224500656128, Weights 0.30450645089149475, bias : 0.9706411361694336  \n",
      "Steps : 121 , Loss : 2.039687156677246, Weights 0.30578336119651794, bias : 0.9689348936080933  \n",
      "Steps : 122 , Loss : 2.035165786743164, Weights 0.3070581257343292, bias : 0.9672318696975708  \n",
      "Steps : 123 , Loss : 2.030660629272461, Weights 0.308330774307251, bias : 0.9655320048332214  \n",
      "Steps : 124 , Loss : 2.026171922683716, Weights 0.3096013069152832, bias : 0.9638353586196899  \n",
      "Steps : 125 , Loss : 2.0216991901397705, Weights 0.3108696937561035, bias : 0.9621418714523315  \n",
      "Steps : 126 , Loss : 2.017242908477783, Weights 0.3121359646320343, bias : 0.960451602935791  \n",
      "Steps : 127 , Loss : 2.0128026008605957, Weights 0.31340011954307556, bias : 0.9587644934654236  \n",
      "Steps : 128 , Loss : 2.008378028869629, Weights 0.3146621584892273, bias : 0.957080602645874  \n",
      "Steps : 129 , Loss : 2.00396990776062, Weights 0.3159221112728119, bias : 0.9553998708724976  \n",
      "Steps : 130 , Loss : 1.999577283859253, Weights 0.31717994809150696, bias : 0.9537222981452942  \n",
      "Steps : 131 , Loss : 1.9952006340026855, Weights 0.3184356987476349, bias : 0.9520478844642639  \n",
      "Steps : 132 , Loss : 1.9908398389816284, Weights 0.3196893334388733, bias : 0.950376570224762  \n",
      "Steps : 133 , Loss : 1.9864944219589233, Weights 0.32094088196754456, bias : 0.9487084150314331  \n",
      "Steps : 134 , Loss : 1.9821652173995972, Weights 0.3221903443336487, bias : 0.9470434188842773  \n",
      "Steps : 135 , Loss : 1.9778512716293335, Weights 0.32343772053718567, bias : 0.9453815221786499  \n",
      "Steps : 136 , Loss : 1.9735528230667114, Weights 0.3246830105781555, bias : 0.9437227845191956  \n",
      "Steps : 137 , Loss : 1.9692699909210205, Weights 0.3259262144565582, bias : 0.9420671463012695  \n",
      "Steps : 138 , Loss : 1.9650027751922607, Weights 0.3271673321723938, bias : 0.9404146075248718  \n",
      "Steps : 139 , Loss : 1.9607510566711426, Weights 0.32840636372566223, bias : 0.9387652277946472  \n",
      "Steps : 140 , Loss : 1.956514596939087, Weights 0.3296433389186859, bias : 0.9371189475059509  \n",
      "Steps : 141 , Loss : 1.952293038368225, Weights 0.33087825775146484, bias : 0.935475766658783  \n",
      "Steps : 142 , Loss : 1.948087215423584, Weights 0.33211109042167664, bias : 0.9338356256484985  \n",
      "Steps : 143 , Loss : 1.9438958168029785, Weights 0.3333418667316437, bias : 0.9321985840797424  \n",
      "Steps : 144 , Loss : 1.9397201538085938, Weights 0.33457058668136597, bias : 0.9305646419525146  \n",
      "Steps : 145 , Loss : 1.9355593919754028, Weights 0.3357972502708435, bias : 0.9289337396621704  \n",
      "Steps : 146 , Loss : 1.9314138889312744, Weights 0.3370218575000763, bias : 0.9273059368133545  \n",
      "Steps : 147 , Loss : 1.9272830486297607, Weights 0.3382444381713867, bias : 0.9256811738014221  \n",
      "Steps : 148 , Loss : 1.923167109489441, Weights 0.3394649624824524, bias : 0.9240594506263733  \n",
      "Steps : 149 , Loss : 1.9190661907196045, Weights 0.3406834602355957, bias : 0.9224408268928528  \n",
      "Steps : 150 , Loss : 1.9149798154830933, Weights 0.34189990162849426, bias : 0.9208252429962158  \n",
      "Steps : 151 , Loss : 1.9109083414077759, Weights 0.34311431646347046, bias : 0.9192126989364624  \n",
      "Steps : 152 , Loss : 1.9068515300750732, Weights 0.3443267047405243, bias : 0.9176031351089478  \n",
      "Steps : 153 , Loss : 1.902809500694275, Weights 0.34553706645965576, bias : 0.9159966111183167  \n",
      "Steps : 154 , Loss : 1.8987817764282227, Weights 0.34674540162086487, bias : 0.9143931269645691  \n",
      "Steps : 155 , Loss : 1.8947687149047852, Weights 0.3479517102241516, bias : 0.9127926826477051  \n",
      "Steps : 156 , Loss : 1.8907703161239624, Weights 0.349155992269516, bias : 0.9111952185630798  \n",
      "Steps : 157 , Loss : 1.886785864830017, Weights 0.3503582775592804, bias : 0.9096007347106934  \n",
      "Steps : 158 , Loss : 1.882816195487976, Weights 0.35155853629112244, bias : 0.9080092906951904  \n",
      "Steps : 159 , Loss : 1.878860592842102, Weights 0.3527567982673645, bias : 0.9064208269119263  \n",
      "Steps : 160 , Loss : 1.8749194145202637, Weights 0.3539530634880066, bias : 0.9048353433609009  \n",
      "Steps : 161 , Loss : 1.8709923028945923, Weights 0.3551473319530487, bias : 0.9032528400421143  \n",
      "Steps : 162 , Loss : 1.867079496383667, Weights 0.35633960366249084, bias : 0.9016733169555664  \n",
      "Steps : 163 , Loss : 1.8631808757781982, Weights 0.357529878616333, bias : 0.9000967741012573  \n",
      "Steps : 164 , Loss : 1.8592960834503174, Weights 0.3587181568145752, bias : 0.8985231518745422  \n",
      "Steps : 165 , Loss : 1.8554253578186035, Weights 0.3599044382572174, bias : 0.8969525098800659  \n",
      "Steps : 166 , Loss : 1.851568579673767, Weights 0.36108875274658203, bias : 0.8953848481178284  \n",
      "Steps : 167 , Loss : 1.8477258682250977, Weights 0.3622710704803467, bias : 0.8938201069831848  \n",
      "Steps : 168 , Loss : 1.843896746635437, Weights 0.36345142126083374, bias : 0.8922582864761353  \n",
      "Steps : 169 , Loss : 1.840081810951233, Weights 0.3646298050880432, bias : 0.8906994462013245  \n",
      "Steps : 170 , Loss : 1.8362802267074585, Weights 0.3658061921596527, bias : 0.8891435265541077  \n",
      "Steps : 171 , Loss : 1.832492709159851, Weights 0.3669806122779846, bias : 0.8875905275344849  \n",
      "Steps : 172 , Loss : 1.8287185430526733, Weights 0.36815309524536133, bias : 0.886040449142456  \n",
      "Steps : 173 , Loss : 1.8249582052230835, Weights 0.36932361125946045, bias : 0.8844932913780212  \n",
      "Steps : 174 , Loss : 1.8212112188339233, Weights 0.370492160320282, bias : 0.8829490542411804  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 175 , Loss : 1.817478060722351, Weights 0.3716587424278259, bias : 0.8814076781272888  \n",
      "Steps : 176 , Loss : 1.813758134841919, Weights 0.3728233873844147, bias : 0.8798692226409912  \n",
      "Steps : 177 , Loss : 1.8100515604019165, Weights 0.37398606538772583, bias : 0.8783336281776428  \n",
      "Steps : 178 , Loss : 1.8063586950302124, Weights 0.3751468062400818, bias : 0.8768009543418884  \n",
      "Steps : 179 , Loss : 1.8026788234710693, Weights 0.37630560994148254, bias : 0.8752711415290833  \n",
      "Steps : 180 , Loss : 1.799012303352356, Weights 0.3774624764919281, bias : 0.8737441897392273  \n",
      "Steps : 181 , Loss : 1.7953590154647827, Weights 0.37861740589141846, bias : 0.8722201585769653  \n",
      "Steps : 182 , Loss : 1.7917187213897705, Weights 0.3797703981399536, bias : 0.8706989884376526  \n",
      "Steps : 183 , Loss : 1.788091778755188, Weights 0.38092145323753357, bias : 0.8691806793212891  \n",
      "Steps : 184 , Loss : 1.784477949142456, Weights 0.3820706009864807, bias : 0.8676652312278748  \n",
      "Steps : 185 , Loss : 1.7808771133422852, Weights 0.38321781158447266, bias : 0.8661525845527649  \n",
      "Steps : 186 , Loss : 1.7772892713546753, Weights 0.3843631148338318, bias : 0.8646427989006042  \n",
      "Steps : 187 , Loss : 1.7737139463424683, Weights 0.3855065107345581, bias : 0.8631358742713928  \n",
      "Steps : 188 , Loss : 1.770151972770691, Weights 0.3866479694843292, bias : 0.8616317510604858  \n",
      "Steps : 189 , Loss : 1.7666027545928955, Weights 0.38778752088546753, bias : 0.8601304888725281  \n",
      "Steps : 190 , Loss : 1.7630664110183716, Weights 0.388925164937973, bias : 0.8586320281028748  \n",
      "Steps : 191 , Loss : 1.75954270362854, Weights 0.3900609016418457, bias : 0.8571363687515259  \n",
      "Steps : 192 , Loss : 1.7560316324234009, Weights 0.39119476079940796, bias : 0.8556435108184814  \n",
      "Steps : 193 , Loss : 1.7525330781936646, Weights 0.3923267126083374, bias : 0.8541534543037415  \n",
      "Steps : 194 , Loss : 1.7490473985671997, Weights 0.39345675706863403, bias : 0.8526661992073059  \n",
      "Steps : 195 , Loss : 1.7455743551254272, Weights 0.39458492398262024, bias : 0.8511817455291748  \n",
      "Steps : 196 , Loss : 1.7421135902404785, Weights 0.39571118354797363, bias : 0.8497000932693481  \n",
      "Steps : 197 , Loss : 1.7386654615402222, Weights 0.3968355655670166, bias : 0.8482212424278259  \n",
      "Steps : 198 , Loss : 1.7352298498153687, Weights 0.39795807003974915, bias : 0.8467451333999634  \n",
      "Steps : 199 , Loss : 1.7318062782287598, Weights 0.39907869696617126, bias : 0.8452718257904053  \n",
      "Steps : 200 , Loss : 1.7283953428268433, Weights 0.40019744634628296, bias : 0.8438012599945068  \n",
      "Steps : 201 , Loss : 1.7249966859817505, Weights 0.40131431818008423, bias : 0.8423334956169128  \n",
      "Steps : 202 , Loss : 1.7216103076934814, Weights 0.4024293124675751, bias : 0.8408684730529785  \n",
      "Steps : 203 , Loss : 1.718235969543457, Weights 0.4035424590110779, bias : 0.8394061923027039  \n",
      "Steps : 204 , Loss : 1.7148737907409668, Weights 0.40465372800827026, bias : 0.8379466533660889  \n",
      "Steps : 205 , Loss : 1.7115237712860107, Weights 0.4057631492614746, bias : 0.8364898562431335  \n",
      "Steps : 206 , Loss : 1.7081859111785889, Weights 0.4068707227706909, bias : 0.8350358009338379  \n",
      "Steps : 207 , Loss : 1.704859972000122, Weights 0.4079764187335968, bias : 0.8335844874382019  \n",
      "Steps : 208 , Loss : 1.7015460729599, Weights 0.40908026695251465, bias : 0.8321359157562256  \n",
      "Steps : 209 , Loss : 1.6982439756393433, Weights 0.41018226742744446, bias : 0.8306900858879089  \n",
      "Steps : 210 , Loss : 1.6949542760849, Weights 0.41128242015838623, bias : 0.8292469382286072  \n",
      "Steps : 211 , Loss : 1.6916760206222534, Weights 0.41238072514533997, bias : 0.8278065323829651  \n",
      "Steps : 212 , Loss : 1.688409686088562, Weights 0.41347721219062805, bias : 0.8263688087463379  \n",
      "Steps : 213 , Loss : 1.6851550340652466, Weights 0.4145718514919281, bias : 0.8249337673187256  \n",
      "Steps : 214 , Loss : 1.6819120645523071, Weights 0.4156646728515625, bias : 0.823501467704773  \n",
      "Steps : 215 , Loss : 1.6786810159683228, Weights 0.41675564646720886, bias : 0.8220718502998352  \n",
      "Steps : 216 , Loss : 1.6754614114761353, Weights 0.4178448021411896, bias : 0.8206449151039124  \n",
      "Steps : 217 , Loss : 1.6722533702850342, Weights 0.41893213987350464, bias : 0.8192206621170044  \n",
      "Steps : 218 , Loss : 1.6690571308135986, Weights 0.42001765966415405, bias : 0.8177990913391113  \n",
      "Steps : 219 , Loss : 1.6658722162246704, Weights 0.4211013615131378, bias : 0.8163801431655884  \n",
      "Steps : 220 , Loss : 1.6626989841461182, Weights 0.42218324542045593, bias : 0.8149638772010803  \n",
      "Steps : 221 , Loss : 1.6595370769500732, Weights 0.4232633113861084, bias : 0.8135502934455872  \n",
      "Steps : 222 , Loss : 1.656386375427246, Weights 0.4243415594100952, bias : 0.8121393322944641  \n",
      "Steps : 223 , Loss : 1.653247356414795, Weights 0.4254179894924164, bias : 0.810731053352356  \n",
      "Steps : 224 , Loss : 1.650119423866272, Weights 0.4264926314353943, bias : 0.8093253970146179  \n",
      "Steps : 225 , Loss : 1.6470028162002563, Weights 0.42756548523902893, bias : 0.8079224228858948  \n",
      "Steps : 226 , Loss : 1.6438974142074585, Weights 0.4286365211009979, bias : 0.8065220713615417  \n",
      "Steps : 227 , Loss : 1.640803575515747, Weights 0.42970576882362366, bias : 0.8051243424415588  \n",
      "Steps : 228 , Loss : 1.6377204656600952, Weights 0.43077322840690613, bias : 0.803729236125946  \n",
      "Steps : 229 , Loss : 1.6346484422683716, Weights 0.43183889985084534, bias : 0.8023367524147034  \n",
      "Steps : 230 , Loss : 1.6315875053405762, Weights 0.4329027831554413, bias : 0.8009468913078308  \n",
      "Steps : 231 , Loss : 1.628537893295288, Weights 0.43396487832069397, bias : 0.7995596528053284  \n",
      "Steps : 232 , Loss : 1.6254990100860596, Weights 0.4350252151489258, bias : 0.7981749773025513  \n",
      "Steps : 233 , Loss : 1.6224713325500488, Weights 0.43608376383781433, bias : 0.7967929244041443  \n",
      "Steps : 234 , Loss : 1.6194541454315186, Weights 0.4371405243873596, bias : 0.7954134345054626  \n",
      "Steps : 235 , Loss : 1.6164480447769165, Weights 0.43819552659988403, bias : 0.7940365672111511  \n",
      "Steps : 236 , Loss : 1.6134527921676636, Weights 0.4392487704753876, bias : 0.7926622629165649  \n",
      "Steps : 237 , Loss : 1.6104683876037598, Weights 0.44030022621154785, bias : 0.7912905812263489  \n",
      "Steps : 238 , Loss : 1.6074947118759155, Weights 0.44134992361068726, bias : 0.7899214625358582  \n",
      "Steps : 239 , Loss : 1.6045317649841309, Weights 0.4423978626728058, bias : 0.7885549068450928  \n",
      "Steps : 240 , Loss : 1.6015793085098267, Weights 0.44344404339790344, bias : 0.7871909141540527  \n",
      "Steps : 241 , Loss : 1.5986377000808716, Weights 0.4444884955883026, bias : 0.785829484462738  \n",
      "Steps : 242 , Loss : 1.595706582069397, Weights 0.4455311894416809, bias : 0.7844706177711487  \n",
      "Steps : 243 , Loss : 1.5927860736846924, Weights 0.44657212495803833, bias : 0.7831143140792847  \n",
      "Steps : 244 , Loss : 1.5898759365081787, Weights 0.44761133193969727, bias : 0.7817605137825012  \n",
      "Steps : 245 , Loss : 1.5869766473770142, Weights 0.4486487805843353, bias : 0.7804092764854431  \n",
      "Steps : 246 , Loss : 1.5840872526168823, Weights 0.4496845006942749, bias : 0.7790606021881104  \n",
      "Steps : 247 , Loss : 1.5812087059020996, Weights 0.450718492269516, bias : 0.7777144312858582  \n",
      "Steps : 248 , Loss : 1.5783404111862183, Weights 0.4517507255077362, bias : 0.7763708233833313  \n",
      "Steps : 249 , Loss : 1.5754826068878174, Weights 0.45278123021125793, bias : 0.775029718875885  \n",
      "Steps : 250 , Loss : 1.5726349353790283, Weights 0.4538100063800812, bias : 0.7736911177635193  \n",
      "Steps : 251 , Loss : 1.5697975158691406, Weights 0.4548370838165283, bias : 0.7723550200462341  \n",
      "Steps : 252 , Loss : 1.5669702291488647, Weights 0.455862432718277, bias : 0.7710214257240295  \n",
      "Steps : 253 , Loss : 1.5641534328460693, Weights 0.45688605308532715, bias : 0.7696903944015503  \n",
      "Steps : 254 , Loss : 1.561346411705017, Weights 0.4579079747200012, bias : 0.7683618068695068  \n",
      "Steps : 255 , Loss : 1.5585497617721558, Weights 0.4589281678199768, bias : 0.767035722732544  \n",
      "Steps : 256 , Loss : 1.5557631254196167, Weights 0.4599466621875763, bias : 0.7657121419906616  \n",
      "Steps : 257 , Loss : 1.5529865026474, Weights 0.4609634280204773, bias : 0.7643910646438599  \n",
      "Steps : 258 , Loss : 1.5502197742462158, Weights 0.4619784951210022, bias : 0.7630724310874939  \n",
      "Steps : 259 , Loss : 1.5474631786346436, Weights 0.462991863489151, bias : 0.7617563009262085  \n",
      "Steps : 260 , Loss : 1.5447163581848145, Weights 0.4640035331249237, bias : 0.7604426145553589  \n",
      "Steps : 261 , Loss : 1.5419796705245972, Weights 0.4650135040283203, bias : 0.7591314315795898  \n",
      "Steps : 262 , Loss : 1.5392526388168335, Weights 0.4660217761993408, bias : 0.7578226923942566  \n",
      "Steps : 263 , Loss : 1.536535620689392, Weights 0.4670283794403076, bias : 0.7565163969993591  \n",
      "Steps : 264 , Loss : 1.5338281393051147, Weights 0.4680332839488983, bias : 0.7552125453948975  \n",
      "Steps : 265 , Loss : 1.5311306715011597, Weights 0.4690364897251129, bias : 0.7539111971855164  \n",
      "Steps : 266 , Loss : 1.5284427404403687, Weights 0.4700380265712738, bias : 0.752612292766571  \n",
      "Steps : 267 , Loss : 1.5257644653320312, Weights 0.471037894487381, bias : 0.7513157725334167  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 268 , Loss : 1.523095965385437, Weights 0.47203606367111206, bias : 0.7500216960906982  \n",
      "Steps : 269 , Loss : 1.5204370021820068, Weights 0.47303256392478943, bias : 0.7487300634384155  \n",
      "Steps : 270 , Loss : 1.5177878141403198, Weights 0.4740273952484131, bias : 0.7474408745765686  \n",
      "Steps : 271 , Loss : 1.5151478052139282, Weights 0.47502055764198303, bias : 0.7461540699005127  \n",
      "Steps : 272 , Loss : 1.5125175714492798, Weights 0.47601205110549927, bias : 0.7448697090148926  \n",
      "Steps : 273 , Loss : 1.5098965167999268, Weights 0.4770018756389618, bias : 0.7435877323150635  \n",
      "Steps : 274 , Loss : 1.507285475730896, Weights 0.477990061044693, bias : 0.7423081994056702  \n",
      "Steps : 275 , Loss : 1.5046833753585815, Weights 0.4789765775203705, bias : 0.7410310506820679  \n",
      "Steps : 276 , Loss : 1.5020908117294312, Weights 0.47996145486831665, bias : 0.7397562861442566  \n",
      "Steps : 277 , Loss : 1.4995074272155762, Weights 0.4809446632862091, bias : 0.7384839057922363  \n",
      "Steps : 278 , Loss : 1.4969334602355957, Weights 0.48192623257637024, bias : 0.7372139096260071  \n",
      "Steps : 279 , Loss : 1.4943687915802002, Weights 0.48290616273880005, bias : 0.7359462976455688  \n",
      "Steps : 280 , Loss : 1.4918131828308105, Weights 0.48388442397117615, bias : 0.7346810698509216  \n",
      "Steps : 281 , Loss : 1.4892669916152954, Weights 0.4848610460758209, bias : 0.7334182262420654  \n",
      "Steps : 282 , Loss : 1.4867298603057861, Weights 0.4858360290527344, bias : 0.7321577668190002  \n",
      "Steps : 283 , Loss : 1.4842019081115723, Weights 0.4868093729019165, bias : 0.7308996915817261  \n",
      "Steps : 284 , Loss : 1.4816832542419434, Weights 0.4877811074256897, bias : 0.7296439409255981  \n",
      "Steps : 285 , Loss : 1.4791733026504517, Weights 0.48875120282173157, bias : 0.7283905744552612  \n",
      "Steps : 286 , Loss : 1.476672649383545, Weights 0.4897196590900421, bias : 0.7271395325660706  \n",
      "Steps : 287 , Loss : 1.4741809368133545, Weights 0.4906865060329437, bias : 0.7258908748626709  \n",
      "Steps : 288 , Loss : 1.4716981649398804, Weights 0.491651713848114, bias : 0.7246445417404175  \n",
      "Steps : 289 , Loss : 1.469224452972412, Weights 0.49261531233787537, bias : 0.7234005331993103  \n",
      "Steps : 290 , Loss : 1.4667593240737915, Weights 0.4935773015022278, bias : 0.7221588492393494  \n",
      "Steps : 291 , Loss : 1.4643032550811768, Weights 0.49453768134117126, bias : 0.7209195494651794  \n",
      "Steps : 292 , Loss : 1.4618561267852783, Weights 0.4954964518547058, bias : 0.7196825742721558  \n",
      "Steps : 293 , Loss : 1.4594180583953857, Weights 0.4964536130428314, bias : 0.7184479236602783  \n",
      "Steps : 294 , Loss : 1.4569882154464722, Weights 0.4974091649055481, bias : 0.7172155380249023  \n",
      "Steps : 295 , Loss : 1.454567313194275, Weights 0.49836310744285583, bias : 0.7159854769706726  \n",
      "Steps : 296 , Loss : 1.4521554708480835, Weights 0.49931544065475464, bias : 0.7147577404975891  \n",
      "Steps : 297 , Loss : 1.449751853942871, Weights 0.5002661943435669, bias : 0.7135323286056519  \n",
      "Steps : 298 , Loss : 1.447357177734375, Weights 0.5012153387069702, bias : 0.7123091816902161  \n",
      "Steps : 299 , Loss : 1.444970965385437, Weights 0.5021628737449646, bias : 0.7110883593559265  \n",
      "Steps : 300 , Loss : 1.4425933361053467, Weights 0.5031088590621948, bias : 0.7098698019981384  \n",
      "Steps : 301 , Loss : 1.440224289894104, Weights 0.5040532350540161, bias : 0.7086535692214966  \n",
      "Steps : 302 , Loss : 1.437864065170288, Weights 0.5049960017204285, bias : 0.7074396014213562  \n",
      "Steps : 303 , Loss : 1.4355120658874512, Weights 0.5059372186660767, bias : 0.7062278985977173  \n",
      "Steps : 304 , Loss : 1.4331685304641724, Weights 0.5068768262863159, bias : 0.7050184607505798  \n",
      "Steps : 305 , Loss : 1.4308334589004517, Weights 0.507814884185791, bias : 0.7038112878799438  \n",
      "Steps : 306 , Loss : 1.4285067319869995, Weights 0.5087513327598572, bias : 0.7026063799858093  \n",
      "Steps : 307 , Loss : 1.4261884689331055, Weights 0.5096862316131592, bias : 0.7014037370681763  \n",
      "Steps : 308 , Loss : 1.4238786697387695, Weights 0.510619580745697, bias : 0.7002033591270447  \n",
      "Steps : 309 , Loss : 1.421576976776123, Weights 0.5115513205528259, bias : 0.6990052461624146  \n",
      "Steps : 310 , Loss : 1.4192836284637451, Weights 0.5124815106391907, bias : 0.6978093981742859  \n",
      "Steps : 311 , Loss : 1.4169983863830566, Weights 0.5134101510047913, bias : 0.6966157555580139  \n",
      "Steps : 312 , Loss : 1.4147217273712158, Weights 0.5143372416496277, bias : 0.6954243779182434  \n",
      "Steps : 313 , Loss : 1.412453055381775, Weights 0.5152627825737, bias : 0.6942352056503296  \n",
      "Steps : 314 , Loss : 1.410192608833313, Weights 0.5161867737770081, bias : 0.6930482983589172  \n",
      "Steps : 315 , Loss : 1.40794038772583, Weights 0.517109215259552, bias : 0.6918635964393616  \n",
      "Steps : 316 , Loss : 1.405695915222168, Weights 0.5180301070213318, bias : 0.6906810998916626  \n",
      "Steps : 317 , Loss : 1.4034597873687744, Weights 0.5189494490623474, bias : 0.6895008683204651  \n",
      "Steps : 318 , Loss : 1.4012316465377808, Weights 0.5198672413825989, bias : 0.6883228421211243  \n",
      "Steps : 319 , Loss : 1.3990117311477661, Weights 0.5207834839820862, bias : 0.6871470212936401  \n",
      "Steps : 320 , Loss : 1.3967995643615723, Weights 0.5216981768608093, bias : 0.6859734058380127  \n",
      "Steps : 321 , Loss : 1.3945956230163574, Weights 0.5226113200187683, bias : 0.6848019957542419  \n",
      "Steps : 322 , Loss : 1.3923993110656738, Weights 0.5235229730606079, bias : 0.6836327910423279  \n",
      "Steps : 323 , Loss : 1.3902109861373901, Weights 0.5244330763816833, bias : 0.6824657320976257  \n",
      "Steps : 324 , Loss : 1.3880306482315063, Weights 0.5253416299819946, bias : 0.6813008785247803  \n",
      "Steps : 325 , Loss : 1.385858178138733, Weights 0.5262486934661865, bias : 0.6801382303237915  \n",
      "Steps : 326 , Loss : 1.3836933374404907, Weights 0.5271542072296143, bias : 0.6789777278900146  \n",
      "Steps : 327 , Loss : 1.3815363645553589, Weights 0.5280582308769226, bias : 0.6778194308280945  \n",
      "Steps : 328 , Loss : 1.3793872594833374, Weights 0.5289607048034668, bias : 0.6766632795333862  \n",
      "Steps : 329 , Loss : 1.3772456645965576, Weights 0.5298616886138916, bias : 0.6755093336105347  \n",
      "Steps : 330 , Loss : 1.3751122951507568, Weights 0.5307611227035522, bias : 0.674357533454895  \n",
      "Steps : 331 , Loss : 1.3729861974716187, Weights 0.5316590666770935, bias : 0.6732078790664673  \n",
      "Steps : 332 , Loss : 1.3708677291870117, Weights 0.5325555205345154, bias : 0.6720604300498962  \n",
      "Steps : 333 , Loss : 1.3687570095062256, Weights 0.5334504246711731, bias : 0.6709151268005371  \n",
      "Steps : 334 , Loss : 1.3666539192199707, Weights 0.5343438386917114, bias : 0.6697719693183899  \n",
      "Steps : 335 , Loss : 1.364558458328247, Weights 0.5352357625961304, bias : 0.6686309576034546  \n",
      "Steps : 336 , Loss : 1.362470269203186, Weights 0.5361261963844299, bias : 0.6674920916557312  \n",
      "Steps : 337 , Loss : 1.3603898286819458, Weights 0.5370151400566101, bias : 0.666355311870575  \n",
      "Steps : 338 , Loss : 1.3583166599273682, Weights 0.5379025936126709, bias : 0.6652206778526306  \n",
      "Steps : 339 , Loss : 1.3562511205673218, Weights 0.5387885570526123, bias : 0.6640881896018982  \n",
      "Steps : 340 , Loss : 1.354192852973938, Weights 0.5396730303764343, bias : 0.6629578471183777  \n",
      "Steps : 341 , Loss : 1.352142333984375, Weights 0.540556013584137, bias : 0.6618295907974243  \n",
      "Steps : 342 , Loss : 1.3500988483428955, Weights 0.5414375066757202, bias : 0.6607034802436829  \n",
      "Steps : 343 , Loss : 1.3480627536773682, Weights 0.5423175096511841, bias : 0.6595794558525085  \n",
      "Steps : 344 , Loss : 1.346034288406372, Weights 0.5431960225105286, bias : 0.6584575176239014  \n",
      "Steps : 345 , Loss : 1.3440126180648804, Weights 0.5440731048583984, bias : 0.6573377251625061  \n",
      "Steps : 346 , Loss : 1.34199857711792, Weights 0.5449486970901489, bias : 0.656220018863678  \n",
      "Steps : 347 , Loss : 1.3399919271469116, Weights 0.54582279920578, bias : 0.655104398727417  \n",
      "Steps : 348 , Loss : 1.3379920721054077, Weights 0.5466954708099365, bias : 0.6539908647537231  \n",
      "Steps : 349 , Loss : 1.335999608039856, Weights 0.5475666522979736, bias : 0.6528794169425964  \n",
      "Steps : 350 , Loss : 1.3340142965316772, Weights 0.5484363436698914, bias : 0.6517700552940369  \n",
      "Steps : 351 , Loss : 1.3320364952087402, Weights 0.5493046045303345, bias : 0.6506627798080444  \n",
      "Steps : 352 , Loss : 1.3300652503967285, Weights 0.5501713752746582, bias : 0.6495575904846191  \n",
      "Steps : 353 , Loss : 1.328101396560669, Weights 0.5510367155075073, bias : 0.648454487323761  \n",
      "Steps : 354 , Loss : 1.3261445760726929, Weights 0.5519006252288818, bias : 0.64735347032547  \n",
      "Steps : 355 , Loss : 1.3241947889328003, Weights 0.552763044834137, bias : 0.6462544798851013  \n",
      "Steps : 356 , Loss : 1.3222520351409912, Weights 0.5536240339279175, bias : 0.6451575756072998  \n",
      "Steps : 357 , Loss : 1.320316195487976, Weights 0.5544835925102234, bias : 0.6440626978874207  \n",
      "Steps : 358 , Loss : 1.318387508392334, Weights 0.5553416609764099, bias : 0.6429699063301086  \n",
      "Steps : 359 , Loss : 1.3164654970169067, Weights 0.5561982989311218, bias : 0.641879141330719  \n",
      "Steps : 360 , Loss : 1.314550757408142, Weights 0.5570535063743591, bias : 0.6407904028892517  \n",
      "Steps : 361 , Loss : 1.3126425743103027, Weights 0.5579072833061218, bias : 0.6397037506103516  \n",
      "Steps : 362 , Loss : 1.3107413053512573, Weights 0.5587596297264099, bias : 0.6386191248893738  \n",
      "Steps : 363 , Loss : 1.3088470697402954, Weights 0.5596105456352234, bias : 0.6375365257263184  \n",
      "Steps : 364 , Loss : 1.3069595098495483, Weights 0.5604600310325623, bias : 0.6364559531211853  \n",
      "Steps : 365 , Loss : 1.3050787448883057, Weights 0.5613080859184265, bias : 0.6353774070739746  \n",
      "Steps : 366 , Loss : 1.3032046556472778, Weights 0.5621547102928162, bias : 0.6343008875846863  \n",
      "Steps : 367 , Loss : 1.3013375997543335, Weights 0.5629999041557312, bias : 0.6332263946533203  \n",
      "Steps : 368 , Loss : 1.299476981163025, Weights 0.5638436675071716, bias : 0.6321539282798767  \n",
      "Steps : 369 , Loss : 1.2976231575012207, Weights 0.5646860003471375, bias : 0.6310834288597107  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 370 , Loss : 1.2957760095596313, Weights 0.5655269622802734, bias : 0.630014955997467  \n",
      "Steps : 371 , Loss : 1.2939355373382568, Weights 0.5663664937019348, bias : 0.6289485096931458  \n",
      "Steps : 372 , Loss : 1.2921017408370972, Weights 0.5672045946121216, bias : 0.627884030342102  \n",
      "Steps : 373 , Loss : 1.2902746200561523, Weights 0.5680413246154785, bias : 0.6268215775489807  \n",
      "Steps : 374 , Loss : 1.2884538173675537, Weights 0.5688766241073608, bias : 0.625761091709137  \n",
      "Steps : 375 , Loss : 1.2866394519805908, Weights 0.5697104930877686, bias : 0.6247025728225708  \n",
      "Steps : 376 , Loss : 1.2848318815231323, Weights 0.5705429911613464, bias : 0.623646080493927  \n",
      "Steps : 377 , Loss : 1.2830309867858887, Weights 0.5713740587234497, bias : 0.6225915551185608  \n",
      "Steps : 378 , Loss : 1.2812362909317017, Weights 0.5722037553787231, bias : 0.6215389966964722  \n",
      "Steps : 379 , Loss : 1.2794480323791504, Weights 0.5730320811271667, bias : 0.6204884052276611  \n",
      "Steps : 380 , Loss : 1.2776662111282349, Weights 0.5738589763641357, bias : 0.6194397807121277  \n",
      "Steps : 381 , Loss : 1.275890827178955, Weights 0.5746845006942749, bias : 0.6183931231498718  \n",
      "Steps : 382 , Loss : 1.274121880531311, Weights 0.5755086541175842, bias : 0.6173484325408936  \n",
      "Steps : 383 , Loss : 1.2723592519760132, Weights 0.576331377029419, bias : 0.6163057088851929  \n",
      "Steps : 384 , Loss : 1.270603060722351, Weights 0.5771527290344238, bias : 0.6152649521827698  \n",
      "Steps : 385 , Loss : 1.2688533067703247, Weights 0.5779727101325989, bias : 0.6142261624336243  \n",
      "Steps : 386 , Loss : 1.2671096324920654, Weights 0.5787913203239441, bias : 0.6131892800331116  \n",
      "Steps : 387 , Loss : 1.2653721570968628, Weights 0.5796085596084595, bias : 0.6121543645858765  \n",
      "Steps : 388 , Loss : 1.263641119003296, Weights 0.580424427986145, bias : 0.6111213564872742  \n",
      "Steps : 389 , Loss : 1.261916160583496, Weights 0.5812389254570007, bias : 0.6100903153419495  \n",
      "Steps : 390 , Loss : 1.260197401046753, Weights 0.5820520520210266, bias : 0.6090611815452576  \n",
      "Steps : 391 , Loss : 1.258484959602356, Weights 0.5828638076782227, bias : 0.6080340147018433  \n",
      "Steps : 392 , Loss : 1.256778597831726, Weights 0.5836741924285889, bias : 0.6070087552070618  \n",
      "Steps : 393 , Loss : 1.2550784349441528, Weights 0.5844832062721252, bias : 0.6059854030609131  \n",
      "Steps : 394 , Loss : 1.2533843517303467, Weights 0.5852908492088318, bias : 0.6049639582633972  \n",
      "Steps : 395 , Loss : 1.2516963481903076, Weights 0.5860971212387085, bias : 0.6039444804191589  \n",
      "Steps : 396 , Loss : 1.2500145435333252, Weights 0.5869020819664001, bias : 0.6029269099235535  \n",
      "Steps : 397 , Loss : 1.2483385801315308, Weights 0.587705671787262, bias : 0.6019112467765808  \n",
      "Steps : 398 , Loss : 1.2466689348220825, Weights 0.588507890701294, bias : 0.600897490978241  \n",
      "Steps : 399 , Loss : 1.2450050115585327, Weights 0.5893087983131409, bias : 0.5998855829238892  \n",
      "Steps : 400 , Loss : 1.2433470487594604, Weights 0.590108335018158, bias : 0.5988755822181702  \n",
      "Steps : 401 , Loss : 1.2416952848434448, Weights 0.5909065008163452, bias : 0.597867488861084  \n",
      "Steps : 402 , Loss : 1.2400492429733276, Weights 0.5917033553123474, bias : 0.5968613028526306  \n",
      "Steps : 403 , Loss : 1.238409399986267, Weights 0.5924988389015198, bias : 0.5958569645881653  \n",
      "Steps : 404 , Loss : 1.2367753982543945, Weights 0.5932930111885071, bias : 0.5948545336723328  \n",
      "Steps : 405 , Loss : 1.2351469993591309, Weights 0.5940858125686646, bias : 0.5938539505004883  \n",
      "Steps : 406 , Loss : 1.2335247993469238, Weights 0.594877302646637, bias : 0.5928552746772766  \n",
      "Steps : 407 , Loss : 1.2319082021713257, Weights 0.5956674814224243, bias : 0.591858446598053  \n",
      "Steps : 408 , Loss : 1.230297565460205, Weights 0.5964562892913818, bias : 0.5908635258674622  \n",
      "Steps : 409 , Loss : 1.228692650794983, Weights 0.5972437858581543, bias : 0.5898704528808594  \n",
      "Steps : 410 , Loss : 1.2270933389663696, Weights 0.5980299711227417, bias : 0.5888792276382446  \n",
      "Steps : 411 , Loss : 1.2254999876022339, Weights 0.598814845085144, bias : 0.5878898501396179  \n",
      "Steps : 412 , Loss : 1.223912239074707, Weights 0.5995984077453613, bias : 0.5869023203849792  \n",
      "Steps : 413 , Loss : 1.2223302125930786, Weights 0.6003805994987488, bias : 0.5859166383743286  \n",
      "Steps : 414 , Loss : 1.2207540273666382, Weights 0.6011614799499512, bias : 0.584932804107666  \n",
      "Steps : 415 , Loss : 1.2191834449768066, Weights 0.6019410490989685, bias : 0.5839508175849915  \n",
      "Steps : 416 , Loss : 1.217618465423584, Weights 0.6027193069458008, bias : 0.5829706788063049  \n",
      "Steps : 417 , Loss : 1.2160592079162598, Weights 0.603496253490448, bias : 0.5819923877716064  \n",
      "Steps : 418 , Loss : 1.2145054340362549, Weights 0.6042718887329102, bias : 0.581015944480896  \n",
      "Steps : 419 , Loss : 1.212957501411438, Weights 0.605046272277832, bias : 0.5800413489341736  \n",
      "Steps : 420 , Loss : 1.2114148139953613, Weights 0.6058193445205688, bias : 0.5790685415267944  \n",
      "Steps : 421 , Loss : 1.2098777294158936, Weights 0.6065911054611206, bias : 0.5780975818634033  \n",
      "Steps : 422 , Loss : 1.2083462476730347, Weights 0.6073615550994873, bias : 0.5771284103393555  \n",
      "Steps : 423 , Loss : 1.2068204879760742, Weights 0.608130693435669, bias : 0.5761610865592957  \n",
      "Steps : 424 , Loss : 1.2052998542785645, Weights 0.6088985800743103, bias : 0.5751955509185791  \n",
      "Steps : 425 , Loss : 1.2037849426269531, Weights 0.6096651554107666, bias : 0.5742318630218506  \n",
      "Steps : 426 , Loss : 1.202275276184082, Weights 0.6104304194450378, bias : 0.5732699632644653  \n",
      "Steps : 427 , Loss : 1.2007712125778198, Weights 0.6111944317817688, bias : 0.5723098516464233  \n",
      "Steps : 428 , Loss : 1.1992722749710083, Weights 0.6119571328163147, bias : 0.5713515281677246  \n",
      "Steps : 429 , Loss : 1.1977790594100952, Weights 0.6127185821533203, bias : 0.5703949928283691  \n",
      "Steps : 430 , Loss : 1.1962910890579224, Weights 0.6134787201881409, bias : 0.5694402456283569  \n",
      "Steps : 431 , Loss : 1.1948084831237793, Weights 0.6142376065254211, bias : 0.568487286567688  \n",
      "Steps : 432 , Loss : 1.193331003189087, Weights 0.6149951815605164, bias : 0.5675361156463623  \n",
      "Steps : 433 , Loss : 1.191859245300293, Weights 0.6157515048980713, bias : 0.5665867328643799  \n",
      "Steps : 434 , Loss : 1.1903924942016602, Weights 0.6165065169334412, bias : 0.5656391382217407  \n",
      "Steps : 435 , Loss : 1.1889309883117676, Weights 0.6172602772712708, bias : 0.5646933317184448  \n",
      "Steps : 436 , Loss : 1.1874748468399048, Weights 0.6180127859115601, bias : 0.5637493133544922  \n",
      "Steps : 437 , Loss : 1.1860240697860718, Weights 0.6187639832496643, bias : 0.562807023525238  \n",
      "Steps : 438 , Loss : 1.1845784187316895, Weights 0.6195139288902283, bias : 0.5618665218353271  \n",
      "Steps : 439 , Loss : 1.1831378936767578, Weights 0.620262622833252, bias : 0.5609277486801147  \n",
      "Steps : 440 , Loss : 1.1817023754119873, Weights 0.6210100650787354, bias : 0.5599907636642456  \n",
      "Steps : 441 , Loss : 1.1802723407745361, Weights 0.6217562556266785, bias : 0.559055507183075  \n",
      "Steps : 442 , Loss : 1.178847312927246, Weights 0.6225011944770813, bias : 0.5581220388412476  \n",
      "Steps : 443 , Loss : 1.1774274110794067, Weights 0.6232448816299438, bias : 0.5571902990341187  \n",
      "Steps : 444 , Loss : 1.1760125160217285, Weights 0.6239873170852661, bias : 0.5562602877616882  \n",
      "Steps : 445 , Loss : 1.1746028661727905, Weights 0.6247285008430481, bias : 0.5553320646286011  \n",
      "Steps : 446 , Loss : 1.1731982231140137, Weights 0.6254684329032898, bias : 0.5544055700302124  \n",
      "Steps : 447 , Loss : 1.171798586845398, Weights 0.6262071132659912, bias : 0.5534808039665222  \n",
      "Steps : 448 , Loss : 1.170404314994812, Weights 0.6269445419311523, bias : 0.5525577664375305  \n",
      "Steps : 449 , Loss : 1.169014573097229, Weights 0.6276807188987732, bias : 0.5516364574432373  \n",
      "Steps : 450 , Loss : 1.1676299571990967, Weights 0.6284156441688538, bias : 0.5507168769836426  \n",
      "Steps : 451 , Loss : 1.1662505865097046, Weights 0.6291493773460388, bias : 0.5497990250587463  \n",
      "Steps : 452 , Loss : 1.1648759841918945, Weights 0.6298818588256836, bias : 0.5488829016685486  \n",
      "Steps : 453 , Loss : 1.1635063886642456, Weights 0.6306130886077881, bias : 0.5479685068130493  \n",
      "Steps : 454 , Loss : 1.1621416807174683, Weights 0.6313430666923523, bias : 0.5470557808876038  \n",
      "Steps : 455 , Loss : 1.1607818603515625, Weights 0.632071852684021, bias : 0.5461447834968567  \n",
      "Steps : 456 , Loss : 1.1594269275665283, Weights 0.6327993869781494, bias : 0.5452355146408081  \n",
      "Steps : 457 , Loss : 1.1580768823623657, Weights 0.6335257291793823, bias : 0.5443279147148132  \n",
      "Steps : 458 , Loss : 1.1567316055297852, Weights 0.634250819683075, bias : 0.5434220433235168  \n",
      "Steps : 459 , Loss : 1.1553913354873657, Weights 0.6349747180938721, bias : 0.5425178408622742  \n",
      "Steps : 460 , Loss : 1.1540558338165283, Weights 0.6356973648071289, bias : 0.54161536693573  \n",
      "Steps : 461 , Loss : 1.152725100517273, Weights 0.6364188194274902, bias : 0.5407145619392395  \n",
      "Steps : 462 , Loss : 1.1513991355895996, Weights 0.6371390223503113, bias : 0.5398154258728027  \n",
      "Steps : 463 , Loss : 1.1500779390335083, Weights 0.6378580331802368, bias : 0.5389179587364197  \n",
      "Steps : 464 , Loss : 1.1487613916397095, Weights 0.6385758519172668, bias : 0.5380222201347351  \n",
      "Steps : 465 , Loss : 1.1474496126174927, Weights 0.6392924189567566, bias : 0.5371281504631042  \n",
      "Steps : 466 , Loss : 1.146142840385437, Weights 0.6400077939033508, bias : 0.5362357497215271  \n",
      "Steps : 467 , Loss : 1.1448405981063843, Weights 0.6407219767570496, bias : 0.5353450179100037  \n",
      "Steps : 468 , Loss : 1.1435428857803345, Weights 0.6414349675178528, bias : 0.5344559550285339  \n",
      "Steps : 469 , Loss : 1.1422499418258667, Weights 0.6421467661857605, bias : 0.5335685610771179  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 470 , Loss : 1.1409616470336914, Weights 0.6428573131561279, bias : 0.5326828360557556  \n",
      "Steps : 471 , Loss : 1.1396780014038086, Weights 0.6435666680335999, bias : 0.531798779964447  \n",
      "Steps : 472 , Loss : 1.1383987665176392, Weights 0.6442748308181763, bias : 0.5309163331985474  \n",
      "Steps : 473 , Loss : 1.1371245384216309, Weights 0.6449818015098572, bias : 0.5300355553627014  \n",
      "Steps : 474 , Loss : 1.1358546018600464, Weights 0.6456875801086426, bias : 0.5291564464569092  \n",
      "Steps : 475 , Loss : 1.134589433670044, Weights 0.6463921666145325, bias : 0.5282789468765259  \n",
      "Steps : 476 , Loss : 1.1333286762237549, Weights 0.6470955610275269, bias : 0.5274031162261963  \n",
      "Steps : 477 , Loss : 1.1320724487304688, Weights 0.6477978229522705, bias : 0.5265288949012756  \n",
      "Steps : 478 , Loss : 1.1308207511901855, Weights 0.6484988927841187, bias : 0.5256563425064087  \n",
      "Steps : 479 , Loss : 1.1295734643936157, Weights 0.6491987705230713, bias : 0.5247853994369507  \n",
      "Steps : 480 , Loss : 1.1283307075500488, Weights 0.6498974561691284, bias : 0.5239161252975464  \n",
      "Steps : 481 , Loss : 1.1270924806594849, Weights 0.65059494972229, bias : 0.523048460483551  \n",
      "Steps : 482 , Loss : 1.1258586645126343, Weights 0.6512913107872009, bias : 0.5221824049949646  \n",
      "Steps : 483 , Loss : 1.1246293783187866, Weights 0.6519864797592163, bias : 0.5213179588317871  \n",
      "Steps : 484 , Loss : 1.1234043836593628, Weights 0.6526804566383362, bias : 0.5204551815986633  \n",
      "Steps : 485 , Loss : 1.1221837997436523, Weights 0.6533733010292053, bias : 0.5195940136909485  \n",
      "Steps : 486 , Loss : 1.1209675073623657, Weights 0.654064953327179, bias : 0.5187344551086426  \n",
      "Steps : 487 , Loss : 1.1197558641433716, Weights 0.6547554731369019, bias : 0.5178765058517456  \n",
      "Steps : 488 , Loss : 1.1185483932495117, Weights 0.6554448008537292, bias : 0.5170201659202576  \n",
      "Steps : 489 , Loss : 1.1173453330993652, Weights 0.6561329364776611, bias : 0.5161654353141785  \n",
      "Steps : 490 , Loss : 1.116146445274353, Weights 0.6568199396133423, bias : 0.5153122544288635  \n",
      "Steps : 491 , Loss : 1.1149520874023438, Weights 0.6575058102607727, bias : 0.5144606828689575  \n",
      "Steps : 492 , Loss : 1.1137620210647583, Weights 0.6581904888153076, bias : 0.5136107206344604  \n",
      "Steps : 493 , Loss : 1.1125760078430176, Weights 0.6588740348815918, bias : 0.5127623677253723  \n",
      "Steps : 494 , Loss : 1.1113942861557007, Weights 0.6595564484596252, bias : 0.5119155645370483  \n",
      "Steps : 495 , Loss : 1.1102169752120972, Weights 0.6602376699447632, bias : 0.5110703706741333  \n",
      "Steps : 496 , Loss : 1.109043836593628, Weights 0.6609177589416504, bias : 0.5102267265319824  \n",
      "Steps : 497 , Loss : 1.107874870300293, Weights 0.6615967154502869, bias : 0.5093846917152405  \n",
      "Steps : 498 , Loss : 1.1067101955413818, Weights 0.6622744798660278, bias : 0.5085442066192627  \n",
      "Steps : 499 , Loss : 1.1055495738983154, Weights 0.6629511117935181, bias : 0.5077053308486938  \n",
      "Steps : 500 , Loss : 1.1043933629989624, Weights 0.6636266112327576, bias : 0.5068680047988892  \n",
      "Steps : 501 , Loss : 1.103240966796875, Weights 0.6643009781837463, bias : 0.5060322284698486  \n",
      "Steps : 502 , Loss : 1.1020931005477905, Weights 0.6649742126464844, bias : 0.505198061466217  \n",
      "Steps : 503 , Loss : 1.1009490489959717, Weights 0.6656463146209717, bias : 0.5043654441833496  \n",
      "Steps : 504 , Loss : 1.099809169769287, Weights 0.6663172841072083, bias : 0.5035343766212463  \n",
      "Steps : 505 , Loss : 1.0986734628677368, Weights 0.6669871211051941, bias : 0.5027048587799072  \n",
      "Steps : 506 , Loss : 1.0975416898727417, Weights 0.6676558256149292, bias : 0.5018768906593323  \n",
      "Steps : 507 , Loss : 1.0964142084121704, Weights 0.6683233976364136, bias : 0.5010504722595215  \n",
      "Steps : 508 , Loss : 1.0952906608581543, Weights 0.6689898371696472, bias : 0.5002256035804749  \n",
      "Steps : 509 , Loss : 1.094171166419983, Weights 0.6696551442146301, bias : 0.4994022846221924  \n",
      "Steps : 510 , Loss : 1.0930557250976562, Weights 0.6703193783760071, bias : 0.4985805153846741  \n",
      "Steps : 511 , Loss : 1.0919440984725952, Weights 0.6709824800491333, bias : 0.49776026606559753  \n",
      "Steps : 512 , Loss : 1.090836524963379, Weights 0.6716444492340088, bias : 0.49694156646728516  \n",
      "Steps : 513 , Loss : 1.0897331237792969, Weights 0.6723052859306335, bias : 0.49612438678741455  \n",
      "Steps : 514 , Loss : 1.0886335372924805, Weights 0.6729650497436523, bias : 0.4953087270259857  \n",
      "Steps : 515 , Loss : 1.0875378847122192, Weights 0.6736236810684204, bias : 0.49449461698532104  \n",
      "Steps : 516 , Loss : 1.0864462852478027, Weights 0.6742811799049377, bias : 0.49368202686309814  \n",
      "Steps : 517 , Loss : 1.0853585004806519, Weights 0.6749376058578491, bias : 0.492870956659317  \n",
      "Steps : 518 , Loss : 1.0842747688293457, Weights 0.6755928993225098, bias : 0.49206140637397766  \n",
      "Steps : 519 , Loss : 1.0831948518753052, Weights 0.6762471199035645, bias : 0.4912533760070801  \n",
      "Steps : 520 , Loss : 1.0821187496185303, Weights 0.6769002079963684, bias : 0.4904468357563019  \n",
      "Steps : 521 , Loss : 1.081046462059021, Weights 0.6775522232055664, bias : 0.48964181542396545  \n",
      "Steps : 522 , Loss : 1.0799782276153564, Weights 0.6782031059265137, bias : 0.4888383150100708  \n",
      "Steps : 523 , Loss : 1.0789138078689575, Weights 0.678852915763855, bias : 0.48803630471229553  \n",
      "Steps : 524 , Loss : 1.0778530836105347, Weights 0.6795015931129456, bias : 0.48723578453063965  \n",
      "Steps : 525 , Loss : 1.076796293258667, Weights 0.6801491975784302, bias : 0.48643678426742554  \n",
      "Steps : 526 , Loss : 1.0757431983947754, Weights 0.6807957291603088, bias : 0.4856392741203308  \n",
      "Steps : 527 , Loss : 1.0746939182281494, Weights 0.6814411282539368, bias : 0.48484325408935547  \n",
      "Steps : 528 , Loss : 1.073648452758789, Weights 0.6820854544639587, bias : 0.4840487241744995  \n",
      "Steps : 529 , Loss : 1.0726066827774048, Weights 0.6827287077903748, bias : 0.48325568437576294  \n",
      "Steps : 530 , Loss : 1.0715687274932861, Weights 0.6833708882331848, bias : 0.48246413469314575  \n",
      "Steps : 531 , Loss : 1.070534348487854, Weights 0.6840119361877441, bias : 0.48167404532432556  \n",
      "Steps : 532 , Loss : 1.069503903388977, Weights 0.6846519112586975, bias : 0.48088544607162476  \n",
      "Steps : 533 , Loss : 1.068476915359497, Weights 0.6852908134460449, bias : 0.48009830713272095  \n",
      "Steps : 534 , Loss : 1.0674538612365723, Weights 0.6859286427497864, bias : 0.4793126583099365  \n",
      "Steps : 535 , Loss : 1.066434383392334, Weights 0.6865653991699219, bias : 0.4785284698009491  \n",
      "Steps : 536 , Loss : 1.0654186010360718, Weights 0.6872010827064514, bias : 0.47774574160575867  \n",
      "Steps : 537 , Loss : 1.064406394958496, Weights 0.687835693359375, bias : 0.47696447372436523  \n",
      "Steps : 538 , Loss : 1.0633978843688965, Weights 0.6884692311286926, bias : 0.4761846959590912  \n",
      "Steps : 539 , Loss : 1.0623928308486938, Weights 0.6891016960144043, bias : 0.47540634870529175  \n",
      "Steps : 540 , Loss : 1.0613915920257568, Weights 0.68973308801651, bias : 0.4746294617652893  \n",
      "Steps : 541 , Loss : 1.0603939294815063, Weights 0.6903634071350098, bias : 0.47385403513908386  \n",
      "Steps : 542 , Loss : 1.0593998432159424, Weights 0.6909926533699036, bias : 0.4730800688266754  \n",
      "Steps : 543 , Loss : 1.058409333229065, Weights 0.6916208863258362, bias : 0.4723075330257416  \n",
      "Steps : 544 , Loss : 1.0574222803115845, Weights 0.6922480463981628, bias : 0.47153645753860474  \n",
      "Steps : 545 , Loss : 1.0564388036727905, Weights 0.6928741335868835, bias : 0.4707668125629425  \n",
      "Steps : 546 , Loss : 1.0554587841033936, Weights 0.6934991478919983, bias : 0.46999862790107727  \n",
      "Steps : 547 , Loss : 1.0544824600219727, Weights 0.6941231489181519, bias : 0.46923187375068665  \n",
      "Steps : 548 , Loss : 1.0535094738006592, Weights 0.6947460770606995, bias : 0.46846655011177063  \n",
      "Steps : 549 , Loss : 1.0525401830673218, Weights 0.6953679323196411, bias : 0.4677026569843292  \n",
      "Steps : 550 , Loss : 1.0515743494033813, Weights 0.6959887742996216, bias : 0.4669401943683624  \n",
      "Steps : 551 , Loss : 1.0506118535995483, Weights 0.6966085433959961, bias : 0.46617916226387024  \n",
      "Steps : 552 , Loss : 1.0496528148651123, Weights 0.6972272992134094, bias : 0.46541956067085266  \n",
      "Steps : 553 , Loss : 1.0486973524093628, Weights 0.6978449821472168, bias : 0.4646613597869873  \n",
      "Steps : 554 , Loss : 1.0477452278137207, Weights 0.698461651802063, bias : 0.46390458941459656  \n",
      "Steps : 555 , Loss : 1.0467965602874756, Weights 0.6990772485733032, bias : 0.46314921975135803  \n",
      "Steps : 556 , Loss : 1.045851230621338, Weights 0.6996918320655823, bias : 0.4623952805995941  \n",
      "Steps : 557 , Loss : 1.0449094772338867, Weights 0.7003053426742554, bias : 0.4616427421569824  \n",
      "Steps : 558 , Loss : 1.0439708232879639, Weights 0.7009178400039673, bias : 0.46089160442352295  \n",
      "Steps : 559 , Loss : 1.0430357456207275, Weights 0.701529324054718, bias : 0.4601418673992157  \n",
      "Steps : 560 , Loss : 1.042103886604309, Weights 0.7021397352218628, bias : 0.45939353108406067  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 561 , Loss : 1.0411754846572876, Weights 0.7027491331100464, bias : 0.45864659547805786  \n",
      "Steps : 562 , Loss : 1.0402504205703735, Weights 0.7033575177192688, bias : 0.4579010605812073  \n",
      "Steps : 563 , Loss : 1.0393288135528564, Weights 0.7039648294448853, bias : 0.4571569263935089  \n",
      "Steps : 564 , Loss : 1.0384103059768677, Weights 0.7045711278915405, bias : 0.45641419291496277  \n",
      "Steps : 565 , Loss : 1.0374951362609863, Weights 0.7051764130592346, bias : 0.45567283034324646  \n",
      "Steps : 566 , Loss : 1.0365833044052124, Weights 0.7057806849479675, bias : 0.4549328684806824  \n",
      "Steps : 567 , Loss : 1.0356746912002563, Weights 0.7063839435577393, bias : 0.4541942775249481  \n",
      "Steps : 568 , Loss : 1.0347694158554077, Weights 0.7069861888885498, bias : 0.4534570872783661  \n",
      "Steps : 569 , Loss : 1.0338672399520874, Weights 0.7075874209403992, bias : 0.4527212679386139  \n",
      "Steps : 570 , Loss : 1.032968282699585, Weights 0.7081876397132874, bias : 0.45198681950569153  \n",
      "Steps : 571 , Loss : 1.0320727825164795, Weights 0.7087868452072144, bias : 0.451253741979599  \n",
      "Steps : 572 , Loss : 1.0311803817749023, Weights 0.7093850374221802, bias : 0.4505220353603363  \n",
      "Steps : 573 , Loss : 1.0302910804748535, Weights 0.7099822163581848, bias : 0.44979169964790344  \n",
      "Steps : 574 , Loss : 1.029405117034912, Weights 0.7105783820152283, bias : 0.4490627348423004  \n",
      "Steps : 575 , Loss : 1.0285223722457886, Weights 0.7111735343933105, bias : 0.4483351409435272  \n",
      "Steps : 576 , Loss : 1.0276427268981934, Weights 0.7117676734924316, bias : 0.4476088881492615  \n",
      "Steps : 577 , Loss : 1.0267661809921265, Weights 0.7123607993125916, bias : 0.44688400626182556  \n",
      "Steps : 578 , Loss : 1.025892972946167, Weights 0.7129529118537903, bias : 0.4461604654788971  \n",
      "Steps : 579 , Loss : 1.0250226259231567, Weights 0.7135440707206726, bias : 0.44543829560279846  \n",
      "Steps : 580 , Loss : 1.0241557359695435, Weights 0.7141342163085938, bias : 0.4447174668312073  \n",
      "Steps : 581 , Loss : 1.0232915878295898, Weights 0.7147233486175537, bias : 0.44399797916412354  \n",
      "Steps : 582 , Loss : 1.0224308967590332, Weights 0.7153114676475525, bias : 0.44327986240386963  \n",
      "Steps : 583 , Loss : 1.0215730667114258, Weights 0.7158986330032349, bias : 0.44256308674812317  \n",
      "Steps : 584 , Loss : 1.0207183361053467, Weights 0.716484785079956, bias : 0.44184765219688416  \n",
      "Steps : 585 , Loss : 1.0198668241500854, Weights 0.7170699834823608, bias : 0.4411335289478302  \n",
      "Steps : 586 , Loss : 1.019018292427063, Weights 0.7176541686058044, bias : 0.4404207468032837  \n",
      "Steps : 587 , Loss : 1.0181727409362793, Weights 0.7182373404502869, bias : 0.43970930576324463  \n",
      "Steps : 588 , Loss : 1.017330288887024, Weights 0.7188195586204529, bias : 0.438999205827713  \n",
      "Steps : 589 , Loss : 1.0164909362792969, Weights 0.7194007635116577, bias : 0.43829041719436646  \n",
      "Steps : 590 , Loss : 1.015654444694519, Weights 0.7199810147285461, bias : 0.43758296966552734  \n",
      "Steps : 591 , Loss : 1.0148210525512695, Weights 0.7205602526664734, bias : 0.4368768334388733  \n",
      "Steps : 592 , Loss : 1.0139905214309692, Weights 0.7211385369300842, bias : 0.4361720085144043  \n",
      "Steps : 593 , Loss : 1.0131630897521973, Weights 0.7217158675193787, bias : 0.43546852469444275  \n",
      "Steps : 594 , Loss : 1.012338638305664, Weights 0.7222921848297119, bias : 0.43476635217666626  \n",
      "Steps : 595 , Loss : 1.0115171670913696, Weights 0.7228675484657288, bias : 0.43406549096107483  \n",
      "Steps : 596 , Loss : 1.0106985569000244, Weights 0.7234419584274292, bias : 0.43336594104766846  \n",
      "Steps : 597 , Loss : 1.009882926940918, Weights 0.7240153551101685, bias : 0.43266770243644714  \n",
      "Steps : 598 , Loss : 1.0090703964233398, Weights 0.7245877981185913, bias : 0.4319707453250885  \n",
      "Steps : 599 , Loss : 1.0082603693008423, Weights 0.7251592874526978, bias : 0.4312750995159149  \n",
      "Steps : 600 , Loss : 1.0074535608291626, Weights 0.7257298231124878, bias : 0.4305807650089264  \n",
      "Steps : 601 , Loss : 1.0066496133804321, Weights 0.7262993454933167, bias : 0.42988771200180054  \n",
      "Steps : 602 , Loss : 1.0058486461639404, Weights 0.7268679141998291, bias : 0.42919597029685974  \n",
      "Steps : 603 , Loss : 1.0050503015518188, Weights 0.7274355292320251, bias : 0.4285055100917816  \n",
      "Steps : 604 , Loss : 1.004254937171936, Weights 0.7280021905899048, bias : 0.42781636118888855  \n",
      "Steps : 605 , Loss : 1.003462553024292, Weights 0.728567898273468, bias : 0.42712849378585815  \n",
      "Steps : 606 , Loss : 1.0026729106903076, Weights 0.7291326522827148, bias : 0.42644190788269043  \n",
      "Steps : 607 , Loss : 1.0018861293792725, Weights 0.7296964526176453, bias : 0.4257566034793854  \n",
      "Steps : 608 , Loss : 1.001102089881897, Weights 0.7302592992782593, bias : 0.425072580575943  \n",
      "Steps : 609 , Loss : 1.0003210306167603, Weights 0.7308211922645569, bias : 0.4243898391723633  \n",
      "Steps : 610 , Loss : 0.9995425343513489, Weights 0.7313821315765381, bias : 0.42370837926864624  \n",
      "Steps : 611 , Loss : 0.9987670183181763, Weights 0.7319421172142029, bias : 0.4230281710624695  \n",
      "Steps : 612 , Loss : 0.9979942440986633, Weights 0.7325011491775513, bias : 0.4223492443561554  \n",
      "Steps : 613 , Loss : 0.9972242116928101, Weights 0.733059287071228, bias : 0.421671599149704  \n",
      "Steps : 614 , Loss : 0.996457040309906, Weights 0.7336164712905884, bias : 0.42099520564079285  \n",
      "Steps : 615 , Loss : 0.9956924319267273, Weights 0.7341727018356323, bias : 0.4203200936317444  \n",
      "Steps : 616 , Loss : 0.9949307441711426, Weights 0.7347279787063599, bias : 0.4196462333202362  \n",
      "Steps : 617 , Loss : 0.9941717386245728, Weights 0.735282301902771, bias : 0.4189736247062683  \n",
      "Steps : 618 , Loss : 0.993415355682373, Weights 0.7358357310295105, bias : 0.4183022677898407  \n",
      "Steps : 619 , Loss : 0.9926618933677673, Weights 0.7363882064819336, bias : 0.41763216257095337  \n",
      "Steps : 620 , Loss : 0.991910994052887, Weights 0.7369397282600403, bias : 0.4169633090496063  \n",
      "Steps : 621 , Loss : 0.9911627769470215, Weights 0.7374903559684753, bias : 0.41629570722579956  \n",
      "Steps : 622 , Loss : 0.9904173016548157, Weights 0.738040030002594, bias : 0.4156293570995331  \n",
      "Steps : 623 , Loss : 0.9896745085716248, Weights 0.7385887503623962, bias : 0.4149642586708069  \n",
      "Steps : 624 , Loss : 0.9889343976974487, Weights 0.7391365766525269, bias : 0.41430041193962097  \n",
      "Steps : 625 , Loss : 0.988196849822998, Weights 0.7396834492683411, bias : 0.41363778710365295  \n",
      "Steps : 626 , Loss : 0.9874619841575623, Weights 0.7402294278144836, bias : 0.4129764139652252  \n",
      "Steps : 627 , Loss : 0.9867297410964966, Weights 0.7407744526863098, bias : 0.4123162627220154  \n",
      "Steps : 628 , Loss : 0.9860002398490906, Weights 0.7413185834884644, bias : 0.4116573631763458  \n",
      "Steps : 629 , Loss : 0.9852733016014099, Weights 0.7418617606163025, bias : 0.41099968552589417  \n",
      "Steps : 630 , Loss : 0.9845490455627441, Weights 0.742404043674469, bias : 0.4103432297706604  \n",
      "Steps : 631 , Loss : 0.9838272333145142, Weights 0.7429454326629639, bias : 0.4096880257129669  \n",
      "Steps : 632 , Loss : 0.9831080436706543, Weights 0.7434858679771423, bias : 0.40903404355049133  \n",
      "Steps : 633 , Loss : 0.9823915362358093, Weights 0.7440254092216492, bias : 0.40838128328323364  \n",
      "Steps : 634 , Loss : 0.9816775321960449, Weights 0.7445640563964844, bias : 0.40772974491119385  \n",
      "Steps : 635 , Loss : 0.9809662103652954, Weights 0.7451017498970032, bias : 0.40707939863204956  \n",
      "Steps : 636 , Loss : 0.9802572727203369, Weights 0.7456385493278503, bias : 0.40643027424812317  \n",
      "Steps : 637 , Loss : 0.9795509576797485, Weights 0.7461744546890259, bias : 0.4057823717594147  \n",
      "Steps : 638 , Loss : 0.9788471460342407, Weights 0.7467094659805298, bias : 0.4051356911659241  \n",
      "Steps : 639 , Loss : 0.9781458973884583, Weights 0.7472435832023621, bias : 0.404490202665329  \n",
      "Steps : 640 , Loss : 0.9774470329284668, Weights 0.7477767467498779, bias : 0.4038459360599518  \n",
      "Steps : 641 , Loss : 0.9767508506774902, Weights 0.7483090162277222, bias : 0.4032028615474701  \n",
      "Steps : 642 , Loss : 0.9760571122169495, Weights 0.7488403916358948, bias : 0.4025609791278839  \n",
      "Steps : 643 , Loss : 0.9753658771514893, Weights 0.7493708729743958, bias : 0.4019203186035156  \n",
      "Steps : 644 , Loss : 0.9746771454811096, Weights 0.7499004602432251, bias : 0.40128085017204285  \n",
      "Steps : 645 , Loss : 0.973990797996521, Weights 0.7504291534423828, bias : 0.4006425738334656  \n",
      "Steps : 646 , Loss : 0.9733070135116577, Weights 0.7509569525718689, bias : 0.4000054895877838  \n",
      "Steps : 647 , Loss : 0.9726255536079407, Weights 0.7514838576316833, bias : 0.39936959743499756  \n",
      "Steps : 648 , Loss : 0.9719467163085938, Weights 0.7520098686218262, bias : 0.3987348973751068  \n",
      "Steps : 649 , Loss : 0.9712700843811035, Weights 0.7525349855422974, bias : 0.3981013596057892  \n",
      "Steps : 650 , Loss : 0.9705960154533386, Weights 0.7530592083930969, bias : 0.39746901392936707  \n",
      "Steps : 651 , Loss : 0.9699244499206543, Weights 0.7535825371742249, bias : 0.39683786034584045  \n",
      "Steps : 652 , Loss : 0.9692550897598267, Weights 0.7541050314903259, bias : 0.39620786905288696  \n",
      "Steps : 653 , Loss : 0.9685884118080139, Weights 0.7546266317367554, bias : 0.395579069852829  \n",
      "Steps : 654 , Loss : 0.9679238796234131, Weights 0.7551473379135132, bias : 0.3949514329433441  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 655 , Loss : 0.967261791229248, Weights 0.7556671500205994, bias : 0.39432498812675476  \n",
      "Steps : 656 , Loss : 0.966602087020874, Weights 0.7561860680580139, bias : 0.3936997056007385  \n",
      "Steps : 657 , Loss : 0.9659448266029358, Weights 0.7567041516304016, bias : 0.3930755853652954  \n",
      "Steps : 658 , Loss : 0.965289831161499, Weights 0.7572213411331177, bias : 0.3924526274204254  \n",
      "Steps : 659 , Loss : 0.9646371603012085, Weights 0.7577376365661621, bias : 0.39183083176612854  \n",
      "Steps : 660 , Loss : 0.9639869928359985, Weights 0.7582530975341797, bias : 0.3912101984024048  \n",
      "Steps : 661 , Loss : 0.9633389115333557, Weights 0.7587676644325256, bias : 0.39059072732925415  \n",
      "Steps : 662 , Loss : 0.9626933336257935, Weights 0.7592813372612, bias : 0.38997241854667664  \n",
      "Steps : 663 , Loss : 0.9620500802993774, Weights 0.7597941756248474, bias : 0.38935527205467224  \n",
      "Steps : 664 , Loss : 0.9614091515541077, Weights 0.7603061199188232, bias : 0.3887392580509186  \n",
      "Steps : 665 , Loss : 0.9607704281806946, Weights 0.7608172297477722, bias : 0.38812440633773804  \n",
      "Steps : 666 , Loss : 0.9601340293884277, Weights 0.7613274455070496, bias : 0.3875107169151306  \n",
      "Steps : 667 , Loss : 0.9594999551773071, Weights 0.7618368268013, bias : 0.3868981599807739  \n",
      "Steps : 668 , Loss : 0.9588680863380432, Weights 0.7623453140258789, bias : 0.38628673553466797  \n",
      "Steps : 669 , Loss : 0.9582385420799255, Weights 0.7628529667854309, bias : 0.38567647337913513  \n",
      "Steps : 670 , Loss : 0.9576112627983093, Weights 0.7633597254753113, bias : 0.385067343711853  \n",
      "Steps : 671 , Loss : 0.9569861888885498, Weights 0.7638656497001648, bias : 0.38445934653282166  \n",
      "Steps : 672 , Loss : 0.9563633799552917, Weights 0.7643707394599915, bias : 0.383852481842041  \n",
      "Steps : 673 , Loss : 0.9557427763938904, Weights 0.7648749351501465, bias : 0.3832467496395111  \n",
      "Steps : 674 , Loss : 0.9551243782043457, Weights 0.7653782963752747, bias : 0.38264214992523193  \n",
      "Steps : 675 , Loss : 0.9545083045959473, Weights 0.765880823135376, bias : 0.3820386826992035  \n",
      "Steps : 676 , Loss : 0.9538943767547607, Weights 0.7663824558258057, bias : 0.3814363479614258  \n",
      "Steps : 677 , Loss : 0.9532825946807861, Weights 0.7668832540512085, bias : 0.3808351457118988  \n",
      "Steps : 678 , Loss : 0.9526731967926025, Weights 0.7673832178115845, bias : 0.38023504614830017  \n",
      "Steps : 679 , Loss : 0.9520657062530518, Weights 0.7678823471069336, bias : 0.37963607907295227  \n",
      "Steps : 680 , Loss : 0.951460599899292, Weights 0.7683806419372559, bias : 0.3790382444858551  \n",
      "Steps : 681 , Loss : 0.9508576989173889, Weights 0.7688780426979065, bias : 0.3784415125846863  \n",
      "Steps : 682 , Loss : 0.9502568244934082, Weights 0.7693746089935303, bias : 0.3778458833694458  \n",
      "Steps : 683 , Loss : 0.949658215045929, Weights 0.7698703408241272, bias : 0.37725138664245605  \n",
      "Steps : 684 , Loss : 0.9490615725517273, Weights 0.7703652381896973, bias : 0.37665799260139465  \n",
      "Steps : 685 , Loss : 0.9484674334526062, Weights 0.7708593010902405, bias : 0.3760657012462616  \n",
      "Steps : 686 , Loss : 0.9478751420974731, Weights 0.7713525295257568, bias : 0.3754745125770569  \n",
      "Steps : 687 , Loss : 0.947284996509552, Weights 0.7718449234962463, bias : 0.3748844265937805  \n",
      "Steps : 688 , Loss : 0.946696937084198, Weights 0.772336483001709, bias : 0.3742954432964325  \n",
      "Steps : 689 , Loss : 0.9461109638214111, Weights 0.7728272080421448, bias : 0.3737075626850128  \n",
      "Steps : 690 , Loss : 0.9455272555351257, Weights 0.7733170986175537, bias : 0.3731207847595215  \n",
      "Steps : 691 , Loss : 0.9449455142021179, Weights 0.7738061547279358, bias : 0.3725351095199585  \n",
      "Steps : 692 , Loss : 0.9443659782409668, Weights 0.774294376373291, bias : 0.37195050716400146  \n",
      "Steps : 693 , Loss : 0.9437885284423828, Weights 0.7747818231582642, bias : 0.3713670074939728  \n",
      "Steps : 694 , Loss : 0.9432129859924316, Weights 0.7752684354782104, bias : 0.37078461050987244  \n",
      "Steps : 695 , Loss : 0.9426395893096924, Weights 0.7757542133331299, bias : 0.37020328640937805  \n",
      "Steps : 696 , Loss : 0.9420681595802307, Weights 0.7762391567230225, bias : 0.369623064994812  \n",
      "Steps : 697 , Loss : 0.9414989352226257, Weights 0.7767232656478882, bias : 0.36904391646385193  \n",
      "Steps : 698 , Loss : 0.9409316778182983, Weights 0.7772065997123718, bias : 0.3684658408164978  \n",
      "Steps : 699 , Loss : 0.9403665065765381, Weights 0.7776890993118286, bias : 0.36788883805274963  \n",
      "Steps : 700 , Loss : 0.9398033022880554, Weights 0.7781707644462585, bias : 0.3673129379749298  \n",
      "Steps : 701 , Loss : 0.9392422437667847, Weights 0.7786515951156616, bias : 0.36673811078071594  \n",
      "Steps : 702 , Loss : 0.938683032989502, Weights 0.7791316509246826, bias : 0.36616435647010803  \n",
      "Steps : 703 , Loss : 0.9381259083747864, Weights 0.7796108722686768, bias : 0.3655916750431061  \n",
      "Steps : 704 , Loss : 0.9375708103179932, Weights 0.7800893187522888, bias : 0.3650200664997101  \n",
      "Steps : 705 , Loss : 0.9370176792144775, Weights 0.780566930770874, bias : 0.36444950103759766  \n",
      "Steps : 706 , Loss : 0.9364665150642395, Weights 0.7810437083244324, bias : 0.3638800084590912  \n",
      "Steps : 707 , Loss : 0.9359173774719238, Weights 0.7815197110176086, bias : 0.3633115887641907  \n",
      "Steps : 708 , Loss : 0.9353700876235962, Weights 0.7819948792457581, bias : 0.36274421215057373  \n",
      "Steps : 709 , Loss : 0.9348248839378357, Weights 0.7824692726135254, bias : 0.36217790842056274  \n",
      "Steps : 710 , Loss : 0.9342815279960632, Weights 0.7829428315162659, bias : 0.3616126477718353  \n",
      "Steps : 711 , Loss : 0.9337401390075684, Weights 0.7834156155586243, bias : 0.36104846000671387  \n",
      "Steps : 712 , Loss : 0.9332008361816406, Weights 0.7838875651359558, bias : 0.360485315322876  \n",
      "Steps : 713 , Loss : 0.9326632618904114, Weights 0.7843587398529053, bias : 0.35992321372032166  \n",
      "Steps : 714 , Loss : 0.9321277141571045, Weights 0.7848291397094727, bias : 0.3593621850013733  \n",
      "Steps : 715 , Loss : 0.9315941333770752, Weights 0.7852987051010132, bias : 0.3588021993637085  \n",
      "Steps : 716 , Loss : 0.9310625195503235, Weights 0.7857674956321716, bias : 0.35824325680732727  \n",
      "Steps : 717 , Loss : 0.930532693862915, Weights 0.786235511302948, bias : 0.3576853573322296  \n",
      "Steps : 718 , Loss : 0.9300047159194946, Weights 0.7867026925086975, bias : 0.3571285009384155  \n",
      "Steps : 719 , Loss : 0.9294787645339966, Weights 0.7871690988540649, bias : 0.356572687625885  \n",
      "Steps : 720 , Loss : 0.9289546012878418, Weights 0.7876347303390503, bias : 0.3560178875923157  \n",
      "Steps : 721 , Loss : 0.9284324049949646, Weights 0.7880995869636536, bias : 0.3554641306400299  \n",
      "Steps : 722 , Loss : 0.9279119968414307, Weights 0.78856360912323, bias : 0.3549114167690277  \n",
      "Steps : 723 , Loss : 0.9273935556411743, Weights 0.7890268564224243, bias : 0.3543597459793091  \n",
      "Steps : 724 , Loss : 0.926876962184906, Weights 0.7894893288612366, bias : 0.35380908846855164  \n",
      "Steps : 725 , Loss : 0.926362156867981, Weights 0.7899510264396667, bias : 0.35325947403907776  \n",
      "Steps : 726 , Loss : 0.925849199295044, Weights 0.7904119491577148, bias : 0.35271087288856506  \n",
      "Steps : 727 , Loss : 0.9253380298614502, Weights 0.7908720970153809, bias : 0.35216328501701355  \n",
      "Steps : 728 , Loss : 0.9248287081718445, Weights 0.7913314700126648, bias : 0.3516167402267456  \n",
      "Steps : 729 , Loss : 0.9243214130401611, Weights 0.7917900681495667, bias : 0.35107120871543884  \n",
      "Steps : 730 , Loss : 0.9238157868385315, Weights 0.7922478318214417, bias : 0.35052669048309326  \n",
      "Steps : 731 , Loss : 0.9233119487762451, Weights 0.7927048206329346, bias : 0.34998318552970886  \n",
      "Steps : 732 , Loss : 0.9228099584579468, Weights 0.7931610345840454, bias : 0.34944069385528564  \n",
      "Steps : 733 , Loss : 0.9223098158836365, Weights 0.793616533279419, bias : 0.3488992154598236  \n",
      "Steps : 734 , Loss : 0.9218112826347351, Weights 0.7940712571144104, bias : 0.34835875034332275  \n",
      "Steps : 735 , Loss : 0.9213147163391113, Weights 0.7945252060890198, bias : 0.3478192985057831  \n",
      "Steps : 736 , Loss : 0.9208199381828308, Weights 0.7949783802032471, bias : 0.3472808599472046  \n",
      "Steps : 737 , Loss : 0.920326828956604, Weights 0.7954307794570923, bias : 0.3467434048652649  \n",
      "Steps : 738 , Loss : 0.9198355078697205, Weights 0.7958824038505554, bias : 0.3462069630622864  \n",
      "Steps : 739 , Loss : 0.919346034526825, Weights 0.7963332533836365, bias : 0.34567153453826904  \n",
      "Steps : 740 , Loss : 0.9188581705093384, Weights 0.7967833280563354, bias : 0.3451370894908905  \n",
      "Steps : 741 , Loss : 0.9183722138404846, Weights 0.7972326874732971, bias : 0.34460365772247314  \n",
      "Steps : 742 , Loss : 0.9178878664970398, Weights 0.7976812720298767, bias : 0.3440712094306946  \n",
      "Steps : 743 , Loss : 0.9174052476882935, Weights 0.7981290817260742, bias : 0.3435397446155548  \n",
      "Steps : 744 , Loss : 0.9169245958328247, Weights 0.7985761165618896, bias : 0.3430092930793762  \n",
      "Steps : 745 , Loss : 0.9164454936981201, Weights 0.7990224361419678, bias : 0.3424798250198364  \n",
      "Steps : 746 , Loss : 0.9159680008888245, Weights 0.7994679808616638, bias : 0.3419513404369354  \n",
      "Steps : 747 , Loss : 0.9154924154281616, Weights 0.7999127507209778, bias : 0.3414238393306732  \n",
      "Steps : 748 , Loss : 0.9150184392929077, Weights 0.8003568053245544, bias : 0.3408973217010498  \n",
      "Steps : 749 , Loss : 0.9145461916923523, Weights 0.800800085067749, bias : 0.3403717875480652  \n",
      "Steps : 750 , Loss : 0.9140756130218506, Weights 0.8012425899505615, bias : 0.33984723687171936  \n",
      "Steps : 751 , Loss : 0.9136068224906921, Weights 0.8016843795776367, bias : 0.33932366967201233  \n",
      "Steps : 752 , Loss : 0.9131395816802979, Weights 0.8021253943443298, bias : 0.3388010859489441  \n",
      "Steps : 753 , Loss : 0.9126741886138916, Weights 0.8025656938552856, bias : 0.33827945590019226  \n",
      "Steps : 754 , Loss : 0.91221022605896, Weights 0.8030052185058594, bias : 0.3377588093280792  \n",
      "Steps : 755 , Loss : 0.9117480516433716, Weights 0.8034440279006958, bias : 0.337239146232605  \n",
      "Steps : 756 , Loss : 0.9112875461578369, Weights 0.8038820624351501, bias : 0.33672043681144714  \n",
      "Steps : 757 , Loss : 0.910828709602356, Weights 0.8043193817138672, bias : 0.3362027108669281  \n",
      "Steps : 758 , Loss : 0.9103714823722839, Weights 0.8047559261322021, bias : 0.33568593859672546  \n",
      "Steps : 759 , Loss : 0.9099159240722656, Weights 0.8051917552947998, bias : 0.33517012000083923  \n",
      "Steps : 760 , Loss : 0.9094619154930115, Weights 0.8056268095970154, bias : 0.3346552848815918  \n",
      "Steps : 761 , Loss : 0.909009575843811, Weights 0.8060611486434937, bias : 0.33414140343666077  \n",
      "Steps : 762 , Loss : 0.9085587859153748, Weights 0.8064947724342346, bias : 0.33362847566604614  \n",
      "Steps : 763 , Loss : 0.9081098437309265, Weights 0.8069276213645935, bias : 0.3331165015697479  \n",
      "Steps : 764 , Loss : 0.9076623320579529, Weights 0.8073597550392151, bias : 0.3326054811477661  \n",
      "Steps : 765 , Loss : 0.9072164297103882, Weights 0.8077911734580994, bias : 0.3320954144001007  \n",
      "Steps : 766 , Loss : 0.9067721366882324, Weights 0.8082218766212463, bias : 0.3315863013267517  \n",
      "Steps : 767 , Loss : 0.9063295125961304, Weights 0.8086518049240112, bias : 0.3310781419277191  \n",
      "Steps : 768 , Loss : 0.9058883190155029, Weights 0.8090810179710388, bias : 0.33057093620300293  \n",
      "Steps : 769 , Loss : 0.9054489135742188, Weights 0.8095095157623291, bias : 0.33006468415260315  \n",
      "Steps : 770 , Loss : 0.9050109386444092, Weights 0.8099372982978821, bias : 0.3295593559741974  \n",
      "Steps : 771 , Loss : 0.9045745730400085, Weights 0.8103643655776978, bias : 0.32905498147010803  \n",
      "Steps : 772 , Loss : 0.9041396975517273, Weights 0.8107906579971313, bias : 0.3285515606403351  \n",
      "Steps : 773 , Loss : 0.9037064909934998, Weights 0.8112162351608276, bias : 0.32804906368255615  \n",
      "Steps : 774 , Loss : 0.9032748341560364, Weights 0.8116410970687866, bias : 0.32754752039909363  \n",
      "Steps : 775 , Loss : 0.9028446674346924, Weights 0.8120652437210083, bias : 0.3270469009876251  \n",
      "Steps : 776 , Loss : 0.9024160504341125, Weights 0.8124886751174927, bias : 0.32654720544815063  \n",
      "Steps : 777 , Loss : 0.9019890427589417, Weights 0.8129113912582397, bias : 0.32604846358299255  \n",
      "Steps : 778 , Loss : 0.9015635848045349, Weights 0.8133333921432495, bias : 0.3255506455898285  \n",
      "Steps : 779 , Loss : 0.901139497756958, Weights 0.813754677772522, bias : 0.32505375146865845  \n",
      "Steps : 780 , Loss : 0.9007170796394348, Weights 0.8141752481460571, bias : 0.3245577812194824  \n",
      "Steps : 781 , Loss : 0.9002960920333862, Weights 0.814595103263855, bias : 0.3240627348423004  \n",
      "Steps : 782 , Loss : 0.8998765349388123, Weights 0.8150142431259155, bias : 0.3235686123371124  \n",
      "Steps : 783 , Loss : 0.8994587063789368, Weights 0.8154326677322388, bias : 0.32307541370391846  \n",
      "Steps : 784 , Loss : 0.8990421891212463, Weights 0.8158503770828247, bias : 0.3225831389427185  \n",
      "Steps : 785 , Loss : 0.8986271619796753, Weights 0.8162673711776733, bias : 0.3220917880535126  \n",
      "Steps : 786 , Loss : 0.898213803768158, Weights 0.8166836500167847, bias : 0.32160133123397827  \n",
      "Steps : 787 , Loss : 0.8978018164634705, Weights 0.8170992732048035, bias : 0.321111798286438  \n",
      "Steps : 788 , Loss : 0.8973913788795471, Weights 0.817514181137085, bias : 0.3206231892108917  \n",
      "Steps : 789 , Loss : 0.8969822525978088, Weights 0.8179283738136292, bias : 0.3201354742050171  \n",
      "Steps : 790 , Loss : 0.8965746164321899, Weights 0.818341851234436, bias : 0.3196486830711365  \n",
      "Steps : 791 , Loss : 0.8961685299873352, Weights 0.8187546133995056, bias : 0.3191627860069275  \n",
      "Steps : 792 , Loss : 0.8957638740539551, Weights 0.8191667199134827, bias : 0.3186778128147125  \n",
      "Steps : 793 , Loss : 0.8953606486320496, Weights 0.8195781111717224, bias : 0.3181937336921692  \n",
      "Steps : 794 , Loss : 0.8949588537216187, Weights 0.8199887871742249, bias : 0.3177105486392975  \n",
      "Steps : 795 , Loss : 0.8945585489273071, Weights 0.8203988075256348, bias : 0.3172282874584198  \n",
      "Steps : 796 , Loss : 0.8941596746444702, Weights 0.8208081126213074, bias : 0.31674692034721375  \n",
      "Steps : 797 , Loss : 0.8937622308731079, Weights 0.8212167024612427, bias : 0.3162664473056793  \n",
      "Steps : 798 , Loss : 0.8933662176132202, Weights 0.8216246366500854, bias : 0.31578686833381653  \n",
      "Steps : 799 , Loss : 0.8929715156555176, Weights 0.8220318555831909, bias : 0.31530818343162537  \n",
      "Steps : 800 , Loss : 0.8925783634185791, Weights 0.8224383592605591, bias : 0.31483039259910583  \n",
      "Steps : 801 , Loss : 0.8921865820884705, Weights 0.8228442072868347, bias : 0.31435349583625793  \n",
      "Steps : 802 , Loss : 0.8917962908744812, Weights 0.823249340057373, bias : 0.31387749314308167  \n",
      "Steps : 803 , Loss : 0.8914072513580322, Weights 0.8236538171768188, bias : 0.31340235471725464  \n",
      "Steps : 804 , Loss : 0.8910195231437683, Weights 0.8240575790405273, bias : 0.31292811036109924  \n",
      "Steps : 805 , Loss : 0.8906334042549133, Weights 0.8244606852531433, bias : 0.3124547600746155  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 806 , Loss : 0.8902486562728882, Weights 0.824863076210022, bias : 0.31198227405548096  \n",
      "Steps : 807 , Loss : 0.8898651599884033, Weights 0.8252648115158081, bias : 0.31151068210601807  \n",
      "Steps : 808 , Loss : 0.8894830346107483, Weights 0.8256658315658569, bias : 0.3110399842262268  \n",
      "Steps : 809 , Loss : 0.8891023993492126, Weights 0.8260661959648132, bias : 0.3105701506137848  \n",
      "Steps : 810 , Loss : 0.8887230157852173, Weights 0.826465904712677, bias : 0.310101181268692  \n",
      "Steps : 811 , Loss : 0.8883451819419861, Weights 0.8268648982048035, bias : 0.3096331059932709  \n",
      "Steps : 812 , Loss : 0.8879684209823608, Weights 0.8272632360458374, bias : 0.309165894985199  \n",
      "Steps : 813 , Loss : 0.887593150138855, Weights 0.8276609182357788, bias : 0.3086995482444763  \n",
      "Steps : 814 , Loss : 0.8872193694114685, Weights 0.8280578851699829, bias : 0.3082340657711029  \n",
      "Steps : 815 , Loss : 0.8868467211723328, Weights 0.8284541964530945, bias : 0.3077694773674011  \n",
      "Steps : 816 , Loss : 0.8864755034446716, Weights 0.8288498520851135, bias : 0.3073057532310486  \n",
      "Steps : 817 , Loss : 0.8861056566238403, Weights 0.8292447924613953, bias : 0.3068428933620453  \n",
      "Steps : 818 , Loss : 0.8857370018959045, Weights 0.8296390771865845, bias : 0.30638089776039124  \n",
      "Steps : 819 , Loss : 0.8853697180747986, Weights 0.8300327062606812, bias : 0.30591973662376404  \n",
      "Steps : 820 , Loss : 0.8850038051605225, Weights 0.8304256796836853, bias : 0.3054594397544861  \n",
      "Steps : 821 , Loss : 0.8846391439437866, Weights 0.8308179974555969, bias : 0.3050000071525574  \n",
      "Steps : 822 , Loss : 0.8842757940292358, Weights 0.8312095999717712, bias : 0.3045414388179779  \n",
      "Steps : 823 , Loss : 0.8839138746261597, Weights 0.831600546836853, bias : 0.3040837347507477  \n",
      "Steps : 824 , Loss : 0.8835530877113342, Weights 0.8319908380508423, bias : 0.3036268651485443  \n",
      "Steps : 825 , Loss : 0.8831936717033386, Weights 0.832380473613739, bias : 0.3031708598136902  \n",
      "Steps : 826 , Loss : 0.8828354477882385, Weights 0.8327694535255432, bias : 0.3027156889438629  \n",
      "Steps : 827 , Loss : 0.882478654384613, Weights 0.8331577777862549, bias : 0.3022613823413849  \n",
      "Steps : 828 , Loss : 0.8821229338645935, Weights 0.833545446395874, bias : 0.3018079102039337  \n",
      "Steps : 829 , Loss : 0.8817686438560486, Weights 0.8339324593544006, bias : 0.3013553023338318  \n",
      "Steps : 830 , Loss : 0.881415605545044, Weights 0.8343188166618347, bias : 0.3009035289287567  \n",
      "Steps : 831 , Loss : 0.8810638189315796, Weights 0.8347045183181763, bias : 0.3004525899887085  \n",
      "Steps : 832 , Loss : 0.8807132840156555, Weights 0.8350895643234253, bias : 0.30000248551368713  \n",
      "Steps : 833 , Loss : 0.8803640007972717, Weights 0.8354739546775818, bias : 0.2995532155036926  \n",
      "Steps : 834 , Loss : 0.8800159692764282, Weights 0.8358576893806458, bias : 0.29910480976104736  \n",
      "Steps : 835 , Loss : 0.879669189453125, Weights 0.8362407684326172, bias : 0.29865723848342896  \n",
      "Steps : 836 , Loss : 0.8793237209320068, Weights 0.8366231918334961, bias : 0.2982105016708374  \n",
      "Steps : 837 , Loss : 0.878979504108429, Weights 0.8370049595832825, bias : 0.2977645993232727  \n",
      "Steps : 838 , Loss : 0.8786364793777466, Weights 0.8373860716819763, bias : 0.2973195016384125  \n",
      "Steps : 839 , Loss : 0.8782945275306702, Weights 0.8377665281295776, bias : 0.2968752384185791  \n",
      "Steps : 840 , Loss : 0.8779540061950684, Weights 0.8381463289260864, bias : 0.2964318096637726  \n",
      "Steps : 841 , Loss : 0.8776147365570068, Weights 0.8385255336761475, bias : 0.2959892153739929  \n",
      "Steps : 842 , Loss : 0.8772764801979065, Weights 0.838904082775116, bias : 0.2955474257469177  \n",
      "Steps : 843 , Loss : 0.8769395351409912, Weights 0.8392819762229919, bias : 0.2951064705848694  \n",
      "Steps : 844 , Loss : 0.8766038417816162, Weights 0.8396592140197754, bias : 0.2946663498878479  \n",
      "Steps : 845 , Loss : 0.8762692213058472, Weights 0.8400358557701111, bias : 0.2942270338535309  \n",
      "Steps : 846 , Loss : 0.8759359121322632, Weights 0.8404118418693542, bias : 0.2937885522842407  \n",
      "Steps : 847 , Loss : 0.8756037950515747, Weights 0.8407871723175049, bias : 0.29335087537765503  \n",
      "Steps : 848 , Loss : 0.8752729296684265, Weights 0.841161847114563, bias : 0.2929140031337738  \n",
      "Steps : 849 , Loss : 0.8749430775642395, Weights 0.8415359258651733, bias : 0.29247796535491943  \n",
      "Steps : 850 , Loss : 0.8746145367622375, Weights 0.8419093489646912, bias : 0.29204273223876953  \n",
      "Steps : 851 , Loss : 0.8742871284484863, Weights 0.8422821164131165, bias : 0.2916083037853241  \n",
      "Steps : 852 , Loss : 0.8739608526229858, Weights 0.842654287815094, bias : 0.29117467999458313  \n",
      "Steps : 853 , Loss : 0.8736358880996704, Weights 0.843025803565979, bias : 0.290741890668869  \n",
      "Steps : 854 , Loss : 0.8733119368553162, Weights 0.8433967232704163, bias : 0.2903099060058594  \n",
      "Steps : 855 , Loss : 0.8729891777038574, Weights 0.843766987323761, bias : 0.2898787260055542  \n",
      "Steps : 856 , Loss : 0.8726676106452942, Weights 0.844136655330658, bias : 0.2894483506679535  \n",
      "Steps : 857 , Loss : 0.8723471164703369, Weights 0.8445056676864624, bias : 0.28901877999305725  \n",
      "Steps : 858 , Loss : 0.8720278143882751, Weights 0.8448740839958191, bias : 0.2885899841785431  \n",
      "Steps : 859 , Loss : 0.8717097640037537, Weights 0.8452418446540833, bias : 0.2881619930267334  \n",
      "Steps : 860 , Loss : 0.8713926672935486, Weights 0.8456090092658997, bias : 0.2877348065376282  \n",
      "Steps : 861 , Loss : 0.8710768818855286, Weights 0.8459755182266235, bias : 0.2873084247112274  \n",
      "Steps : 862 , Loss : 0.8707621097564697, Weights 0.8463414311408997, bias : 0.28688281774520874  \n",
      "Steps : 863 , Loss : 0.8704485893249512, Weights 0.8467066884040833, bias : 0.28645801544189453  \n",
      "Steps : 864 , Loss : 0.8701360821723938, Weights 0.8470713496208191, bias : 0.2860339879989624  \n",
      "Steps : 865 , Loss : 0.8698247671127319, Weights 0.8474353551864624, bias : 0.28561076521873474  \n",
      "Steps : 866 , Loss : 0.8695144653320312, Weights 0.847798764705658, bias : 0.28518831729888916  \n",
      "Steps : 867 , Loss : 0.8692054152488708, Weights 0.8481615781784058, bias : 0.28476667404174805  \n",
      "Steps : 868 , Loss : 0.8688973188400269, Weights 0.848523736000061, bias : 0.284345805644989  \n",
      "Steps : 869 , Loss : 0.8685904145240784, Weights 0.8488852977752686, bias : 0.28392574191093445  \n",
      "Steps : 870 , Loss : 0.8682845830917358, Weights 0.8492462635040283, bias : 0.28350645303726196  \n",
      "Steps : 871 , Loss : 0.867979884147644, Weights 0.8496065735816956, bias : 0.28308793902397156  \n",
      "Steps : 872 , Loss : 0.8676762580871582, Weights 0.849966287612915, bias : 0.28267019987106323  \n",
      "Steps : 873 , Loss : 0.8673738241195679, Weights 0.8503254055976868, bias : 0.282253235578537  \n",
      "Steps : 874 , Loss : 0.8670722842216492, Weights 0.8506839275360107, bias : 0.2818370461463928  \n",
      "Steps : 875 , Loss : 0.8667718768119812, Weights 0.8510417938232422, bias : 0.28142163157463074  \n",
      "Steps : 876 , Loss : 0.8664725422859192, Weights 0.8513990640640259, bias : 0.28100699186325073  \n",
      "Steps : 877 , Loss : 0.8661743402481079, Weights 0.8517557382583618, bias : 0.2805931270122528  \n",
      "Steps : 878 , Loss : 0.8658771514892578, Weights 0.85211181640625, bias : 0.28018003702163696  \n",
      "Steps : 879 , Loss : 0.8655810356140137, Weights 0.8524672985076904, bias : 0.2797677218914032  \n",
      "Steps : 880 , Loss : 0.865286111831665, Weights 0.8528221845626831, bias : 0.2793561816215515  \n",
      "Steps : 881 , Loss : 0.8649919629096985, Weights 0.853176474571228, bias : 0.2789453864097595  \n",
      "Steps : 882 , Loss : 0.864699125289917, Weights 0.8535301089286804, bias : 0.2785353660583496  \n",
      "Steps : 883 , Loss : 0.8644073009490967, Weights 0.8538831472396851, bias : 0.2781261205673218  \n",
      "Steps : 884 , Loss : 0.8641164302825928, Weights 0.8542355895042419, bias : 0.27771762013435364  \n",
      "Steps : 885 , Loss : 0.86382657289505, Weights 0.8545874357223511, bias : 0.2773098945617676  \n",
      "Steps : 886 , Loss : 0.8635379076004028, Weights 0.8549386858940125, bias : 0.2769029140472412  \n",
      "Steps : 887 , Loss : 0.8632502555847168, Weights 0.8552893400192261, bias : 0.2764967083930969  \n",
      "Steps : 888 , Loss : 0.8629634976387024, Weights 0.8556393980979919, bias : 0.27609124779701233  \n",
      "Steps : 889 , Loss : 0.8626779317855835, Weights 0.8559888601303101, bias : 0.2756865620613098  \n",
      "Steps : 890 , Loss : 0.8623932003974915, Weights 0.8563377261161804, bias : 0.275282621383667  \n",
      "Steps : 891 , Loss : 0.8621096014976501, Weights 0.856685996055603, bias : 0.27487942576408386  \n",
      "Steps : 892 , Loss : 0.86182701587677, Weights 0.8570336699485779, bias : 0.2744769752025604  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 893 , Loss : 0.8615453839302063, Weights 0.8573808073997498, bias : 0.2740752696990967  \n",
      "Steps : 894 , Loss : 0.8612648248672485, Weights 0.8577273488044739, bias : 0.273674339056015  \n",
      "Steps : 895 , Loss : 0.8609851002693176, Weights 0.8580732941627502, bias : 0.27327415347099304  \n",
      "Steps : 896 , Loss : 0.860706627368927, Weights 0.8584186434745789, bias : 0.27287471294403076  \n",
      "Steps : 897 , Loss : 0.860429048538208, Weights 0.8587633967399597, bias : 0.2724760174751282  \n",
      "Steps : 898 , Loss : 0.8601526021957397, Weights 0.8591075539588928, bias : 0.2720780670642853  \n",
      "Steps : 899 , Loss : 0.8598769903182983, Weights 0.8594511151313782, bias : 0.2716808319091797  \n",
      "Steps : 900 , Loss : 0.8596023321151733, Weights 0.8597941398620605, bias : 0.2712843418121338  \n",
      "Steps : 901 , Loss : 0.8593286871910095, Weights 0.8601365685462952, bias : 0.2708885967731476  \n",
      "Steps : 902 , Loss : 0.8590561151504517, Weights 0.860478401184082, bias : 0.27049359679222107  \n",
      "Steps : 903 , Loss : 0.8587844371795654, Weights 0.8608196377754211, bias : 0.27009931206703186  \n",
      "Steps : 904 , Loss : 0.8585137724876404, Weights 0.8611603379249573, bias : 0.26970577239990234  \n",
      "Steps : 905 , Loss : 0.858244001865387, Weights 0.8615004420280457, bias : 0.2693129777908325  \n",
      "Steps : 906 , Loss : 0.8579753041267395, Weights 0.8618399500846863, bias : 0.2689208984375  \n",
      "Steps : 907 , Loss : 0.8577075004577637, Weights 0.8621788620948792, bias : 0.2685295641422272  \n",
      "Steps : 908 , Loss : 0.8574406504631042, Weights 0.862517237663269, bias : 0.26813894510269165  \n",
      "Steps : 909 , Loss : 0.857174813747406, Weights 0.8628550171852112, bias : 0.2677490711212158  \n",
      "Steps : 910 , Loss : 0.8569098711013794, Weights 0.8631922602653503, bias : 0.2673599123954773  \n",
      "Steps : 911 , Loss : 0.8566458821296692, Weights 0.8635289072990417, bias : 0.2669714689254761  \n",
      "Steps : 912 , Loss : 0.8563828468322754, Weights 0.8638649582862854, bias : 0.26658377051353455  \n",
      "Steps : 913 , Loss : 0.856120765209198, Weights 0.8642004728317261, bias : 0.2661967873573303  \n",
      "Steps : 914 , Loss : 0.8558595776557922, Weights 0.864535391330719, bias : 0.2658105194568634  \n",
      "Steps : 915 , Loss : 0.8555993437767029, Weights 0.8648697733879089, bias : 0.2654249668121338  \n",
      "Steps : 916 , Loss : 0.8553401231765747, Weights 0.8652035593986511, bias : 0.2650401294231415  \n",
      "Steps : 917 , Loss : 0.8550817370414734, Weights 0.8655368089675903, bias : 0.2646560072898865  \n",
      "Steps : 918 , Loss : 0.8548243641853333, Weights 0.8658694624900818, bias : 0.2642726004123688  \n",
      "Steps : 919 , Loss : 0.8545678853988647, Weights 0.8662015795707703, bias : 0.2638899087905884  \n",
      "Steps : 920 , Loss : 0.8543123006820679, Weights 0.866533100605011, bias : 0.2635079324245453  \n",
      "Steps : 921 , Loss : 0.8540575504302979, Weights 0.8668640851974487, bias : 0.2631266713142395  \n",
      "Steps : 922 , Loss : 0.8538038730621338, Weights 0.8671944737434387, bias : 0.262746125459671  \n",
      "Steps : 923 , Loss : 0.8535509705543518, Weights 0.8675243258476257, bias : 0.26236629486083984  \n",
      "Steps : 924 , Loss : 0.8532990217208862, Weights 0.867853581905365, bias : 0.2619871497154236  \n",
      "Steps : 925 , Loss : 0.8530479669570923, Weights 0.8681823015213013, bias : 0.26160871982574463  \n",
      "Steps : 926 , Loss : 0.8527976870536804, Weights 0.8685104846954346, bias : 0.261231005191803  \n",
      "Steps : 927 , Loss : 0.8525485992431641, Weights 0.8688380718231201, bias : 0.26085397601127625  \n",
      "Steps : 928 , Loss : 0.8523002862930298, Weights 0.8691651225090027, bias : 0.2604776620864868  \n",
      "Steps : 929 , Loss : 0.8520527482032776, Weights 0.8694916367530823, bias : 0.2601020336151123  \n",
      "Steps : 930 , Loss : 0.8518061637878418, Weights 0.8698175549507141, bias : 0.2597271203994751  \n",
      "Steps : 931 , Loss : 0.8515605926513672, Weights 0.870142936706543, bias : 0.2593528926372528  \n",
      "Steps : 932 , Loss : 0.8513156771659851, Weights 0.8704677820205688, bias : 0.2589793801307678  \n",
      "Steps : 933 , Loss : 0.8510717749595642, Weights 0.870792031288147, bias : 0.25860655307769775  \n",
      "Steps : 934 , Loss : 0.8508287072181702, Weights 0.8711157441139221, bias : 0.2582344114780426  \n",
      "Steps : 935 , Loss : 0.850586473941803, Weights 0.8714389204978943, bias : 0.25786295533180237  \n",
      "Steps : 936 , Loss : 0.8503451347351074, Weights 0.8717615604400635, bias : 0.25749221444129944  \n",
      "Steps : 937 , Loss : 0.8501047492027283, Weights 0.8720836639404297, bias : 0.2571221590042114  \n",
      "Steps : 938 , Loss : 0.8498651385307312, Weights 0.8724051713943481, bias : 0.25675278902053833  \n",
      "Steps : 939 , Loss : 0.8496263027191162, Weights 0.8727261424064636, bias : 0.25638410449028015  \n",
      "Steps : 940 , Loss : 0.8493884205818176, Weights 0.8730465769767761, bias : 0.2560161054134369  \n",
      "Steps : 941 , Loss : 0.8491514325141907, Weights 0.8733664751052856, bias : 0.25564879179000854  \n",
      "Steps : 942 , Loss : 0.8489152789115906, Weights 0.8736858367919922, bias : 0.2552821636199951  \n",
      "Steps : 943 , Loss : 0.8486799597740173, Weights 0.8740046620368958, bias : 0.2549162209033966  \n",
      "Steps : 944 , Loss : 0.8484454154968262, Weights 0.8743229508399963, bias : 0.254550963640213  \n",
      "Steps : 945 , Loss : 0.8482118248939514, Weights 0.8746406435966492, bias : 0.25418639183044434  \n",
      "Steps : 946 , Loss : 0.8479788899421692, Weights 0.874957799911499, bias : 0.2538224756717682  \n",
      "Steps : 947 , Loss : 0.8477470278739929, Weights 0.8752744197845459, bias : 0.25345924496650696  \n",
      "Steps : 948 , Loss : 0.8475158214569092, Weights 0.8755905032157898, bias : 0.25309669971466064  \n",
      "Steps : 949 , Loss : 0.8472855091094971, Weights 0.8759060502052307, bias : 0.25273481011390686  \n",
      "Steps : 950 , Loss : 0.847055971622467, Weights 0.8762210607528687, bias : 0.252373605966568  \n",
      "Steps : 951 , Loss : 0.8468272686004639, Weights 0.8765355348587036, bias : 0.25201308727264404  \n",
      "Steps : 952 , Loss : 0.8465995788574219, Weights 0.8768494725227356, bias : 0.2516532242298126  \n",
      "Steps : 953 , Loss : 0.8463723659515381, Weights 0.8771628737449646, bias : 0.2512940466403961  \n",
      "Steps : 954 , Loss : 0.8461461663246155, Weights 0.8774757385253906, bias : 0.25093552470207214  \n",
      "Steps : 955 , Loss : 0.8459208607673645, Weights 0.8777880668640137, bias : 0.2505776584148407  \n",
      "Steps : 956 , Loss : 0.845696210861206, Weights 0.8780999183654785, bias : 0.25022047758102417  \n",
      "Steps : 957 , Loss : 0.8454722762107849, Weights 0.8784112334251404, bias : 0.24986395239830017  \n",
      "Steps : 958 , Loss : 0.845249354839325, Weights 0.8787220120429993, bias : 0.2495080977678299  \n",
      "Steps : 959 , Loss : 0.8450270891189575, Weights 0.8790322542190552, bias : 0.24915289878845215  \n",
      "Steps : 960 , Loss : 0.8448057770729065, Weights 0.8793419599533081, bias : 0.24879835546016693  \n",
      "Steps : 961 , Loss : 0.844585120677948, Weights 0.8796511292457581, bias : 0.24844448268413544  \n",
      "Steps : 962 , Loss : 0.8443652391433716, Weights 0.879959762096405, bias : 0.24809126555919647  \n",
      "Steps : 963 , Loss : 0.8441462516784668, Weights 0.8802679181098938, bias : 0.24773870408535004  \n",
      "Steps : 964 , Loss : 0.8439279198646545, Weights 0.8805755376815796, bias : 0.24738679826259613  \n",
      "Steps : 965 , Loss : 0.8437104821205139, Weights 0.8808826208114624, bias : 0.24703554809093475  \n",
      "Steps : 966 , Loss : 0.8434937000274658, Weights 0.8811891674995422, bias : 0.2466849535703659  \n",
      "Steps : 967 , Loss : 0.8432778716087341, Weights 0.8814951777458191, bias : 0.2463350147008896  \n",
      "Steps : 968 , Loss : 0.8430626392364502, Weights 0.8818007111549377, bias : 0.2459857165813446  \n",
      "Steps : 969 , Loss : 0.8428483009338379, Weights 0.8821057081222534, bias : 0.24563707411289215  \n",
      "Steps : 970 , Loss : 0.8426346778869629, Weights 0.8824101686477661, bias : 0.24528908729553223  \n",
      "Steps : 971 , Loss : 0.8424217104911804, Weights 0.8827141523361206, bias : 0.24494174122810364  \n",
      "Steps : 972 , Loss : 0.8422096967697144, Weights 0.8830175995826721, bias : 0.24459503591060638  \n",
      "Steps : 973 , Loss : 0.8419983386993408, Weights 0.8833205103874207, bias : 0.24424898624420166  \n",
      "Steps : 974 , Loss : 0.8417878746986389, Weights 0.883622944355011, bias : 0.24390357732772827  \n",
      "Steps : 975 , Loss : 0.8415780663490295, Weights 0.8839248418807983, bias : 0.24355880916118622  \n",
      "Steps : 976 , Loss : 0.8413690328598022, Weights 0.8842262029647827, bias : 0.2432146817445755  \n",
      "Steps : 977 , Loss : 0.8411605954170227, Weights 0.8845270872116089, bias : 0.24287119507789612  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 978 , Loss : 0.84095299243927, Weights 0.8848274350166321, bias : 0.24252834916114807  \n",
      "Steps : 979 , Loss : 0.8407461643218994, Weights 0.8851273059844971, bias : 0.24218614399433136  \n",
      "Steps : 980 , Loss : 0.8405401706695557, Weights 0.8854266405105591, bias : 0.24184457957744598  \n",
      "Steps : 981 , Loss : 0.8403347730636597, Weights 0.8857254981994629, bias : 0.24150364100933075  \n",
      "Steps : 982 , Loss : 0.8401301503181458, Weights 0.8860238194465637, bias : 0.24116334319114685  \n",
      "Steps : 983 , Loss : 0.8399262428283691, Weights 0.8863216042518616, bias : 0.2408236712217331  \n",
      "Steps : 984 , Loss : 0.8397231101989746, Weights 0.8866189122200012, bias : 0.24048464000225067  \n",
      "Steps : 985 , Loss : 0.8395206928253174, Weights 0.8869156837463379, bias : 0.2401462346315384  \n",
      "Steps : 986 , Loss : 0.8393189907073975, Weights 0.8872119784355164, bias : 0.23980845510959625  \n",
      "Steps : 987 , Loss : 0.8391180634498596, Weights 0.8875077962875366, bias : 0.23947130143642426  \n",
      "Steps : 988 , Loss : 0.8389176726341248, Weights 0.8878030776977539, bias : 0.2391347885131836  \n",
      "Steps : 989 , Loss : 0.8387181758880615, Weights 0.888097882270813, bias : 0.23879890143871307  \n",
      "Steps : 990 , Loss : 0.838519275188446, Weights 0.8883921504020691, bias : 0.2384636253118515  \n",
      "Steps : 991 , Loss : 0.8383212089538574, Weights 0.888685941696167, bias : 0.23812897503376007  \n",
      "Steps : 992 , Loss : 0.8381238579750061, Weights 0.8889791965484619, bias : 0.23779495060443878  \n",
      "Steps : 993 , Loss : 0.8379271030426025, Weights 0.8892719745635986, bias : 0.23746155202388763  \n",
      "Steps : 994 , Loss : 0.8377311825752258, Weights 0.8895642757415771, bias : 0.23712876439094543  \n",
      "Steps : 995 , Loss : 0.8375358581542969, Weights 0.8898560404777527, bias : 0.23679660260677338  \n",
      "Steps : 996 , Loss : 0.83734130859375, Weights 0.89014732837677, bias : 0.23646505177021027  \n",
      "Steps : 997 , Loss : 0.8371473550796509, Weights 0.8904381394386292, bias : 0.2361341267824173  \n",
      "Steps : 998 , Loss : 0.8369541168212891, Weights 0.8907284140586853, bias : 0.23580381274223328  \n",
      "Steps : 999 , Loss : 0.8367616534233093, Weights 0.8910182118415833, bias : 0.2354741096496582  \n",
      "Steps : 1000 , Loss : 0.8365697264671326, Weights 0.891307532787323, bias : 0.23514503240585327  \n",
      "Steps : 1001 , Loss : 0.8363786339759827, Weights 0.8915963768959045, bias : 0.2348165661096573  \n",
      "Steps : 1002 , Loss : 0.8361881971359253, Weights 0.8918846845626831, bias : 0.23448871076107025  \n",
      "Steps : 1003 , Loss : 0.83599853515625, Weights 0.8921725153923035, bias : 0.23416146636009216  \n",
      "Steps : 1004 , Loss : 0.8358093500137329, Weights 0.8924598693847656, bias : 0.23383481800556183  \n",
      "Steps : 1005 , Loss : 0.8356209397315979, Weights 0.8927467465400696, bias : 0.23350878059864044  \n",
      "Steps : 1006 , Loss : 0.8354331254959106, Weights 0.8930330872535706, bias : 0.233183354139328  \n",
      "Steps : 1007 , Loss : 0.8352461457252502, Weights 0.8933189511299133, bias : 0.2328585386276245  \n",
      "Steps : 1008 , Loss : 0.8350597620010376, Weights 0.8936043381690979, bias : 0.23253431916236877  \n",
      "Steps : 1009 , Loss : 0.8348740935325623, Weights 0.8938892483711243, bias : 0.23221071064472198  \n",
      "Steps : 1010 , Loss : 0.8346889019012451, Weights 0.8941736817359924, bias : 0.23188769817352295  \n",
      "Steps : 1011 , Loss : 0.8345045447349548, Weights 0.8944576382637024, bias : 0.23156528174877167  \n",
      "Steps : 1012 , Loss : 0.8343208432197571, Weights 0.8947411179542542, bias : 0.23124347627162933  \n",
      "Steps : 1013 , Loss : 0.8341376781463623, Weights 0.8950241208076477, bias : 0.23092226684093475  \n",
      "Steps : 1014 , Loss : 0.8339552283287048, Weights 0.8953065872192383, bias : 0.23060165345668793  \n",
      "Steps : 1015 , Loss : 0.8337735533714294, Weights 0.8955885767936707, bias : 0.23028163611888885  \n",
      "Steps : 1016 , Loss : 0.8335923552513123, Weights 0.8958700895309448, bias : 0.22996221482753754  \n",
      "Steps : 1017 , Loss : 0.8334119319915771, Weights 0.8961511254310608, bias : 0.22964338958263397  \n",
      "Steps : 1018 , Loss : 0.833232045173645, Weights 0.8964316844940186, bias : 0.22932514548301697  \n",
      "Steps : 1019 , Loss : 0.8330528140068054, Weights 0.8967117667198181, bias : 0.22900749742984772  \n",
      "Steps : 1020 , Loss : 0.8328742980957031, Weights 0.8969913721084595, bias : 0.22869044542312622  \n",
      "Steps : 1021 , Loss : 0.8326963782310486, Weights 0.8972705006599426, bias : 0.22837397456169128  \n",
      "Steps : 1022 , Loss : 0.8325191736221313, Weights 0.8975491523742676, bias : 0.2280580997467041  \n",
      "Steps : 1023 , Loss : 0.8323424458503723, Weights 0.8978273272514343, bias : 0.22774280607700348  \n",
      "Steps : 1024 , Loss : 0.8321664929389954, Weights 0.8981050252914429, bias : 0.2274281084537506  \n",
      "Steps : 1025 , Loss : 0.8319911360740662, Weights 0.8983822464942932, bias : 0.2271139919757843  \n",
      "Steps : 1026 , Loss : 0.8318164348602295, Weights 0.8986589908599854, bias : 0.22680045664310455  \n",
      "Steps : 1027 , Loss : 0.831642210483551, Weights 0.8989353179931641, bias : 0.22648750245571136  \n",
      "Steps : 1028 , Loss : 0.8314687609672546, Weights 0.8992111682891846, bias : 0.22617512941360474  \n",
      "Steps : 1029 , Loss : 0.8312958478927612, Weights 0.8994865417480469, bias : 0.22586333751678467  \n",
      "Steps : 1030 , Loss : 0.8311235308647156, Weights 0.899761438369751, bias : 0.22555212676525116  \n",
      "Steps : 1031 , Loss : 0.830951988697052, Weights 0.9000358581542969, bias : 0.2252414971590042  \n",
      "Steps : 1032 , Loss : 0.8307808637619019, Weights 0.9003098011016846, bias : 0.22493144869804382  \n",
      "Steps : 1033 , Loss : 0.8306105136871338, Weights 0.9005832672119141, bias : 0.22462198138237  \n",
      "Steps : 1034 , Loss : 0.8304407000541687, Weights 0.9008563160896301, bias : 0.22431308031082153  \n",
      "Steps : 1035 , Loss : 0.8302714824676514, Weights 0.901128888130188, bias : 0.22400476038455963  \n",
      "Steps : 1036 , Loss : 0.8301029205322266, Weights 0.9014009833335876, bias : 0.2236970067024231  \n",
      "Steps : 1037 , Loss : 0.8299348950386047, Weights 0.9016726016998291, bias : 0.22338983416557312  \n",
      "Steps : 1038 , Loss : 0.8297675251960754, Weights 0.9019437432289124, bias : 0.2230832278728485  \n",
      "Steps : 1039 , Loss : 0.8296007513999939, Weights 0.9022144675254822, bias : 0.22277718782424927  \n",
      "Steps : 1040 , Loss : 0.8294345140457153, Weights 0.9024847149848938, bias : 0.2224717140197754  \n",
      "Steps : 1041 , Loss : 0.8292689323425293, Weights 0.9027544856071472, bias : 0.22216682136058807  \n",
      "Steps : 1042 , Loss : 0.8291040062904358, Weights 0.9030238389968872, bias : 0.22186249494552612  \n",
      "Steps : 1043 , Loss : 0.8289394974708557, Weights 0.903292715549469, bias : 0.22155873477458954  \n",
      "Steps : 1044 , Loss : 0.8287757635116577, Weights 0.9035611152648926, bias : 0.22125552594661713  \n",
      "Steps : 1045 , Loss : 0.8286123871803284, Weights 0.9038290977478027, bias : 0.22095288336277008  \n",
      "Steps : 1046 , Loss : 0.8284497261047363, Weights 0.9040966033935547, bias : 0.2206508070230484  \n",
      "Steps : 1047 , Loss : 0.8282877206802368, Weights 0.9043636322021484, bias : 0.2203492969274521  \n",
      "Steps : 1048 , Loss : 0.8281261324882507, Weights 0.9046302437782288, bias : 0.22004833817481995  \n",
      "Steps : 1049 , Loss : 0.827965259552002, Weights 0.9048963785171509, bias : 0.21974794566631317  \n",
      "Steps : 1050 , Loss : 0.8278049230575562, Weights 0.9051620364189148, bias : 0.21944810450077057  \n",
      "Steps : 1051 , Loss : 0.8276451230049133, Weights 0.9054272770881653, bias : 0.21914882957935333  \n",
      "Steps : 1052 , Loss : 0.8274860382080078, Weights 0.9056920409202576, bias : 0.21885010600090027  \n",
      "Steps : 1053 , Loss : 0.8273272514343262, Weights 0.9059563875198364, bias : 0.21855193376541138  \n",
      "Steps : 1054 , Loss : 0.8271692395210266, Weights 0.9062202572822571, bias : 0.21825432777404785  \n",
      "Steps : 1055 , Loss : 0.8270118236541748, Weights 0.9064836502075195, bias : 0.2179572731256485  \n",
      "Steps : 1056 , Loss : 0.8268547654151917, Weights 0.9067466259002686, bias : 0.21766076982021332  \n",
      "Steps : 1057 , Loss : 0.8266984820365906, Weights 0.9070091247558594, bias : 0.2173648178577423  \n",
      "Steps : 1058 , Loss : 0.8265426754951477, Weights 0.9072712063789368, bias : 0.21706941723823547  \n",
      "Steps : 1059 , Loss : 0.8263874650001526, Weights 0.907532811164856, bias : 0.2167745679616928  \n",
      "Steps : 1060 , Loss : 0.8262327313423157, Weights 0.9077939987182617, bias : 0.21648025512695312  \n",
      "Steps : 1061 , Loss : 0.8260786533355713, Weights 0.908054769039154, bias : 0.2161864936351776  \n",
      "Steps : 1062 , Loss : 0.8259249925613403, Weights 0.9083150625228882, bias : 0.21589328348636627  \n",
      "Steps : 1063 , Loss : 0.8257719278335571, Weights 0.9085749387741089, bias : 0.2156006097793579  \n",
      "Steps : 1064 , Loss : 0.8256194591522217, Weights 0.9088343381881714, bias : 0.21530848741531372  \n",
      "Steps : 1065 , Loss : 0.8254675269126892, Weights 0.9090933203697205, bias : 0.2150169014930725  \n",
      "Steps : 1066 , Loss : 0.8253160715103149, Weights 0.9093518257141113, bias : 0.21472586691379547  \n",
      "Steps : 1067 , Loss : 0.8251652717590332, Weights 0.9096099138259888, bias : 0.2144353687763214  \n",
      "Steps : 1068 , Loss : 0.8250149488449097, Weights 0.9098675847053528, bias : 0.21414540708065033  \n",
      "Steps : 1069 , Loss : 0.8248651027679443, Weights 0.9101247787475586, bias : 0.21385598182678223  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1070 , Loss : 0.8247158527374268, Weights 0.910381555557251, bias : 0.2135670930147171  \n",
      "Steps : 1071 , Loss : 0.8245671987533569, Weights 0.9106379151344299, bias : 0.21327875554561615  \n",
      "Steps : 1072 , Loss : 0.8244189620018005, Weights 0.9108937978744507, bias : 0.21299093961715698  \n",
      "Steps : 1073 , Loss : 0.8242713809013367, Weights 0.911149263381958, bias : 0.2127036601305008  \n",
      "Steps : 1074 , Loss : 0.8241241574287415, Weights 0.9114043116569519, bias : 0.21241691708564758  \n",
      "Steps : 1075 , Loss : 0.8239775896072388, Weights 0.9116588830947876, bias : 0.21213071048259735  \n",
      "Steps : 1076 , Loss : 0.8238314986228943, Weights 0.9119130373001099, bias : 0.2118450403213501  \n",
      "Steps : 1077 , Loss : 0.823685884475708, Weights 0.9121667742729187, bias : 0.21155989170074463  \n",
      "Steps : 1078 , Loss : 0.8235409259796143, Weights 0.9124200344085693, bias : 0.21127527952194214  \n",
      "Steps : 1079 , Loss : 0.8233964443206787, Weights 0.9126728773117065, bias : 0.21099118888378143  \n",
      "Steps : 1080 , Loss : 0.8232524394989014, Weights 0.9129253029823303, bias : 0.2107076346874237  \n",
      "Steps : 1081 , Loss : 0.823108971118927, Weights 0.9131773114204407, bias : 0.21042460203170776  \n",
      "Steps : 1082 , Loss : 0.8229659795761108, Weights 0.9134289026260376, bias : 0.2101420909166336  \n",
      "Steps : 1083 , Loss : 0.8228234648704529, Weights 0.9136800169944763, bias : 0.20986011624336243  \n",
      "Steps : 1084 , Loss : 0.8226816654205322, Weights 0.9139307141304016, bias : 0.20957866311073303  \n",
      "Steps : 1085 , Loss : 0.8225401043891907, Weights 0.9141809940338135, bias : 0.20929773151874542  \n",
      "Steps : 1086 , Loss : 0.8223991990089417, Weights 0.9144308567047119, bias : 0.2090173214673996  \n",
      "Steps : 1087 , Loss : 0.8222587704658508, Weights 0.9146803021430969, bias : 0.20873743295669556  \n",
      "Steps : 1088 , Loss : 0.8221189975738525, Weights 0.9149293303489685, bias : 0.2084580659866333  \n",
      "Steps : 1089 , Loss : 0.8219793438911438, Weights 0.9151778817176819, bias : 0.20817922055721283  \n",
      "Steps : 1090 , Loss : 0.8218405246734619, Weights 0.9154260158538818, bias : 0.20790089666843414  \n",
      "Steps : 1091 , Loss : 0.8217021226882935, Weights 0.9156737327575684, bias : 0.20762307941913605  \n",
      "Steps : 1092 , Loss : 0.8215641975402832, Weights 0.9159210324287415, bias : 0.20734578371047974  \n",
      "Steps : 1093 , Loss : 0.8214266896247864, Weights 0.9161679148674011, bias : 0.20706899464130402  \n",
      "Steps : 1094 , Loss : 0.8212898373603821, Weights 0.9164143800735474, bias : 0.20679272711277008  \n",
      "Steps : 1095 , Loss : 0.8211533427238464, Weights 0.9166604280471802, bias : 0.20651696622371674  \n",
      "Steps : 1096 , Loss : 0.8210173845291138, Weights 0.9169060587882996, bias : 0.20624172687530518  \n",
      "Steps : 1097 , Loss : 0.8208819627761841, Weights 0.9171512722969055, bias : 0.2059669941663742  \n",
      "Steps : 1098 , Loss : 0.820746898651123, Weights 0.917396068572998, bias : 0.20569276809692383  \n",
      "Steps : 1099 , Loss : 0.8206123113632202, Weights 0.9176404476165771, bias : 0.20541906356811523  \n",
      "Steps : 1100 , Loss : 0.8204783797264099, Weights 0.9178844094276428, bias : 0.20514586567878723  \n",
      "Steps : 1101 , Loss : 0.820344865322113, Weights 0.9181279540061951, bias : 0.20487317442893982  \n",
      "Steps : 1102 , Loss : 0.8202117681503296, Weights 0.9183710813522339, bias : 0.204600989818573  \n",
      "Steps : 1103 , Loss : 0.8200790882110596, Weights 0.9186137914657593, bias : 0.20432931184768677  \n",
      "Steps : 1104 , Loss : 0.8199470043182373, Weights 0.9188560843467712, bias : 0.20405814051628113  \n",
      "Steps : 1105 , Loss : 0.8198153972625732, Weights 0.9190979599952698, bias : 0.20378746092319489  \n",
      "Steps : 1106 , Loss : 0.8196841478347778, Weights 0.9193394184112549, bias : 0.20351728796958923  \n",
      "Steps : 1107 , Loss : 0.8195533752441406, Weights 0.9195804595947266, bias : 0.20324762165546417  \n",
      "Steps : 1108 , Loss : 0.8194231390953064, Weights 0.9198210835456848, bias : 0.2029784619808197  \n",
      "Steps : 1109 , Loss : 0.8192934393882751, Weights 0.9200613498687744, bias : 0.20270979404449463  \n",
      "Steps : 1110 , Loss : 0.8191640377044678, Weights 0.9203011989593506, bias : 0.20244163274765015  \n",
      "Steps : 1111 , Loss : 0.8190352916717529, Weights 0.9205406308174133, bias : 0.20217396318912506  \n",
      "Steps : 1112 , Loss : 0.818906843662262, Weights 0.9207796454429626, bias : 0.20190678536891937  \n",
      "Steps : 1113 , Loss : 0.8187788128852844, Weights 0.9210182428359985, bias : 0.20164011418819427  \n",
      "Steps : 1114 , Loss : 0.8186514377593994, Weights 0.921256422996521, bias : 0.20137393474578857  \n",
      "Steps : 1115 , Loss : 0.8185244202613831, Weights 0.92149418592453, bias : 0.20110824704170227  \n",
      "Steps : 1116 , Loss : 0.8183978199958801, Weights 0.9217315912246704, bias : 0.20084305107593536  \n",
      "Steps : 1117 , Loss : 0.818271815776825, Weights 0.9219685792922974, bias : 0.20057834684848785  \n",
      "Steps : 1118 , Loss : 0.8181460499763489, Weights 0.9222051501274109, bias : 0.20031413435935974  \n",
      "Steps : 1119 , Loss : 0.8180208802223206, Weights 0.922441303730011, bias : 0.20005041360855103  \n",
      "Steps : 1120 , Loss : 0.8178959488868713, Weights 0.9226770997047424, bias : 0.1997871845960617  \n",
      "Steps : 1121 , Loss : 0.8177717328071594, Weights 0.9229124784469604, bias : 0.19952444732189178  \n",
      "Steps : 1122 , Loss : 0.8176478743553162, Weights 0.923147439956665, bias : 0.19926220178604126  \n",
      "Steps : 1123 , Loss : 0.8175245523452759, Weights 0.9233819842338562, bias : 0.19900043308734894  \n",
      "Steps : 1124 , Loss : 0.8174015283584595, Weights 0.9236161708831787, bias : 0.198739156126976  \n",
      "Steps : 1125 , Loss : 0.8172789216041565, Weights 0.9238499402999878, bias : 0.19847837090492249  \n",
      "Steps : 1126 , Loss : 0.8171568512916565, Weights 0.9240832924842834, bias : 0.19821806252002716  \n",
      "Steps : 1127 , Loss : 0.8170351386070251, Weights 0.9243162870407104, bias : 0.19795824587345123  \n",
      "Steps : 1128 , Loss : 0.816913902759552, Weights 0.924548864364624, bias : 0.1976989060640335  \n",
      "Steps : 1129 , Loss : 0.8167930841445923, Weights 0.9247810244560242, bias : 0.197440043091774  \n",
      "Steps : 1130 , Loss : 0.8166727423667908, Weights 0.9250128269195557, bias : 0.19718167185783386  \n",
      "Steps : 1131 , Loss : 0.8165527582168579, Weights 0.9252442121505737, bias : 0.19692377746105194  \n",
      "Steps : 1132 , Loss : 0.8164334297180176, Weights 0.9254751801490784, bias : 0.19666635990142822  \n",
      "Steps : 1133 , Loss : 0.816314160823822, Weights 0.9257057905197144, bias : 0.1964094191789627  \n",
      "Steps : 1134 , Loss : 0.816195547580719, Weights 0.9259359836578369, bias : 0.1961529552936554  \n",
      "Steps : 1135 , Loss : 0.8160772919654846, Weights 0.9261658191680908, bias : 0.1958969682455063  \n",
      "Steps : 1136 , Loss : 0.8159594535827637, Weights 0.9263952374458313, bias : 0.19564145803451538  \n",
      "Steps : 1137 , Loss : 0.8158421516418457, Weights 0.9266242384910583, bias : 0.19538642466068268  \n",
      "Steps : 1138 , Loss : 0.8157250881195068, Weights 0.9268528819084167, bias : 0.19513186812400818  \n",
      "Steps : 1139 , Loss : 0.8156085014343262, Weights 0.9270811080932617, bias : 0.1948777735233307  \n",
      "Steps : 1140 , Loss : 0.8154923915863037, Weights 0.927308976650238, bias : 0.1946241557598114  \n",
      "Steps : 1141 , Loss : 0.8153767585754395, Weights 0.9275364279747009, bias : 0.19437101483345032  \n",
      "Steps : 1142 , Loss : 0.8152613639831543, Weights 0.9277635216712952, bias : 0.19411833584308624  \n",
      "Steps : 1143 , Loss : 0.8151465654373169, Weights 0.927990198135376, bias : 0.19386613368988037  \n",
      "Steps : 1144 , Loss : 0.8150320649147034, Weights 0.9282165169715881, bias : 0.1936143934726715  \n",
      "Steps : 1145 , Loss : 0.8149179816246033, Weights 0.9284424185752869, bias : 0.19336311519145966  \n",
      "Steps : 1146 , Loss : 0.8148042559623718, Weights 0.9286679625511169, bias : 0.193112313747406  \n",
      "Steps : 1147 , Loss : 0.8146910071372986, Weights 0.9288930892944336, bias : 0.19286197423934937  \n",
      "Steps : 1148 , Loss : 0.814578115940094, Weights 0.9291178584098816, bias : 0.19261209666728973  \n",
      "Steps : 1149 , Loss : 0.8144657015800476, Weights 0.9293422102928162, bias : 0.1923626810312271  \n",
      "Steps : 1150 , Loss : 0.8143536448478699, Weights 0.9295662045478821, bias : 0.1921137273311615  \n",
      "Steps : 1151 , Loss : 0.8142420053482056, Weights 0.9297898411750793, bias : 0.1918652355670929  \n",
      "Steps : 1152 , Loss : 0.8141307830810547, Weights 0.9300130605697632, bias : 0.1916172057390213  \n",
      "Steps : 1153 , Loss : 0.8140199184417725, Weights 0.9302359223365784, bias : 0.19136963784694672  \n",
      "Steps : 1154 , Loss : 0.8139094710350037, Weights 0.9304583668708801, bias : 0.19112253189086914  \n",
      "Steps : 1155 , Loss : 0.8137994408607483, Weights 0.9306804537773132, bias : 0.19087588787078857  \n",
      "Steps : 1156 , Loss : 0.8136897087097168, Weights 0.9309021830558777, bias : 0.19062970578670502  \n",
      "Steps : 1157 , Loss : 0.8135804533958435, Weights 0.9311234951019287, bias : 0.19038397073745728  \n",
      "Steps : 1158 , Loss : 0.8134716153144836, Weights 0.9313444495201111, bias : 0.19013869762420654  \n",
      "Steps : 1159 , Loss : 0.8133630156517029, Weights 0.9315650463104248, bias : 0.18989388644695282  \n",
      "Steps : 1160 , Loss : 0.8132550120353699, Weights 0.9317852258682251, bias : 0.1896495223045349  \n",
      "Steps : 1161 , Loss : 0.8131474256515503, Weights 0.9320050477981567, bias : 0.189405620098114  \n",
      "Steps : 1162 , Loss : 0.8130399584770203, Weights 0.9322245121002197, bias : 0.18916216492652893  \n",
      "Steps : 1163 , Loss : 0.8129329085350037, Weights 0.9324436187744141, bias : 0.18891915678977966  \n",
      "Steps : 1164 , Loss : 0.81282639503479, Weights 0.932662308216095, bias : 0.1886766105890274  \n",
      "Steps : 1165 , Loss : 0.8127201795578003, Weights 0.9328806400299072, bias : 0.18843451142311096  \n",
      "Steps : 1166 , Loss : 0.8126144409179688, Weights 0.9330986142158508, bias : 0.18819285929203033  \n",
      "Steps : 1167 , Loss : 0.8125090003013611, Weights 0.9333162307739258, bias : 0.18795165419578552  \n",
      "Steps : 1168 , Loss : 0.8124039173126221, Weights 0.9335334300994873, bias : 0.18771089613437653  \n",
      "Steps : 1169 , Loss : 0.8122991919517517, Weights 0.9337502717971802, bias : 0.18747058510780334  \n",
      "Steps : 1170 , Loss : 0.8121950030326843, Weights 0.9339667558670044, bias : 0.18723072111606598  \n",
      "Steps : 1171 , Loss : 0.812091052532196, Weights 0.93418288230896, bias : 0.18699130415916443  \n",
      "Steps : 1172 , Loss : 0.8119874596595764, Weights 0.9343985915184021, bias : 0.1867523342370987  \n",
      "Steps : 1173 , Loss : 0.8118842840194702, Weights 0.9346139430999756, bias : 0.18651381134986877  \n",
      "Steps : 1174 , Loss : 0.8117814660072327, Weights 0.9348289370536804, bias : 0.18627572059631348  \n",
      "Steps : 1175 , Loss : 0.8116790056228638, Weights 0.9350435733795166, bias : 0.186038076877594  \n",
      "Steps : 1176 , Loss : 0.8115769028663635, Weights 0.9352578520774841, bias : 0.18580088019371033  \n",
      "Steps : 1177 , Loss : 0.8114752173423767, Weights 0.935471773147583, bias : 0.18556411564350128  \n",
      "Steps : 1178 , Loss : 0.8113738298416138, Weights 0.9356853365898132, bias : 0.18532779812812805  \n",
      "Steps : 1179 , Loss : 0.8112727999687195, Weights 0.93589848279953, bias : 0.18509191274642944  \n",
      "Steps : 1180 , Loss : 0.8111721873283386, Weights 0.9361112713813782, bias : 0.18485645949840546  \n",
      "Steps : 1181 , Loss : 0.8110719323158264, Weights 0.9363237023353577, bias : 0.18462145328521729  \n",
      "Steps : 1182 , Loss : 0.8109720349311829, Weights 0.9365357756614685, bias : 0.18438687920570374  \n",
      "Steps : 1183 , Loss : 0.810872495174408, Weights 0.9367474913597107, bias : 0.1841527372598648  \n",
      "Steps : 1184 , Loss : 0.8107733130455017, Weights 0.9369588494300842, bias : 0.1839190274477005  \n",
      "Steps : 1185 , Loss : 0.8106744289398193, Weights 0.9371698498725891, bias : 0.183685764670372  \n",
      "Steps : 1186 , Loss : 0.8105759620666504, Weights 0.9373804926872253, bias : 0.18345293402671814  \n",
      "Steps : 1187 , Loss : 0.8104778528213501, Weights 0.9375907778739929, bias : 0.1832205355167389  \n",
      "Steps : 1188 , Loss : 0.8103800415992737, Weights 0.9378007054328918, bias : 0.18298856914043427  \n",
      "Steps : 1189 , Loss : 0.8102825880050659, Weights 0.9380102753639221, bias : 0.18275701999664307  \n",
      "Steps : 1190 , Loss : 0.8101855516433716, Weights 0.9382194876670837, bias : 0.1825259029865265  \n",
      "Steps : 1191 , Loss : 0.8100887537002563, Weights 0.9384283423423767, bias : 0.18229521811008453  \n",
      "Steps : 1192 , Loss : 0.8099923729896545, Weights 0.938636839389801, bias : 0.1820649653673172  \n",
      "Steps : 1193 , Loss : 0.8098962306976318, Weights 0.9388449788093567, bias : 0.1818351298570633  \n",
      "Steps : 1194 , Loss : 0.8098005652427673, Weights 0.9390527606010437, bias : 0.181605726480484  \n",
      "Steps : 1195 , Loss : 0.8097051978111267, Weights 0.9392601847648621, bias : 0.18137675523757935  \n",
      "Steps : 1196 , Loss : 0.8096101880073547, Weights 0.9394672513008118, bias : 0.1811482012271881  \n",
      "Steps : 1197 , Loss : 0.8095154762268066, Weights 0.9396739602088928, bias : 0.1809200793504715  \n",
      "Steps : 1198 , Loss : 0.809421181678772, Weights 0.9398803114891052, bias : 0.1806923747062683  \n",
      "Steps : 1199 , Loss : 0.8093270659446716, Weights 0.940086305141449, bias : 0.18046508729457855  \n",
      "Steps : 1200 , Loss : 0.8092334866523743, Weights 0.9402919411659241, bias : 0.18023823201656342  \n",
      "Steps : 1201 , Loss : 0.809140145778656, Weights 0.9404972195625305, bias : 0.1800117939710617  \n",
      "Steps : 1202 , Loss : 0.8090471029281616, Weights 0.9407021999359131, bias : 0.17978577315807343  \n",
      "Steps : 1203 , Loss : 0.8089544177055359, Weights 0.940906822681427, bias : 0.17956016957759857  \n",
      "Steps : 1204 , Loss : 0.8088619709014893, Weights 0.9411110877990723, bias : 0.17933498322963715  \n",
      "Steps : 1205 , Loss : 0.8087700605392456, Weights 0.9413149952888489, bias : 0.17911022901535034  \n",
      "Steps : 1206 , Loss : 0.808678388595581, Weights 0.9415185451507568, bias : 0.17888589203357697  \n",
      "Steps : 1207 , Loss : 0.8085870146751404, Weights 0.9417217373847961, bias : 0.17866195738315582  \n",
      "Steps : 1208 , Loss : 0.8084959387779236, Weights 0.9419245719909668, bias : 0.1784384399652481  \n",
      "Steps : 1209 , Loss : 0.8084052801132202, Weights 0.9421271085739136, bias : 0.17821533977985382  \n",
      "Steps : 1210 , Loss : 0.8083149790763855, Weights 0.9423292875289917, bias : 0.17799265682697296  \n",
      "Steps : 1211 , Loss : 0.8082248568534851, Weights 0.9425311088562012, bias : 0.17777039110660553  \n",
      "Steps : 1212 , Loss : 0.8081350922584534, Weights 0.942732572555542, bias : 0.17754852771759033  \n",
      "Steps : 1213 , Loss : 0.8080457448959351, Weights 0.9429337382316589, bias : 0.17732708156108856  \n",
      "Steps : 1214 , Loss : 0.8079565167427063, Weights 0.9431345462799072, bias : 0.17710605263710022  \n",
      "Steps : 1215 , Loss : 0.8078677654266357, Weights 0.9433349967002869, bias : 0.1768854260444641  \n",
      "Steps : 1216 , Loss : 0.8077793717384338, Weights 0.9435350894927979, bias : 0.17666521668434143  \n",
      "Steps : 1217 , Loss : 0.807691216468811, Weights 0.943734884262085, bias : 0.17644540965557098  \n",
      "Steps : 1218 , Loss : 0.8076034784317017, Weights 0.9439343214035034, bias : 0.17622600495815277  \n",
      "Steps : 1219 , Loss : 0.8075159192085266, Weights 0.9441334009170532, bias : 0.17600701749324799  \n",
      "Steps : 1220 , Loss : 0.8074285984039307, Weights 0.9443321228027344, bias : 0.17578843235969543  \n",
      "Steps : 1221 , Loss : 0.8073417544364929, Weights 0.9445305466651917, bias : 0.17557024955749512  \n",
      "Steps : 1222 , Loss : 0.8072551488876343, Weights 0.9447286128997803, bias : 0.17535246908664703  \n",
      "Steps : 1223 , Loss : 0.8071689009666443, Weights 0.9449263215065002, bias : 0.17513509094715118  \n",
      "Steps : 1224 , Loss : 0.8070829510688782, Weights 0.9451237320899963, bias : 0.17491813004016876  \n",
      "Steps : 1225 , Loss : 0.8069973587989807, Weights 0.9453207850456238, bias : 0.17470157146453857  \n",
      "Steps : 1226 , Loss : 0.8069120645523071, Weights 0.9455174803733826, bias : 0.17448541522026062  \n",
      "Steps : 1227 , Loss : 0.8068270683288574, Weights 0.9457138776779175, bias : 0.1742696464061737  \n",
      "Steps : 1228 , Loss : 0.8067421913146973, Weights 0.9459099173545837, bias : 0.17405427992343903  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1229 , Loss : 0.8066577315330505, Weights 0.9461055994033813, bias : 0.17383931577205658  \n",
      "Steps : 1230 , Loss : 0.8065736293792725, Weights 0.9463009834289551, bias : 0.17362475395202637  \n",
      "Steps : 1231 , Loss : 0.806489884853363, Weights 0.9464960098266602, bias : 0.1734105944633484  \n",
      "Steps : 1232 , Loss : 0.8064062595367432, Weights 0.9466907382011414, bias : 0.17319682240486145  \n",
      "Steps : 1233 , Loss : 0.8063229918479919, Weights 0.9468851089477539, bias : 0.17298345267772675  \n",
      "Steps : 1234 , Loss : 0.8062400817871094, Weights 0.9470791816711426, bias : 0.17277047038078308  \n",
      "Steps : 1235 , Loss : 0.8061575293540955, Weights 0.9472728967666626, bias : 0.17255789041519165  \n",
      "Steps : 1236 , Loss : 0.8060751557350159, Weights 0.947466254234314, bias : 0.17234569787979126  \n",
      "Steps : 1237 , Loss : 0.8059930205345154, Weights 0.9476593136787415, bias : 0.1721339076757431  \n",
      "Steps : 1238 , Loss : 0.8059113025665283, Weights 0.9478520154953003, bias : 0.171922504901886  \n",
      "Steps : 1239 , Loss : 0.8058297634124756, Weights 0.9480444192886353, bias : 0.1717115044593811  \n",
      "Steps : 1240 , Loss : 0.805748701095581, Weights 0.9482364654541016, bias : 0.17150089144706726  \n",
      "Steps : 1241 , Loss : 0.8056676983833313, Weights 0.948428213596344, bias : 0.17129066586494446  \n",
      "Steps : 1242 , Loss : 0.8055872321128845, Weights 0.9486196041107178, bias : 0.1710808277130127  \n",
      "Steps : 1243 , Loss : 0.8055068850517273, Weights 0.9488106966018677, bias : 0.17087137699127197  \n",
      "Steps : 1244 , Loss : 0.8054267764091492, Weights 0.9490014314651489, bias : 0.1706623136997223  \n",
      "Steps : 1245 , Loss : 0.8053471446037292, Weights 0.9491918683052063, bias : 0.17045363783836365  \n",
      "Steps : 1246 , Loss : 0.8052676320075989, Weights 0.949381947517395, bias : 0.17024534940719604  \n",
      "Steps : 1247 , Loss : 0.8051885366439819, Weights 0.9495717287063599, bias : 0.17003744840621948  \n",
      "Steps : 1248 , Loss : 0.8051096200942993, Weights 0.949761152267456, bias : 0.16982993483543396  \n",
      "Steps : 1249 , Loss : 0.8050310611724854, Weights 0.9499502778053284, bias : 0.16962280869483948  \n",
      "Steps : 1250 , Loss : 0.8049526810646057, Weights 0.9501391053199768, bias : 0.16941606998443604  \n",
      "Steps : 1251 , Loss : 0.8048746585845947, Weights 0.9503275752067566, bias : 0.16920971870422363  \n",
      "Steps : 1252 , Loss : 0.8047969341278076, Weights 0.9505157470703125, bias : 0.16900373995304108  \n",
      "Steps : 1253 , Loss : 0.8047193884849548, Weights 0.9507035613059998, bias : 0.16879814863204956  \n",
      "Steps : 1254 , Loss : 0.8046422600746155, Weights 0.9508910775184631, bias : 0.1685929298400879  \n",
      "Steps : 1255 , Loss : 0.8045652508735657, Weights 0.9510782957077026, bias : 0.16838809847831726  \n",
      "Steps : 1256 , Loss : 0.8044886589050293, Weights 0.9512651562690735, bias : 0.16818365454673767  \n",
      "Steps : 1257 , Loss : 0.8044123649597168, Weights 0.9514517188072205, bias : 0.16797958314418793  \n",
      "Steps : 1258 , Loss : 0.8043361902236938, Weights 0.9516379833221436, bias : 0.16777588427066803  \n",
      "Steps : 1259 , Loss : 0.8042603135108948, Weights 0.951823890209198, bias : 0.16757257282733917  \n",
      "Steps : 1260 , Loss : 0.8041847944259644, Weights 0.9520094990730286, bias : 0.16736963391304016  \n",
      "Steps : 1261 , Loss : 0.804109513759613, Weights 0.9521948099136353, bias : 0.167167067527771  \n",
      "Steps : 1262 , Loss : 0.804034411907196, Weights 0.9523797631263733, bias : 0.16696488857269287  \n",
      "Steps : 1263 , Loss : 0.8039596676826477, Weights 0.9525644183158875, bias : 0.1667630821466446  \n",
      "Steps : 1264 , Loss : 0.8038852214813232, Weights 0.9527487754821777, bias : 0.16656164824962616  \n",
      "Steps : 1265 , Loss : 0.8038108944892883, Weights 0.9529327750205994, bias : 0.16636058688163757  \n",
      "Steps : 1266 , Loss : 0.8037370443344116, Weights 0.9531164765357971, bias : 0.16615989804267883  \n",
      "Steps : 1267 , Loss : 0.8036633133888245, Weights 0.953299880027771, bias : 0.16595958173274994  \n",
      "Steps : 1268 , Loss : 0.803589940071106, Weights 0.953482985496521, bias : 0.1657596379518509  \n",
      "Steps : 1269 , Loss : 0.803516685962677, Weights 0.9536657333374023, bias : 0.1655600666999817  \n",
      "Steps : 1270 , Loss : 0.8034437298774719, Weights 0.9538481831550598, bias : 0.16536086797714233  \n",
      "Steps : 1271 , Loss : 0.8033711314201355, Weights 0.9540303349494934, bias : 0.16516202688217163  \n",
      "Steps : 1272 , Loss : 0.8032986521720886, Weights 0.9542121887207031, bias : 0.16496355831623077  \n",
      "Steps : 1273 , Loss : 0.8032266497612, Weights 0.9543936848640442, bias : 0.16476546227931976  \n",
      "Steps : 1274 , Loss : 0.803154706954956, Weights 0.9545748829841614, bias : 0.1645677238702774  \n",
      "Steps : 1275 , Loss : 0.8030831813812256, Weights 0.9547557830810547, bias : 0.1643703579902649  \n",
      "Steps : 1276 , Loss : 0.8030117750167847, Weights 0.9549363851547241, bias : 0.16417336463928223  \n",
      "Steps : 1277 , Loss : 0.8029406666755676, Weights 0.9551166892051697, bias : 0.1639767289161682  \n",
      "Steps : 1278 , Loss : 0.8028697371482849, Weights 0.9552966356277466, bias : 0.16378046572208405  \n",
      "Steps : 1279 , Loss : 0.8027992844581604, Weights 0.9554762840270996, bias : 0.16358456015586853  \n",
      "Steps : 1280 , Loss : 0.8027288913726807, Weights 0.9556556344032288, bias : 0.16338901221752167  \n",
      "Steps : 1281 , Loss : 0.8026587963104248, Weights 0.955834686756134, bias : 0.16319383680820465  \n",
      "Steps : 1282 , Loss : 0.802588939666748, Weights 0.9560134410858154, bias : 0.1629990190267563  \n",
      "Steps : 1283 , Loss : 0.8025193810462952, Weights 0.956191897392273, bias : 0.16280455887317657  \n",
      "Steps : 1284 , Loss : 0.8024501204490662, Weights 0.9563700556755066, bias : 0.16261045634746552  \n",
      "Steps : 1285 , Loss : 0.8023809790611267, Weights 0.9565479159355164, bias : 0.1624167263507843  \n",
      "Steps : 1286 , Loss : 0.8023120760917664, Weights 0.9567254185676575, bias : 0.16222335398197174  \n",
      "Steps : 1287 , Loss : 0.8022434115409851, Weights 0.9569026231765747, bias : 0.16203033924102783  \n",
      "Steps : 1288 , Loss : 0.8021751642227173, Weights 0.9570795297622681, bias : 0.16183768212795258  \n",
      "Steps : 1289 , Loss : 0.8021069169044495, Weights 0.9572561383247375, bias : 0.16164538264274597  \n",
      "Steps : 1290 , Loss : 0.8020390868186951, Weights 0.9574324488639832, bias : 0.16145344078540802  \n",
      "Steps : 1291 , Loss : 0.8019714951515198, Weights 0.9576084613800049, bias : 0.16126185655593872  \n",
      "Steps : 1292 , Loss : 0.8019041419029236, Weights 0.9577841758728027, bias : 0.16107061505317688  \n",
      "Steps : 1293 , Loss : 0.8018370270729065, Weights 0.9579595923423767, bias : 0.1608797311782837  \n",
      "Steps : 1294 , Loss : 0.8017700910568237, Weights 0.9581347107887268, bias : 0.16068920493125916  \n",
      "Steps : 1295 , Loss : 0.8017033338546753, Weights 0.958309531211853, bias : 0.16049903631210327  \n",
      "Steps : 1296 , Loss : 0.8016369342803955, Weights 0.9584840536117554, bias : 0.16030921041965485  \n",
      "Steps : 1297 , Loss : 0.8015708327293396, Weights 0.9586582779884338, bias : 0.16011974215507507  \n",
      "Steps : 1298 , Loss : 0.8015048503875732, Weights 0.9588322043418884, bias : 0.15993061661720276  \n",
      "Steps : 1299 , Loss : 0.8014390468597412, Weights 0.9590058326721191, bias : 0.1597418487071991  \n",
      "Steps : 1300 , Loss : 0.8013737201690674, Weights 0.959179162979126, bias : 0.1595534384250641  \n",
      "Steps : 1301 , Loss : 0.8013083934783936, Weights 0.9593521952629089, bias : 0.15936537086963654  \n",
      "Steps : 1302 , Loss : 0.8012433648109436, Weights 0.959524929523468, bias : 0.15917764604091644  \n",
      "Steps : 1303 , Loss : 0.8011785745620728, Weights 0.9596973657608032, bias : 0.158990278840065  \n",
      "Steps : 1304 , Loss : 0.8011139631271362, Weights 0.9598695039749146, bias : 0.15880325436592102  \n",
      "Steps : 1305 , Loss : 0.8010496497154236, Weights 0.960041344165802, bias : 0.1586165726184845  \n",
      "Steps : 1306 , Loss : 0.80098557472229, Weights 0.9602128863334656, bias : 0.15843024849891663  \n",
      "Steps : 1307 , Loss : 0.8009217977523804, Weights 0.9603841304779053, bias : 0.1582442671060562  \n",
      "Steps : 1308 , Loss : 0.8008581399917603, Weights 0.9605550765991211, bias : 0.15805862843990326  \n",
      "Steps : 1309 , Loss : 0.800794780254364, Weights 0.960725724697113, bias : 0.15787333250045776  \n",
      "Steps : 1310 , Loss : 0.8007316589355469, Weights 0.9608961343765259, bias : 0.15768837928771973  \n",
      "Steps : 1311 , Loss : 0.8006686568260193, Weights 0.9610662460327148, bias : 0.15750376880168915  \n",
      "Steps : 1312 , Loss : 0.8006057739257812, Weights 0.9612360596656799, bias : 0.15731950104236603  \n",
      "Steps : 1313 , Loss : 0.8005433678627014, Weights 0.9614055752754211, bias : 0.15713557600975037  \n",
      "Steps : 1314 , Loss : 0.8004810810089111, Weights 0.9615747928619385, bias : 0.15695199370384216  \n",
      "Steps : 1315 , Loss : 0.8004190325737, Weights 0.9617437124252319, bias : 0.15676875412464142  \n",
      "Steps : 1316 , Loss : 0.8003572225570679, Weights 0.9619123339653015, bias : 0.15658584237098694  \n",
      "Steps : 1317 , Loss : 0.8002955913543701, Weights 0.9620806574821472, bias : 0.15640327334403992  \n",
      "Steps : 1318 , Loss : 0.8002340793609619, Weights 0.9622487425804138, bias : 0.15622104704380035  \n",
      "Steps : 1319 , Loss : 0.8001729846000671, Weights 0.9624165296554565, bias : 0.15603916347026825  \n",
      "Steps : 1320 , Loss : 0.8001120090484619, Weights 0.9625840187072754, bias : 0.1558576077222824  \n",
      "Steps : 1321 , Loss : 0.8000512719154358, Weights 0.9627512097358704, bias : 0.15567639470100403  \n",
      "Steps : 1322 , Loss : 0.799990713596344, Weights 0.9629181027412415, bias : 0.1554955095052719  \n",
      "Steps : 1323 , Loss : 0.7999303936958313, Weights 0.9630847573280334, bias : 0.15531496703624725  \n",
      "Steps : 1324 , Loss : 0.7998703122138977, Weights 0.9632511138916016, bias : 0.15513475239276886  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1325 , Loss : 0.7998104095458984, Weights 0.9634171724319458, bias : 0.15495488047599792  \n",
      "Steps : 1326 , Loss : 0.7997507452964783, Weights 0.9635829329490662, bias : 0.15477533638477325  \n",
      "Steps : 1327 , Loss : 0.7996914386749268, Weights 0.9637484550476074, bias : 0.15459612011909485  \n",
      "Steps : 1328 , Loss : 0.7996320128440857, Weights 0.9639136791229248, bias : 0.1544172465801239  \n",
      "Steps : 1329 , Loss : 0.7995730638504028, Weights 0.9640786051750183, bias : 0.15423870086669922  \n",
      "Steps : 1330 , Loss : 0.7995142936706543, Weights 0.9642432332038879, bias : 0.1540604829788208  \n",
      "Steps : 1331 , Loss : 0.7994556427001953, Weights 0.9644076228141785, bias : 0.15388259291648865  \n",
      "Steps : 1332 , Loss : 0.7993972301483154, Weights 0.9645717144012451, bias : 0.15370503067970276  \n",
      "Steps : 1333 , Loss : 0.7993390560150146, Weights 0.9647355079650879, bias : 0.15352779626846313  \n",
      "Steps : 1334 , Loss : 0.7992810010910034, Weights 0.9648990035057068, bias : 0.15335088968276978  \n",
      "Steps : 1335 , Loss : 0.7992232441902161, Weights 0.9650622606277466, bias : 0.15317431092262268  \n",
      "Steps : 1336 , Loss : 0.7991657257080078, Weights 0.9652252197265625, bias : 0.15299805998802185  \n",
      "Steps : 1337 , Loss : 0.7991084456443787, Weights 0.9653878808021545, bias : 0.15282213687896729  \n",
      "Steps : 1338 , Loss : 0.7990512251853943, Weights 0.9655503034591675, bias : 0.15264654159545898  \n",
      "Steps : 1339 , Loss : 0.7989943027496338, Weights 0.9657124280929565, bias : 0.15247127413749695  \n",
      "Steps : 1340 , Loss : 0.7989375591278076, Weights 0.9658742547035217, bias : 0.15229633450508118  \n",
      "Steps : 1341 , Loss : 0.798880934715271, Weights 0.9660358428955078, bias : 0.15212172269821167  \n",
      "Steps : 1342 , Loss : 0.7988246083259583, Weights 0.96619713306427, bias : 0.15194742381572723  \n",
      "Steps : 1343 , Loss : 0.7987685203552246, Weights 0.9663581252098083, bias : 0.15177345275878906  \n",
      "Steps : 1344 , Loss : 0.7987125515937805, Weights 0.9665188789367676, bias : 0.15159980952739716  \n",
      "Steps : 1345 , Loss : 0.7986567616462708, Weights 0.9666793346405029, bias : 0.15142647922039032  \n",
      "Steps : 1346 , Loss : 0.7986012101173401, Weights 0.9668395519256592, bias : 0.15125347673892975  \n",
      "Steps : 1347 , Loss : 0.7985458374023438, Weights 0.9669994711875916, bias : 0.15108078718185425  \n",
      "Steps : 1348 , Loss : 0.7984907627105713, Weights 0.9671590924263, bias : 0.150908425450325  \n",
      "Steps : 1349 , Loss : 0.7984358072280884, Weights 0.9673184752464294, bias : 0.15073637664318085  \n",
      "Steps : 1350 , Loss : 0.7983810305595398, Weights 0.967477560043335, bias : 0.15056465566158295  \n",
      "Steps : 1351 , Loss : 0.7983264923095703, Weights 0.9676364064216614, bias : 0.15039324760437012  \n",
      "Steps : 1352 , Loss : 0.7982720732688904, Weights 0.9677949547767639, bias : 0.15022215247154236  \n",
      "Steps : 1353 , Loss : 0.7982180118560791, Weights 0.9679532647132874, bias : 0.15005138516426086  \n",
      "Steps : 1354 , Loss : 0.7981640100479126, Weights 0.9681112766265869, bias : 0.14988093078136444  \n",
      "Steps : 1355 , Loss : 0.7981101870536804, Weights 0.9682690501213074, bias : 0.1497107893228531  \n",
      "Steps : 1356 , Loss : 0.7980566620826721, Weights 0.968426525592804, bias : 0.1495409607887268  \n",
      "Steps : 1357 , Loss : 0.7980032563209534, Weights 0.9685837030410767, bias : 0.1493714451789856  \n",
      "Steps : 1358 , Loss : 0.7979499697685242, Weights 0.9687406420707703, bias : 0.14920224249362946  \n",
      "Steps : 1359 , Loss : 0.7978969812393188, Weights 0.96889728307724, bias : 0.1490333527326584  \n",
      "Steps : 1360 , Loss : 0.7978441715240479, Weights 0.9690536856651306, bias : 0.1488647758960724  \n",
      "Steps : 1361 , Loss : 0.797791600227356, Weights 0.9692097902297974, bias : 0.14869651198387146  \n",
      "Steps : 1362 , Loss : 0.7977390885353088, Weights 0.969365656375885, bias : 0.1485285609960556  \n",
      "Steps : 1363 , Loss : 0.797686755657196, Weights 0.9695212841033936, bias : 0.14836092293262482  \n",
      "Steps : 1364 , Loss : 0.7976347208023071, Weights 0.9696766138076782, bias : 0.1481935977935791  \n",
      "Steps : 1365 , Loss : 0.7975828647613525, Weights 0.9698317050933838, bias : 0.14802658557891846  \n",
      "Steps : 1366 , Loss : 0.7975311279296875, Weights 0.9699864983558655, bias : 0.14785988628864288  \n",
      "Steps : 1367 , Loss : 0.7974795699119568, Weights 0.9701410531997681, bias : 0.1476934850215912  \n",
      "Steps : 1368 , Loss : 0.79742830991745, Weights 0.9702953100204468, bias : 0.14752739667892456  \n",
      "Steps : 1369 , Loss : 0.7973771095275879, Weights 0.9704493284225464, bias : 0.147361621260643  \n",
      "Steps : 1370 , Loss : 0.7973261475563049, Weights 0.9706030488014221, bias : 0.14719614386558533  \n",
      "Steps : 1371 , Loss : 0.7972753047943115, Weights 0.9707565307617188, bias : 0.14703097939491272  \n",
      "Steps : 1372 , Loss : 0.797224760055542, Weights 0.9709097743034363, bias : 0.146866112947464  \n",
      "Steps : 1373 , Loss : 0.7971742749214172, Weights 0.9710627198219299, bias : 0.14670155942440033  \n",
      "Steps : 1374 , Loss : 0.7971240282058716, Weights 0.9712154269218445, bias : 0.14653730392456055  \n",
      "Steps : 1375 , Loss : 0.7970739006996155, Weights 0.9713678359985352, bias : 0.14637336134910583  \n",
      "Steps : 1376 , Loss : 0.7970240712165833, Weights 0.9715200066566467, bias : 0.146209716796875  \n",
      "Steps : 1377 , Loss : 0.7969743609428406, Weights 0.9716719388961792, bias : 0.14604637026786804  \n",
      "Steps : 1378 , Loss : 0.796924889087677, Weights 0.9718235731124878, bias : 0.14588333666324615  \n",
      "Steps : 1379 , Loss : 0.7968754768371582, Weights 0.9719749689102173, bias : 0.14572060108184814  \n",
      "Steps : 1380 , Loss : 0.7968263030052185, Weights 0.9721261262893677, bias : 0.145558163523674  \n",
      "Steps : 1381 , Loss : 0.7967772483825684, Weights 0.9722769856452942, bias : 0.14539602398872375  \n",
      "Steps : 1382 , Loss : 0.7967284917831421, Weights 0.9724276065826416, bias : 0.14523419737815857  \n",
      "Steps : 1383 , Loss : 0.7966797351837158, Weights 0.9725779891014099, bias : 0.14507266879081726  \n",
      "Steps : 1384 , Loss : 0.7966313362121582, Weights 0.9727280735969543, bias : 0.14491143822669983  \n",
      "Steps : 1385 , Loss : 0.7965829968452454, Weights 0.9728779196739197, bias : 0.14475050568580627  \n",
      "Steps : 1386 , Loss : 0.7965348958969116, Weights 0.9730275273323059, bias : 0.1445898711681366  \n",
      "Steps : 1387 , Loss : 0.7964869141578674, Weights 0.9731768369674683, bias : 0.1444295346736908  \n",
      "Steps : 1388 , Loss : 0.7964390516281128, Weights 0.9733259081840515, bias : 0.14426949620246887  \n",
      "Steps : 1389 , Loss : 0.7963914275169373, Weights 0.9734747409820557, bias : 0.14410975575447083  \n",
      "Steps : 1390 , Loss : 0.7963440418243408, Weights 0.9736232757568359, bias : 0.14395029842853546  \n",
      "Steps : 1391 , Loss : 0.7962967157363892, Weights 0.9737715721130371, bias : 0.14379113912582397  \n",
      "Steps : 1392 , Loss : 0.7962496280670166, Weights 0.9739196300506592, bias : 0.14363227784633636  \n",
      "Steps : 1393 , Loss : 0.7962027788162231, Weights 0.9740674495697021, bias : 0.14347371459007263  \n",
      "Steps : 1394 , Loss : 0.7961559295654297, Weights 0.9742149710655212, bias : 0.14331543445587158  \n",
      "Steps : 1395 , Loss : 0.7961093187332153, Weights 0.9743622541427612, bias : 0.1431574523448944  \n",
      "Steps : 1396 , Loss : 0.7960627675056458, Weights 0.9745092988014221, bias : 0.1429997682571411  \n",
      "Steps : 1397 , Loss : 0.7960165143013, Weights 0.9746561050415039, bias : 0.1428423672914505  \n",
      "Steps : 1398 , Loss : 0.7959704399108887, Weights 0.9748026728630066, bias : 0.14268526434898376  \n",
      "Steps : 1399 , Loss : 0.7959245443344116, Weights 0.9749489426612854, bias : 0.1425284445285797  \n",
      "Steps : 1400 , Loss : 0.7958785891532898, Weights 0.9750949740409851, bias : 0.14237192273139954  \n",
      "Steps : 1401 , Loss : 0.7958331108093262, Weights 0.9752407670021057, bias : 0.14221568405628204  \n",
      "Steps : 1402 , Loss : 0.7957876324653625, Weights 0.9753863215446472, bias : 0.14205974340438843  \n",
      "Steps : 1403 , Loss : 0.7957423329353333, Weights 0.9755316376686096, bias : 0.1419040858745575  \n",
      "Steps : 1404 , Loss : 0.7956972718238831, Weights 0.9756766557693481, bias : 0.14174872636795044  \n",
      "Steps : 1405 , Loss : 0.7956522107124329, Weights 0.9758214354515076, bias : 0.14159364998340607  \n",
      "Steps : 1406 , Loss : 0.7956074476242065, Weights 0.9759659767150879, bias : 0.14143885672092438  \n",
      "Steps : 1407 , Loss : 0.795562744140625, Weights 0.9761102795600891, bias : 0.14128434658050537  \n",
      "Steps : 1408 , Loss : 0.7955183386802673, Weights 0.9762543439865112, bias : 0.14113013446331024  \n",
      "Steps : 1409 , Loss : 0.7954739332199097, Weights 0.9763981699943542, bias : 0.1409762054681778  \n",
      "Steps : 1410 , Loss : 0.7954297065734863, Weights 0.9765416979789734, bias : 0.14082255959510803  \n",
      "Steps : 1411 , Loss : 0.7953857183456421, Weights 0.9766849875450134, bias : 0.14066919684410095  \n",
      "Steps : 1412 , Loss : 0.7953418493270874, Weights 0.9768280386924744, bias : 0.14051611721515656  \n",
      "Steps : 1413 , Loss : 0.795298159122467, Weights 0.9769708514213562, bias : 0.14036332070827484  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1414 , Loss : 0.795254647731781, Weights 0.9771134257316589, bias : 0.1402108073234558  \n",
      "Steps : 1415 , Loss : 0.7952111959457397, Weights 0.9772557616233826, bias : 0.14005857706069946  \n",
      "Steps : 1416 , Loss : 0.7951679825782776, Weights 0.9773978590965271, bias : 0.1399066299200058  \n",
      "Steps : 1417 , Loss : 0.7951248288154602, Weights 0.9775397181510925, bias : 0.13975496590137482  \n",
      "Steps : 1418 , Loss : 0.7950819134712219, Weights 0.9776813387870789, bias : 0.13960358500480652  \n",
      "Steps : 1419 , Loss : 0.7950392365455627, Weights 0.9778227210044861, bias : 0.1394524723291397  \n",
      "Steps : 1420 , Loss : 0.7949965000152588, Weights 0.9779638648033142, bias : 0.13930164277553558  \n",
      "Steps : 1421 , Loss : 0.7949541211128235, Weights 0.9781047105789185, bias : 0.13915109634399414  \n",
      "Steps : 1422 , Loss : 0.794911801815033, Weights 0.9782453179359436, bias : 0.13900083303451538  \n",
      "Steps : 1423 , Loss : 0.7948695421218872, Weights 0.9783856868743896, bias : 0.1388508379459381  \n",
      "Steps : 1424 , Loss : 0.7948275804519653, Weights 0.9785258173942566, bias : 0.13870112597942352  \n",
      "Steps : 1425 , Loss : 0.794785737991333, Weights 0.9786657094955444, bias : 0.13855168223381042  \n",
      "Steps : 1426 , Loss : 0.7947440147399902, Weights 0.9788053631782532, bias : 0.13840252161026  \n",
      "Steps : 1427 , Loss : 0.7947024703025818, Weights 0.9789447784423828, bias : 0.13825364410877228  \n",
      "Steps : 1428 , Loss : 0.7946610450744629, Weights 0.9790839552879333, bias : 0.13810503482818604  \n",
      "Steps : 1429 , Loss : 0.7946197390556335, Weights 0.9792228937149048, bias : 0.13795670866966248  \n",
      "Steps : 1430 , Loss : 0.7945787310600281, Weights 0.9793615937232971, bias : 0.1378086507320404  \n",
      "Steps : 1431 , Loss : 0.7945376634597778, Weights 0.9795000553131104, bias : 0.13766086101531982  \n",
      "Steps : 1432 , Loss : 0.7944968342781067, Weights 0.9796382784843445, bias : 0.13751335442066193  \n",
      "Steps : 1433 , Loss : 0.7944561839103699, Weights 0.9797762632369995, bias : 0.13736611604690552  \n",
      "Steps : 1434 , Loss : 0.7944155931472778, Weights 0.9799140095710754, bias : 0.1372191458940506  \n",
      "Steps : 1435 , Loss : 0.7943751811981201, Weights 0.9800515174865723, bias : 0.13707245886325836  \n",
      "Steps : 1436 , Loss : 0.794334888458252, Weights 0.98018878698349, bias : 0.13692604005336761  \n",
      "Steps : 1437 , Loss : 0.7942948341369629, Weights 0.9803258776664734, bias : 0.13677988946437836  \n",
      "Steps : 1438 , Loss : 0.7942547798156738, Weights 0.9804627299308777, bias : 0.1366340070962906  \n",
      "Steps : 1439 , Loss : 0.7942150831222534, Weights 0.9805993437767029, bias : 0.1364883929491043  \n",
      "Steps : 1440 , Loss : 0.794175386428833, Weights 0.980735719203949, bias : 0.13634304702281952  \n",
      "Steps : 1441 , Loss : 0.7941358685493469, Weights 0.980871856212616, bias : 0.13619796931743622  \n",
      "Steps : 1442 , Loss : 0.7940964698791504, Weights 0.9810077548027039, bias : 0.1360531598329544  \n",
      "Steps : 1443 , Loss : 0.7940571308135986, Weights 0.9811434149742126, bias : 0.13590861856937408  \n",
      "Steps : 1444 , Loss : 0.7940179705619812, Weights 0.9812788367271423, bias : 0.13576434552669525  \n",
      "Steps : 1445 , Loss : 0.7939790487289429, Weights 0.9814140200614929, bias : 0.1356203407049179  \n",
      "Steps : 1446 , Loss : 0.7939401865005493, Weights 0.9815489649772644, bias : 0.13547660410404205  \n",
      "Steps : 1447 , Loss : 0.7939015030860901, Weights 0.9816836714744568, bias : 0.1353331357240677  \n",
      "Steps : 1448 , Loss : 0.7938629388809204, Weights 0.9818181991577148, bias : 0.1351899355649948  \n",
      "Steps : 1449 , Loss : 0.7938246130943298, Weights 0.9819524884223938, bias : 0.13504700362682343  \n",
      "Steps : 1450 , Loss : 0.7937862277030945, Weights 0.9820865392684937, bias : 0.13490432500839233  \n",
      "Steps : 1451 , Loss : 0.7937480807304382, Weights 0.9822203516960144, bias : 0.13476191461086273  \n",
      "Steps : 1452 , Loss : 0.7937101125717163, Weights 0.982353925704956, bias : 0.13461977243423462  \n",
      "Steps : 1453 , Loss : 0.7936722040176392, Weights 0.9824872612953186, bias : 0.1344778835773468  \n",
      "Steps : 1454 , Loss : 0.7936344146728516, Weights 0.982620358467102, bias : 0.13433626294136047  \n",
      "Steps : 1455 , Loss : 0.7935968041419983, Weights 0.9827532768249512, bias : 0.13419491052627563  \n",
      "Steps : 1456 , Loss : 0.7935594320297241, Weights 0.9828859567642212, bias : 0.1340538114309311  \n",
      "Steps : 1457 , Loss : 0.79352205991745, Weights 0.9830183982849121, bias : 0.13391298055648804  \n",
      "Steps : 1458 , Loss : 0.7934848070144653, Weights 0.9831506013870239, bias : 0.13377240300178528  \n",
      "Steps : 1459 , Loss : 0.793447732925415, Weights 0.9832825660705566, bias : 0.133632093667984  \n",
      "Steps : 1460 , Loss : 0.7934107780456543, Weights 0.983414351940155, bias : 0.13349203765392303  \n",
      "Steps : 1461 , Loss : 0.7933739423751831, Weights 0.9835458993911743, bias : 0.13335224986076355  \n",
      "Steps : 1462 , Loss : 0.793337345123291, Weights 0.9836772084236145, bias : 0.13321271538734436  \n",
      "Steps : 1463 , Loss : 0.7933008074760437, Weights 0.9838082790374756, bias : 0.13307343423366547  \n",
      "Steps : 1464 , Loss : 0.7932643294334412, Weights 0.9839391112327576, bias : 0.13293442130088806  \n",
      "Steps : 1465 , Loss : 0.7932279706001282, Weights 0.9840697646141052, bias : 0.13279566168785095  \n",
      "Steps : 1466 , Loss : 0.7931918501853943, Weights 0.9842001795768738, bias : 0.13265715539455414  \n",
      "Steps : 1467 , Loss : 0.79315584897995, Weights 0.9843303561210632, bias : 0.13251890242099762  \n",
      "Steps : 1468 , Loss : 0.7931199669837952, Weights 0.9844602942466736, bias : 0.1323809027671814  \n",
      "Steps : 1469 , Loss : 0.7930842041969299, Weights 0.9845900535583496, bias : 0.13224317133426666  \n",
      "Steps : 1470 , Loss : 0.7930485606193542, Weights 0.9847195744514465, bias : 0.13210569322109222  \n",
      "Steps : 1471 , Loss : 0.7930129766464233, Weights 0.9848488569259644, bias : 0.13196846842765808  \n",
      "Steps : 1472 , Loss : 0.7929776906967163, Weights 0.9849779605865479, bias : 0.13183149695396423  \n",
      "Steps : 1473 , Loss : 0.7929423451423645, Weights 0.9851068258285522, bias : 0.13169477880001068  \n",
      "Steps : 1474 , Loss : 0.792907178401947, Weights 0.9852354526519775, bias : 0.13155831396579742  \n",
      "Steps : 1475 , Loss : 0.7928723096847534, Weights 0.9853638410568237, bias : 0.13142210245132446  \n",
      "Steps : 1476 , Loss : 0.7928373217582703, Weights 0.9854920506477356, bias : 0.1312861442565918  \n",
      "Steps : 1477 , Loss : 0.7928025722503662, Weights 0.9856200218200684, bias : 0.13115042448043823  \n",
      "Steps : 1478 , Loss : 0.7927679419517517, Weights 0.985747754573822, bias : 0.13101495802402496  \n",
      "Steps : 1479 , Loss : 0.7927334308624268, Weights 0.9858753085136414, bias : 0.130879744887352  \n",
      "Steps : 1480 , Loss : 0.7926989793777466, Weights 0.9860026240348816, bias : 0.1307447850704193  \n",
      "Steps : 1481 , Loss : 0.7926648855209351, Weights 0.9861297011375427, bias : 0.13061007857322693  \n",
      "Steps : 1482 , Loss : 0.7926306128501892, Weights 0.9862565994262695, bias : 0.13047561049461365  \n",
      "Steps : 1483 , Loss : 0.7925965785980225, Weights 0.9863832592964172, bias : 0.13034139573574066  \n",
      "Steps : 1484 , Loss : 0.7925627827644348, Weights 0.9865096807479858, bias : 0.13020743429660797  \n",
      "Steps : 1485 , Loss : 0.7925289273262024, Weights 0.9866359233856201, bias : 0.13007371127605438  \n",
      "Steps : 1486 , Loss : 0.7924952507019043, Weights 0.9867619276046753, bias : 0.1299402415752411  \n",
      "Steps : 1487 , Loss : 0.7924616932868958, Weights 0.9868876934051514, bias : 0.1298070251941681  \n",
      "Steps : 1488 , Loss : 0.7924282550811768, Weights 0.9870132803916931, bias : 0.1296740472316742  \n",
      "Steps : 1489 , Loss : 0.7923949360847473, Weights 0.9871386289596558, bias : 0.1295413225889206  \n",
      "Steps : 1490 , Loss : 0.792361855506897, Weights 0.9872637987136841, bias : 0.1294088363647461  \n",
      "Steps : 1491 , Loss : 0.7923286557197571, Weights 0.9873887300491333, bias : 0.1292766034603119  \n",
      "Steps : 1492 , Loss : 0.7922957539558411, Weights 0.9875134229660034, bias : 0.1291446089744568  \n",
      "Steps : 1493 , Loss : 0.792262852191925, Weights 0.9876379370689392, bias : 0.1290128529071808  \n",
      "Steps : 1494 , Loss : 0.7922302484512329, Weights 0.9877622127532959, bias : 0.12888135015964508  \n",
      "Steps : 1495 , Loss : 0.792197585105896, Weights 0.9878863096237183, bias : 0.12875008583068848  \n",
      "Steps : 1496 , Loss : 0.7921650409698486, Weights 0.9880101680755615, bias : 0.12861905992031097  \n",
      "Steps : 1497 , Loss : 0.7921327352523804, Weights 0.9881338477134705, bias : 0.12848828732967377  \n",
      "Steps : 1498 , Loss : 0.7921006679534912, Weights 0.9882572889328003, bias : 0.12835775315761566  \n",
      "Steps : 1499 , Loss : 0.7920683026313782, Weights 0.988380491733551, bias : 0.12822745740413666  \n",
      "Steps : 1500 , Loss : 0.7920364141464233, Weights 0.9885035157203674, bias : 0.12809740006923676  \n",
      "Steps : 1501 , Loss : 0.7920045256614685, Weights 0.9886263012886047, bias : 0.12796758115291595  \n",
      "Steps : 1502 , Loss : 0.7919726371765137, Weights 0.9887489080429077, bias : 0.12783800065517426  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1503 , Loss : 0.7919410467147827, Weights 0.9888712763786316, bias : 0.12770867347717285  \n",
      "Steps : 1504 , Loss : 0.791909396648407, Weights 0.9889934659004211, bias : 0.12757958471775055  \n",
      "Steps : 1505 , Loss : 0.7918779850006104, Weights 0.9891154170036316, bias : 0.12745073437690735  \n",
      "Steps : 1506 , Loss : 0.7918466329574585, Weights 0.9892371892929077, bias : 0.12732212245464325  \n",
      "Steps : 1507 , Loss : 0.7918155193328857, Weights 0.9893587231636047, bias : 0.12719374895095825  \n",
      "Steps : 1508 , Loss : 0.7917843461036682, Weights 0.9894800782203674, bias : 0.12706561386585236  \n",
      "Steps : 1509 , Loss : 0.7917532920837402, Weights 0.989601194858551, bias : 0.12693770229816437  \n",
      "Steps : 1510 , Loss : 0.7917224168777466, Weights 0.9897221326828003, bias : 0.12681002914905548  \n",
      "Steps : 1511 , Loss : 0.7916916012763977, Weights 0.9898428320884705, bias : 0.1266825944185257  \n",
      "Steps : 1512 , Loss : 0.7916609048843384, Weights 0.9899633526802063, bias : 0.126555398106575  \n",
      "Steps : 1513 , Loss : 0.7916303873062134, Weights 0.990083634853363, bias : 0.12642844021320343  \n",
      "Steps : 1514 , Loss : 0.7915999889373779, Weights 0.9902037382125854, bias : 0.12630172073841095  \n",
      "Steps : 1515 , Loss : 0.7915696501731873, Weights 0.9903236031532288, bias : 0.12617522478103638  \n",
      "Steps : 1516 , Loss : 0.7915393710136414, Weights 0.9904432892799377, bias : 0.1260489672422409  \n",
      "Steps : 1517 , Loss : 0.7915092706680298, Weights 0.9905627965927124, bias : 0.12592294812202454  \n",
      "Steps : 1518 , Loss : 0.7914791703224182, Weights 0.990682065486908, bias : 0.12579715251922607  \n",
      "Steps : 1519 , Loss : 0.7914493680000305, Weights 0.9908011555671692, bias : 0.1256715953350067  \n",
      "Steps : 1520 , Loss : 0.791419506072998, Weights 0.9909200072288513, bias : 0.12554626166820526  \n",
      "Steps : 1521 , Loss : 0.7913897633552551, Weights 0.9910386800765991, bias : 0.1254211664199829  \n",
      "Steps : 1522 , Loss : 0.7913601398468018, Weights 0.9911571145057678, bias : 0.12529630959033966  \n",
      "Steps : 1523 , Loss : 0.7913306951522827, Weights 0.9912753701210022, bias : 0.12517167627811432  \n",
      "Steps : 1524 , Loss : 0.791301429271698, Weights 0.9913934469223022, bias : 0.12504726648330688  \n",
      "Steps : 1525 , Loss : 0.7912721037864685, Weights 0.9915112853050232, bias : 0.12492309510707855  \n",
      "Steps : 1526 , Loss : 0.7912428379058838, Weights 0.9916289448738098, bias : 0.12479915469884872  \n",
      "Steps : 1527 , Loss : 0.7912138104438782, Weights 0.9917463660240173, bias : 0.1246754378080368  \n",
      "Steps : 1528 , Loss : 0.7911848425865173, Weights 0.9918636083602905, bias : 0.12455195188522339  \n",
      "Steps : 1529 , Loss : 0.791155993938446, Weights 0.9919806718826294, bias : 0.12442869693040848  \n",
      "Steps : 1530 , Loss : 0.7911271452903748, Weights 0.9920974969863892, bias : 0.12430566549301147  \n",
      "Steps : 1531 , Loss : 0.7910984754562378, Weights 0.9922141432762146, bias : 0.12418286502361298  \n",
      "Steps : 1532 , Loss : 0.7910699248313904, Weights 0.9923306107521057, bias : 0.12406028807163239  \n",
      "Steps : 1533 , Loss : 0.7910415530204773, Weights 0.9924468398094177, bias : 0.1239379420876503  \n",
      "Steps : 1534 , Loss : 0.7910131216049194, Weights 0.9925628900527954, bias : 0.12381581962108612  \n",
      "Steps : 1535 , Loss : 0.7909849286079407, Weights 0.9926787614822388, bias : 0.12369392067193985  \n",
      "Steps : 1536 , Loss : 0.7909567356109619, Weights 0.992794394493103, bias : 0.12357225269079208  \n",
      "Steps : 1537 , Loss : 0.7909286618232727, Weights 0.992909848690033, bias : 0.12345080822706223  \n",
      "Steps : 1538 , Loss : 0.7909008264541626, Weights 0.9930251240730286, bias : 0.12332958728075027  \n",
      "Steps : 1539 , Loss : 0.7908728718757629, Weights 0.9931401610374451, bias : 0.12320858985185623  \n",
      "Steps : 1540 , Loss : 0.7908450961112976, Weights 0.9932550191879272, bias : 0.1230878159403801  \n",
      "Steps : 1541 , Loss : 0.7908174395561218, Weights 0.9933696985244751, bias : 0.12296726554632187  \n",
      "Steps : 1542 , Loss : 0.7907898426055908, Weights 0.9934841394424438, bias : 0.12284693866968155  \n",
      "Steps : 1543 , Loss : 0.7907624244689941, Weights 0.9935984015464783, bias : 0.12272683531045914  \n",
      "Steps : 1544 , Loss : 0.7907350659370422, Weights 0.9937124848365784, bias : 0.12260695546865463  \n",
      "Steps : 1545 , Loss : 0.7907078266143799, Weights 0.9938263297080994, bias : 0.12248729169368744  \n",
      "Steps : 1546 , Loss : 0.7906807065010071, Weights 0.993939995765686, bias : 0.12236785143613815  \n",
      "Steps : 1547 , Loss : 0.7906537055969238, Weights 0.9940534830093384, bias : 0.12224863469600677  \n",
      "Steps : 1548 , Loss : 0.790626585483551, Weights 0.9941667914390564, bias : 0.12212963402271271  \n",
      "Steps : 1549 , Loss : 0.7905997633934021, Weights 0.9942798614501953, bias : 0.12201085686683655  \n",
      "Steps : 1550 , Loss : 0.7905729413032532, Weights 0.9943927526473999, bias : 0.1218922957777977  \n",
      "Steps : 1551 , Loss : 0.7905462384223938, Weights 0.9945054650306702, bias : 0.12177395820617676  \n",
      "Steps : 1552 , Loss : 0.7905197143554688, Weights 0.9946179986000061, bias : 0.12165583670139313  \n",
      "Steps : 1553 , Loss : 0.7904933094978333, Weights 0.9947302937507629, bias : 0.12153793126344681  \n",
      "Steps : 1554 , Loss : 0.7904667854309082, Weights 0.9948424100875854, bias : 0.1214202493429184  \n",
      "Steps : 1555 , Loss : 0.7904404997825623, Weights 0.9949543476104736, bias : 0.1213027834892273  \n",
      "Steps : 1556 , Loss : 0.7904143333435059, Weights 0.9950661063194275, bias : 0.1211855337023735  \n",
      "Steps : 1557 , Loss : 0.7903881669044495, Weights 0.9951776266098022, bias : 0.12106849998235703  \n",
      "Steps : 1558 , Loss : 0.7903621792793274, Weights 0.9952889680862427, bias : 0.12095168232917786  \n",
      "Steps : 1559 , Loss : 0.7903362512588501, Weights 0.9954001307487488, bias : 0.120835080742836  \n",
      "Steps : 1560 , Loss : 0.7903103828430176, Weights 0.9955111145973206, bias : 0.12071869522333145  \n",
      "Steps : 1561 , Loss : 0.7902846336364746, Weights 0.995621919631958, bias : 0.12060252577066422  \n",
      "Steps : 1562 , Loss : 0.7902590036392212, Weights 0.9957324862480164, bias : 0.12048657238483429  \n",
      "Steps : 1563 , Loss : 0.7902334332466125, Weights 0.9958428740501404, bias : 0.12037083506584167  \n",
      "Steps : 1564 , Loss : 0.7902080416679382, Weights 0.9959530830383301, bias : 0.12025530636310577  \n",
      "Steps : 1565 , Loss : 0.7901826500892639, Weights 0.9960631132125854, bias : 0.12013999372720718  \n",
      "Steps : 1566 , Loss : 0.7901572585105896, Weights 0.9961729645729065, bias : 0.1200248971581459  \n",
      "Steps : 1567 , Loss : 0.7901320457458496, Weights 0.9962826371192932, bias : 0.11991000920534134  \n",
      "Steps : 1568 , Loss : 0.790107011795044, Weights 0.9963920712471008, bias : 0.11979533731937408  \n",
      "Steps : 1569 , Loss : 0.7900819182395935, Weights 0.9965013265609741, bias : 0.11968087404966354  \n",
      "Steps : 1570 , Loss : 0.7900570631027222, Weights 0.9966104030609131, bias : 0.11956661939620972  \n",
      "Steps : 1571 , Loss : 0.7900322675704956, Weights 0.9967193007469177, bias : 0.1194525808095932  \n",
      "Steps : 1572 , Loss : 0.790007472038269, Weights 0.996828019618988, bias : 0.1193387508392334  \n",
      "Steps : 1573 , Loss : 0.7899827361106873, Weights 0.996936559677124, bias : 0.11922512948513031  \n",
      "Steps : 1574 , Loss : 0.7899581789970398, Weights 0.9970449209213257, bias : 0.11911172419786453  \n",
      "Steps : 1575 , Loss : 0.7899338006973267, Weights 0.9971530437469482, bias : 0.11899852752685547  \n",
      "Steps : 1576 , Loss : 0.7899093627929688, Weights 0.9972609877586365, bias : 0.11888553947210312  \n",
      "Steps : 1577 , Loss : 0.7898850440979004, Weights 0.9973687529563904, bias : 0.11877276003360748  \n",
      "Steps : 1578 , Loss : 0.7898608446121216, Weights 0.99747633934021, bias : 0.11866018921136856  \n",
      "Steps : 1579 , Loss : 0.7898366451263428, Weights 0.9975837469100952, bias : 0.11854782700538635  \n",
      "Steps : 1580 , Loss : 0.7898126244544983, Weights 0.9976909756660461, bias : 0.11843567341566086  \n",
      "Steps : 1581 , Loss : 0.7897886633872986, Weights 0.9977980256080627, bias : 0.11832372099161148  \n",
      "Steps : 1582 , Loss : 0.7897647023200989, Weights 0.997904896736145, bias : 0.11821197718381882  \n",
      "Steps : 1583 , Loss : 0.7897409796714783, Weights 0.998011589050293, bias : 0.11810044199228287  \n",
      "Steps : 1584 , Loss : 0.7897171974182129, Weights 0.9981181025505066, bias : 0.11798911541700363  \n",
      "Steps : 1585 , Loss : 0.7896935939788818, Weights 0.9982244372367859, bias : 0.11787799000740051  \n",
      "Steps : 1586 , Loss : 0.7896700501441956, Weights 0.9983305931091309, bias : 0.11776707321405411  \n",
      "Steps : 1587 , Loss : 0.7896465063095093, Weights 0.9984365105628967, bias : 0.11765635758638382  \n",
      "Steps : 1588 , Loss : 0.7896232604980469, Weights 0.9985422492027283, bias : 0.11754585057497025  \n",
      "Steps : 1589 , Loss : 0.7895998954772949, Weights 0.9986478090286255, bias : 0.11743554472923279  \n",
      "Steps : 1590 , Loss : 0.7895767092704773, Weights 0.9987531900405884, bias : 0.11732544004917145  \n",
      "Steps : 1591 , Loss : 0.7895535826683044, Weights 0.9988583922386169, bias : 0.11721554398536682  \n",
      "Steps : 1592 , Loss : 0.7895305156707764, Weights 0.9989634156227112, bias : 0.11710584908723831  \n",
      "Steps : 1593 , Loss : 0.7895076274871826, Weights 0.9990682601928711, bias : 0.11699635535478592  \n",
      "Steps : 1594 , Loss : 0.7894847393035889, Weights 0.9991729259490967, bias : 0.11688706278800964  \n",
      "Steps : 1595 , Loss : 0.7894619107246399, Weights 0.9992774128913879, bias : 0.11677797138690948  \n",
      "Steps : 1596 , Loss : 0.7894392609596252, Weights 0.9993817210197449, bias : 0.11666908860206604  \n",
      "Steps : 1597 , Loss : 0.7894165515899658, Weights 0.9994858503341675, bias : 0.11656040698289871  \n",
      "Steps : 1598 , Loss : 0.7893940210342407, Weights 0.9995898008346558, bias : 0.1164519190788269  \n",
      "Steps : 1599 , Loss : 0.7893715500831604, Weights 0.9996935725212097, bias : 0.11634363234043121  \n",
      "Steps : 1600 , Loss : 0.7893490791320801, Weights 0.9997971653938293, bias : 0.11623554676771164  \n",
      "Steps : 1601 , Loss : 0.7893267869949341, Weights 0.9999005794525146, bias : 0.11612766236066818  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1602 , Loss : 0.7893045544624329, Weights 1.0000038146972656, bias : 0.11601997911930084  \n",
      "Steps : 1603 , Loss : 0.7892823815345764, Weights 1.000106930732727, bias : 0.11591249704360962  \n",
      "Steps : 1604 , Loss : 0.7892602682113647, Weights 1.0002098083496094, bias : 0.11580520868301392  \n",
      "Steps : 1605 , Loss : 0.7892382740974426, Weights 1.0003125667572021, bias : 0.11569812148809433  \n",
      "Steps : 1606 , Loss : 0.7892164587974548, Weights 1.0004150867462158, bias : 0.11559122800827026  \n",
      "Steps : 1607 , Loss : 0.7891945838928223, Weights 1.00051748752594, bias : 0.11548453569412231  \n",
      "Steps : 1608 , Loss : 0.7891728281974792, Weights 1.000619649887085, bias : 0.11537803709506989  \n",
      "Steps : 1609 , Loss : 0.7891510128974915, Weights 1.0007216930389404, bias : 0.11527173966169357  \n",
      "Steps : 1610 , Loss : 0.7891294956207275, Weights 1.0008234977722168, bias : 0.11516563594341278  \n",
      "Steps : 1611 , Loss : 0.7891079783439636, Weights 1.0009251832962036, bias : 0.11505972594022751  \n",
      "Steps : 1612 , Loss : 0.7890864610671997, Weights 1.0010266304016113, bias : 0.11495401710271835  \n",
      "Steps : 1613 , Loss : 0.7890651822090149, Weights 1.0011279582977295, bias : 0.11484850198030472  \n",
      "Steps : 1614 , Loss : 0.7890437841415405, Weights 1.0012290477752686, bias : 0.1147431805729866  \n",
      "Steps : 1615 , Loss : 0.78902268409729, Weights 1.001330018043518, bias : 0.11463805288076401  \n",
      "Steps : 1616 , Loss : 0.78900146484375, Weights 1.001430869102478, bias : 0.11453311890363693  \n",
      "Steps : 1617 , Loss : 0.7889804244041443, Weights 1.0015314817428589, bias : 0.11442837864160538  \n",
      "Steps : 1618 , Loss : 0.7889593243598938, Weights 1.0016319751739502, bias : 0.11432383209466934  \n",
      "Steps : 1619 , Loss : 0.7889384627342224, Weights 1.0017322301864624, bias : 0.11421947926282883  \n",
      "Steps : 1620 , Loss : 0.7889175415039062, Weights 1.001832365989685, bias : 0.11411532014608383  \n",
      "Steps : 1621 , Loss : 0.7888968586921692, Weights 1.0019322633743286, bias : 0.11401135474443436  \n",
      "Steps : 1622 , Loss : 0.7888761162757874, Weights 1.0020320415496826, bias : 0.1139075830578804  \n",
      "Steps : 1623 , Loss : 0.7888554930686951, Weights 1.0021315813064575, bias : 0.11380399763584137  \n",
      "Steps : 1624 , Loss : 0.7888349890708923, Weights 1.0022310018539429, bias : 0.11370060592889786  \n",
      "Steps : 1625 , Loss : 0.7888144850730896, Weights 1.0023303031921387, bias : 0.11359740793704987  \n",
      "Steps : 1626 , Loss : 0.7887940406799316, Weights 1.0024293661117554, bias : 0.1134943962097168  \n",
      "Steps : 1627 , Loss : 0.7887736558914185, Weights 1.0025283098220825, bias : 0.11339157819747925  \n",
      "Steps : 1628 , Loss : 0.7887535095214844, Weights 1.0026270151138306, bias : 0.11328894644975662  \n",
      "Steps : 1629 , Loss : 0.7887333035469055, Weights 1.002725601196289, bias : 0.11318650841712952  \n",
      "Steps : 1630 , Loss : 0.7887130975723267, Weights 1.0028239488601685, bias : 0.11308425664901733  \n",
      "Steps : 1631 , Loss : 0.7886930704116821, Weights 1.0029221773147583, bias : 0.11298219114542007  \n",
      "Steps : 1632 , Loss : 0.7886731028556824, Weights 1.0030202865600586, bias : 0.11288031935691833  \n",
      "Steps : 1633 , Loss : 0.7886532545089722, Weights 1.0031181573867798, bias : 0.11277863383293152  \n",
      "Steps : 1634 , Loss : 0.788633406162262, Weights 1.0032159090042114, bias : 0.11267713457345963  \n",
      "Steps : 1635 , Loss : 0.7886135578155518, Weights 1.003313422203064, bias : 0.11257582157850266  \n",
      "Steps : 1636 , Loss : 0.7885938286781311, Weights 1.003410816192627, bias : 0.1124747022986412  \n",
      "Steps : 1637 , Loss : 0.78857421875, Weights 1.0035080909729004, bias : 0.11237376928329468  \n",
      "Steps : 1638 , Loss : 0.7885547876358032, Weights 1.0036051273345947, bias : 0.11227302253246307  \n",
      "Steps : 1639 , Loss : 0.7885352969169617, Weights 1.0037020444869995, bias : 0.11217246204614639  \n",
      "Steps : 1640 , Loss : 0.7885159254074097, Weights 1.0037987232208252, bias : 0.11207208037376404  \n",
      "Steps : 1641 , Loss : 0.7884964942932129, Weights 1.0038952827453613, bias : 0.1119718849658966  \n",
      "Steps : 1642 , Loss : 0.7884771823883057, Weights 1.003991723060608, bias : 0.1118718758225441  \n",
      "Steps : 1643 , Loss : 0.7884580492973328, Weights 1.0040879249572754, bias : 0.11177205294370651  \n",
      "Steps : 1644 , Loss : 0.7884388566017151, Weights 1.0041840076446533, bias : 0.11167241632938385  \n",
      "Steps : 1645 , Loss : 0.788419783115387, Weights 1.0042798519134521, bias : 0.11157295852899551  \n",
      "Steps : 1646 , Loss : 0.7884008884429932, Weights 1.0043755769729614, bias : 0.1114736869931221  \n",
      "Steps : 1647 , Loss : 0.7883818745613098, Weights 1.0044711828231812, bias : 0.11137460172176361  \n",
      "Steps : 1648 , Loss : 0.7883630394935608, Weights 1.0045665502548218, bias : 0.11127569526433945  \n",
      "Steps : 1649 , Loss : 0.7883442044258118, Weights 1.0046617984771729, bias : 0.1111769750714302  \n",
      "Steps : 1650 , Loss : 0.7883254289627075, Weights 1.0047569274902344, bias : 0.11107843369245529  \n",
      "Steps : 1651 , Loss : 0.7883068323135376, Weights 1.0048518180847168, bias : 0.1109800785779953  \n",
      "Steps : 1652 , Loss : 0.7882882952690125, Weights 1.0049465894699097, bias : 0.11088190227746964  \n",
      "Steps : 1653 , Loss : 0.7882696390151978, Weights 1.0050411224365234, bias : 0.1107839047908783  \n",
      "Steps : 1654 , Loss : 0.7882511615753174, Weights 1.0051355361938477, bias : 0.11068609356880188  \n",
      "Steps : 1655 , Loss : 0.7882328033447266, Weights 1.0052298307418823, bias : 0.11058846116065979  \n",
      "Steps : 1656 , Loss : 0.7882145047187805, Weights 1.005323886871338, bias : 0.11049100756645203  \n",
      "Steps : 1657 , Loss : 0.7881962060928345, Weights 1.005417823791504, bias : 0.11039373278617859  \n",
      "Steps : 1658 , Loss : 0.788178026676178, Weights 1.0055116415023804, bias : 0.11029663681983948  \n",
      "Steps : 1659 , Loss : 0.7881597876548767, Weights 1.0056052207946777, bias : 0.11019971966743469  \n",
      "Steps : 1660 , Loss : 0.7881417870521545, Weights 1.0056986808776855, bias : 0.11010298132896423  \n",
      "Steps : 1661 , Loss : 0.7881237864494324, Weights 1.0057920217514038, bias : 0.1100064218044281  \n",
      "Steps : 1662 , Loss : 0.7881057858467102, Weights 1.005885124206543, bias : 0.1099100410938263  \n",
      "Steps : 1663 , Loss : 0.7880879044532776, Weights 1.0059781074523926, bias : 0.10981383919715881  \n",
      "Steps : 1664 , Loss : 0.7880700826644897, Weights 1.0060709714889526, bias : 0.10971781611442566  \n",
      "Steps : 1665 , Loss : 0.7880523800849915, Weights 1.0061635971069336, bias : 0.10962197184562683  \n",
      "Steps : 1666 , Loss : 0.7880346775054932, Weights 1.006256103515625, bias : 0.10952630639076233  \n",
      "Steps : 1667 , Loss : 0.7880170941352844, Weights 1.0063484907150269, bias : 0.10943081229925156  \n",
      "Steps : 1668 , Loss : 0.7879995703697205, Weights 1.0064406394958496, bias : 0.10933549702167511  \n",
      "Steps : 1669 , Loss : 0.7879820466041565, Weights 1.0065326690673828, bias : 0.10924036055803299  \n",
      "Steps : 1670 , Loss : 0.7879644632339478, Weights 1.0066245794296265, bias : 0.1091453954577446  \n",
      "Steps : 1671 , Loss : 0.7879472374916077, Weights 1.006716251373291, bias : 0.10905060917139053  \n",
      "Steps : 1672 , Loss : 0.7879298329353333, Weights 1.006807804107666, bias : 0.1089559942483902  \n",
      "Steps : 1673 , Loss : 0.7879126071929932, Weights 1.0068992376327515, bias : 0.10886155813932419  \n",
      "Steps : 1674 , Loss : 0.7878953814506531, Weights 1.0069904327392578, bias : 0.10876729339361191  \n",
      "Steps : 1675 , Loss : 0.7878782749176025, Weights 1.0070815086364746, bias : 0.10867320001125336  \n",
      "Steps : 1676 , Loss : 0.787861168384552, Weights 1.0071724653244019, bias : 0.10857928544282913  \n",
      "Steps : 1677 , Loss : 0.7878442406654358, Weights 1.00726318359375, bias : 0.10848554223775864  \n",
      "Steps : 1678 , Loss : 0.7878272533416748, Weights 1.0073537826538086, bias : 0.10839197039604187  \n",
      "Steps : 1679 , Loss : 0.7878103852272034, Weights 1.0074442625045776, bias : 0.10829856991767883  \n",
      "Steps : 1680 , Loss : 0.7877935767173767, Weights 1.0075345039367676, bias : 0.10820534825325012  \n",
      "Steps : 1681 , Loss : 0.78777676820755, Weights 1.007624626159668, bias : 0.10811229795217514  \n",
      "Steps : 1682 , Loss : 0.7877600193023682, Weights 1.0077146291732788, bias : 0.10801941901445389  \n",
      "Steps : 1683 , Loss : 0.7877433896064758, Weights 1.0078043937683105, bias : 0.10792671144008636  \n",
      "Steps : 1684 , Loss : 0.7877267599105835, Weights 1.0078940391540527, bias : 0.10783417522907257  \n",
      "Steps : 1685 , Loss : 0.7877102494239807, Weights 1.0079835653305054, bias : 0.1077418103814125  \n",
      "Steps : 1686 , Loss : 0.7876938581466675, Weights 1.0080729722976685, bias : 0.10764961689710617  \n",
      "Steps : 1687 , Loss : 0.7876772880554199, Weights 1.0081621408462524, bias : 0.10755758732557297  \n",
      "Steps : 1688 , Loss : 0.787661075592041, Weights 1.0082511901855469, bias : 0.1074657291173935  \n",
      "Steps : 1689 , Loss : 0.7876447439193726, Weights 1.0083401203155518, bias : 0.10737404227256775  \n",
      "Steps : 1690 , Loss : 0.7876284718513489, Weights 1.0084288120269775, bias : 0.10728252679109573  \n",
      "Steps : 1691 , Loss : 0.7876123189926147, Weights 1.0085173845291138, bias : 0.10719117522239685  \n",
      "Steps : 1692 , Loss : 0.7875962853431702, Weights 1.0086058378219604, bias : 0.1070999950170517  \n",
      "Steps : 1693 , Loss : 0.7875801920890808, Weights 1.0086941719055176, bias : 0.10700898617506027  \n",
      "Steps : 1694 , Loss : 0.7875640988349915, Weights 1.0087822675704956, bias : 0.10691814124584198  \n",
      "Steps : 1695 , Loss : 0.7875482439994812, Weights 1.008870244026184, bias : 0.10682746767997742  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1696 , Loss : 0.7875322103500366, Weights 1.008958101272583, bias : 0.10673695802688599  \n",
      "Steps : 1697 , Loss : 0.7875164747238159, Weights 1.0090458393096924, bias : 0.10664661973714828  \n",
      "Steps : 1698 , Loss : 0.7875007390975952, Weights 1.0091333389282227, bias : 0.10655644536018372  \n",
      "Steps : 1699 , Loss : 0.7874850630760193, Weights 1.0092207193374634, bias : 0.10646643489599228  \n",
      "Steps : 1700 , Loss : 0.7874693274497986, Weights 1.0093079805374146, bias : 0.10637659579515457  \n",
      "Steps : 1701 , Loss : 0.7874537110328674, Weights 1.0093951225280762, bias : 0.10628692060709  \n",
      "Steps : 1702 , Loss : 0.7874380350112915, Weights 1.0094820261001587, bias : 0.10619740933179855  \n",
      "Steps : 1703 , Loss : 0.7874226570129395, Weights 1.0095688104629517, bias : 0.10610806196928024  \n",
      "Steps : 1704 , Loss : 0.7874071598052979, Weights 1.009655475616455, bias : 0.10601887851953506  \n",
      "Steps : 1705 , Loss : 0.787391722202301, Weights 1.009742021560669, bias : 0.10592985898256302  \n",
      "Steps : 1706 , Loss : 0.7873764038085938, Weights 1.0098283290863037, bias : 0.1058410033583641  \n",
      "Steps : 1707 , Loss : 0.7873611450195312, Weights 1.009914517402649, bias : 0.10575231164693832  \n",
      "Steps : 1708 , Loss : 0.7873458862304688, Weights 1.0100005865097046, bias : 0.10566378384828568  \n",
      "Steps : 1709 , Loss : 0.7873308062553406, Weights 1.0100865364074707, bias : 0.10557541996240616  \n",
      "Steps : 1710 , Loss : 0.7873156070709229, Weights 1.0101722478866577, bias : 0.10548721998929977  \n",
      "Steps : 1711 , Loss : 0.7873006463050842, Weights 1.0102578401565552, bias : 0.10539918392896652  \n",
      "Steps : 1712 , Loss : 0.7872856259346008, Weights 1.010343313217163, bias : 0.1053113117814064  \n",
      "Steps : 1713 , Loss : 0.7872706651687622, Weights 1.0104286670684814, bias : 0.10522360354661942  \n",
      "Steps : 1714 , Loss : 0.7872557044029236, Weights 1.0105137825012207, bias : 0.10513605177402496  \n",
      "Steps : 1715 , Loss : 0.7872408032417297, Weights 1.0105987787246704, bias : 0.10504866391420364  \n",
      "Steps : 1716 , Loss : 0.7872260808944702, Weights 1.0106836557388306, bias : 0.10496143996715546  \n",
      "Steps : 1717 , Loss : 0.7872112989425659, Weights 1.0107684135437012, bias : 0.1048743724822998  \n",
      "Steps : 1718 , Loss : 0.7871966361999512, Weights 1.0108529329299927, bias : 0.10478746891021729  \n",
      "Steps : 1719 , Loss : 0.7871820330619812, Weights 1.0109373331069946, bias : 0.1047007218003273  \n",
      "Steps : 1720 , Loss : 0.7871674299240112, Weights 1.011021614074707, bias : 0.10461413860321045  \n",
      "Steps : 1721 , Loss : 0.787152886390686, Weights 1.0111057758331299, bias : 0.10452771186828613  \n",
      "Steps : 1722 , Loss : 0.7871383428573608, Weights 1.0111898183822632, bias : 0.10444144904613495  \n",
      "Steps : 1723 , Loss : 0.7871239185333252, Weights 1.0112736225128174, bias : 0.1043553426861763  \n",
      "Steps : 1724 , Loss : 0.7871095538139343, Weights 1.011357307434082, bias : 0.10426939278841019  \n",
      "Steps : 1725 , Loss : 0.7870952486991882, Weights 1.0114408731460571, bias : 0.1041836068034172  \n",
      "Steps : 1726 , Loss : 0.7870810031890869, Weights 1.0115243196487427, bias : 0.10409797728061676  \n",
      "Steps : 1727 , Loss : 0.7870666980743408, Weights 1.0116076469421387, bias : 0.10401250422000885  \n",
      "Steps : 1728 , Loss : 0.7870525121688843, Weights 1.0116907358169556, bias : 0.10392718762159348  \n",
      "Steps : 1729 , Loss : 0.7870384454727173, Weights 1.011773705482483, bias : 0.10384203493595123  \n",
      "Steps : 1730 , Loss : 0.7870244383811951, Weights 1.0118565559387207, bias : 0.10375703871250153  \n",
      "Steps : 1731 , Loss : 0.7870102524757385, Weights 1.011939287185669, bias : 0.10367219895124435  \n",
      "Steps : 1732 , Loss : 0.7869963645935059, Weights 1.0120218992233276, bias : 0.10358751565217972  \n",
      "Steps : 1733 , Loss : 0.7869824171066284, Weights 1.0121042728424072, bias : 0.10350298881530762  \n",
      "Steps : 1734 , Loss : 0.7869685292243958, Weights 1.0121865272521973, bias : 0.10341861844062805  \n",
      "Steps : 1735 , Loss : 0.7869547009468079, Weights 1.0122686624526978, bias : 0.10333439707756042  \n",
      "Steps : 1736 , Loss : 0.7869409322738647, Weights 1.0123506784439087, bias : 0.10325033217668533  \n",
      "Steps : 1737 , Loss : 0.7869271636009216, Weights 1.01243257522583, bias : 0.10316642373800278  \n",
      "Steps : 1738 , Loss : 0.7869136333465576, Weights 1.0125142335891724, bias : 0.10308267176151276  \n",
      "Steps : 1739 , Loss : 0.7868998646736145, Weights 1.012595772743225, bias : 0.10299907624721527  \n",
      "Steps : 1740 , Loss : 0.7868863940238953, Weights 1.0126771926879883, bias : 0.10291562974452972  \n",
      "Steps : 1741 , Loss : 0.7868728637695312, Weights 1.012758493423462, bias : 0.10283233970403671  \n",
      "Steps : 1742 , Loss : 0.7868593335151672, Weights 1.012839674949646, bias : 0.10274920612573624  \n",
      "Steps : 1743 , Loss : 0.786845862865448, Weights 1.0129207372665405, bias : 0.1026662215590477  \n",
      "Steps : 1744 , Loss : 0.7868325114250183, Weights 1.013001561164856, bias : 0.1025833934545517  \n",
      "Steps : 1745 , Loss : 0.7868191599845886, Weights 1.0130822658538818, bias : 0.10250071436166763  \n",
      "Steps : 1746 , Loss : 0.7868058681488037, Weights 1.0131628513336182, bias : 0.1024181917309761  \n",
      "Steps : 1747 , Loss : 0.7867925763130188, Weights 1.013243317604065, bias : 0.10233581811189651  \n",
      "Steps : 1748 , Loss : 0.7867794036865234, Weights 1.0133236646652222, bias : 0.10225360095500946  \n",
      "Steps : 1749 , Loss : 0.7867662310600281, Weights 1.0134038925170898, bias : 0.10217153280973434  \n",
      "Steps : 1750 , Loss : 0.7867530584335327, Weights 1.0134838819503784, bias : 0.10208961367607117  \n",
      "Steps : 1751 , Loss : 0.7867401242256165, Weights 1.0135637521743774, bias : 0.10200785100460052  \n",
      "Steps : 1752 , Loss : 0.7867270708084106, Weights 1.013643503189087, bias : 0.10192623734474182  \n",
      "Steps : 1753 , Loss : 0.7867140769958496, Weights 1.0137231349945068, bias : 0.10184477269649506  \n",
      "Steps : 1754 , Loss : 0.7867011427879333, Weights 1.0138026475906372, bias : 0.10176345705986023  \n",
      "Steps : 1755 , Loss : 0.7866883277893066, Weights 1.013882040977478, bias : 0.10168229043483734  \n",
      "Steps : 1756 , Loss : 0.7866755127906799, Weights 1.0139613151550293, bias : 0.10160127282142639  \n",
      "Steps : 1757 , Loss : 0.786662757396698, Weights 1.0140403509140015, bias : 0.10152040421962738  \n",
      "Steps : 1758 , Loss : 0.7866500020027161, Weights 1.014119267463684, bias : 0.10143968462944031  \n",
      "Steps : 1759 , Loss : 0.7866372466087341, Weights 1.0141980648040771, bias : 0.10135911405086517  \n",
      "Steps : 1760 , Loss : 0.7866246104240417, Weights 1.0142767429351807, bias : 0.10127869248390198  \n",
      "Steps : 1761 , Loss : 0.7866120338439941, Weights 1.0143553018569946, bias : 0.10119841992855072  \n",
      "Steps : 1762 , Loss : 0.7865995168685913, Weights 1.014433741569519, bias : 0.1011182963848114  \n",
      "Steps : 1763 , Loss : 0.7865869998931885, Weights 1.014512062072754, bias : 0.10103832185268402  \n",
      "Steps : 1764 , Loss : 0.7865745425224304, Weights 1.0145901441574097, bias : 0.10095849633216858  \n",
      "Steps : 1765 , Loss : 0.7865620255470276, Weights 1.0146681070327759, bias : 0.10087881982326508  \n",
      "Steps : 1766 , Loss : 0.7865496873855591, Weights 1.0147459506988525, bias : 0.10079928487539291  \n",
      "Steps : 1767 , Loss : 0.7865374088287354, Weights 1.0148236751556396, bias : 0.10071989893913269  \n",
      "Steps : 1768 , Loss : 0.7865249514579773, Weights 1.0149012804031372, bias : 0.1006406620144844  \n",
      "Steps : 1769 , Loss : 0.7865128517150879, Weights 1.0149787664413452, bias : 0.10056156665086746  \n",
      "Steps : 1770 , Loss : 0.7865005731582642, Weights 1.0150561332702637, bias : 0.10048262029886246  \n",
      "Steps : 1771 , Loss : 0.7864883542060852, Weights 1.0151333808898926, bias : 0.1004038155078888  \n",
      "Steps : 1772 , Loss : 0.7864764332771301, Weights 1.0152103900909424, bias : 0.10032515972852707  \n",
      "Steps : 1773 , Loss : 0.7864642143249512, Weights 1.0152872800827026, bias : 0.10024664551019669  \n",
      "Steps : 1774 , Loss : 0.7864521741867065, Weights 1.0153640508651733, bias : 0.10016828030347824  \n",
      "Steps : 1775 , Loss : 0.7864402532577515, Weights 1.0154407024383545, bias : 0.10009005665779114  \n",
      "Steps : 1776 , Loss : 0.7864283919334412, Weights 1.015517234802246, bias : 0.10001198202371597  \n",
      "Steps : 1777 , Loss : 0.7864164113998413, Weights 1.0155936479568481, bias : 0.09993404895067215  \n",
      "Steps : 1778 , Loss : 0.786404550075531, Weights 1.0156699419021606, bias : 0.09985625743865967  \n",
      "Steps : 1779 , Loss : 0.7863927483558655, Weights 1.0157461166381836, bias : 0.09977860748767853  \n",
      "Steps : 1780 , Loss : 0.7863809466362, Weights 1.015822172164917, bias : 0.09970110654830933  \n",
      "Steps : 1781 , Loss : 0.786369264125824, Weights 1.0158981084823608, bias : 0.09962374716997147  \n",
      "Steps : 1782 , Loss : 0.786357581615448, Weights 1.0159738063812256, bias : 0.09954652935266495  \n",
      "Steps : 1783 , Loss : 0.7863459587097168, Weights 1.0160493850708008, bias : 0.09946945309638977  \n",
      "Steps : 1784 , Loss : 0.786334216594696, Weights 1.0161248445510864, bias : 0.09939251840114594  \n",
      "Steps : 1785 , Loss : 0.7863227725028992, Weights 1.0162001848220825, bias : 0.09931572526693344  \n",
      "Steps : 1786 , Loss : 0.7863112092018127, Weights 1.016275405883789, bias : 0.09923907369375229  \n",
      "Steps : 1787 , Loss : 0.7862997651100159, Weights 1.016350507736206, bias : 0.09916256368160248  \n",
      "Steps : 1788 , Loss : 0.7862882018089294, Weights 1.0164254903793335, bias : 0.09908619523048401  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1789 , Loss : 0.7862768769264221, Weights 1.0165003538131714, bias : 0.09900996834039688  \n",
      "Steps : 1790 , Loss : 0.78626549243927, Weights 1.0165750980377197, bias : 0.0989338830113411  \n",
      "Steps : 1791 , Loss : 0.7862541079521179, Weights 1.0166497230529785, bias : 0.09885793924331665  \n",
      "Steps : 1792 , Loss : 0.7862429618835449, Weights 1.0167242288589478, bias : 0.09878213703632355  \n",
      "Steps : 1793 , Loss : 0.7862316966056824, Weights 1.016798496246338, bias : 0.09870646893978119  \n",
      "Steps : 1794 , Loss : 0.7862205505371094, Weights 1.0168726444244385, bias : 0.09863094240427017  \n",
      "Steps : 1795 , Loss : 0.7862093448638916, Weights 1.0169466733932495, bias : 0.0985555574297905  \n",
      "Steps : 1796 , Loss : 0.7861981987953186, Weights 1.017020583152771, bias : 0.09848030656576157  \n",
      "Steps : 1797 , Loss : 0.7861871123313904, Weights 1.017094373703003, bias : 0.09840519726276398  \n",
      "Steps : 1798 , Loss : 0.7861759662628174, Weights 1.0171680450439453, bias : 0.09833022952079773  \n",
      "Steps : 1799 , Loss : 0.7861650586128235, Weights 1.0172415971755981, bias : 0.09825539588928223  \n",
      "Steps : 1800 , Loss : 0.7861540913581848, Weights 1.0173150300979614, bias : 0.09818070381879807  \n",
      "Steps : 1801 , Loss : 0.7861431837081909, Weights 1.0173883438110352, bias : 0.09810614585876465  \n",
      "Steps : 1802 , Loss : 0.786132276058197, Weights 1.0174615383148193, bias : 0.09803172945976257  \n",
      "Steps : 1803 , Loss : 0.7861215472221375, Weights 1.017534613609314, bias : 0.09795744717121124  \n",
      "Steps : 1804 , Loss : 0.7861106395721436, Weights 1.017607569694519, bias : 0.09788329899311066  \n",
      "Steps : 1805 , Loss : 0.7860998511314392, Weights 1.0176804065704346, bias : 0.09780929237604141  \n",
      "Steps : 1806 , Loss : 0.7860891819000244, Weights 1.0177531242370605, bias : 0.09773541986942291  \n",
      "Steps : 1807 , Loss : 0.7860785126686096, Weights 1.017825722694397, bias : 0.09766168147325516  \n",
      "Steps : 1808 , Loss : 0.78606778383255, Weights 1.0178982019424438, bias : 0.09758808463811874  \n",
      "Steps : 1809 , Loss : 0.7860572338104248, Weights 1.0179705619812012, bias : 0.09751462191343307  \n",
      "Steps : 1810 , Loss : 0.7860466241836548, Weights 1.0180426836013794, bias : 0.09744129329919815  \n",
      "Steps : 1811 , Loss : 0.7860360741615295, Weights 1.018114686012268, bias : 0.09736809879541397  \n",
      "Steps : 1812 , Loss : 0.7860256433486938, Weights 1.0181865692138672, bias : 0.09729503840208054  \n",
      "Steps : 1813 , Loss : 0.7860150933265686, Weights 1.0182583332061768, bias : 0.09722211211919785  \n",
      "Steps : 1814 , Loss : 0.7860047817230225, Weights 1.0183299779891968, bias : 0.0971493199467659  \n",
      "Steps : 1815 , Loss : 0.7859943509101868, Weights 1.0184015035629272, bias : 0.0970766618847847  \n",
      "Steps : 1816 , Loss : 0.7859839797019958, Weights 1.0184729099273682, bias : 0.09700413793325424  \n",
      "Steps : 1817 , Loss : 0.7859736680984497, Weights 1.0185441970825195, bias : 0.09693174809217453  \n",
      "Steps : 1818 , Loss : 0.7859634160995483, Weights 1.0186153650283813, bias : 0.09685949236154556  \n",
      "Steps : 1819 , Loss : 0.785953164100647, Weights 1.0186864137649536, bias : 0.09678737074136734  \n",
      "Steps : 1820 , Loss : 0.7859429717063904, Weights 1.0187573432922363, bias : 0.09671538323163986  \n",
      "Steps : 1821 , Loss : 0.7859327793121338, Weights 1.0188281536102295, bias : 0.09664352983236313  \n",
      "Steps : 1822 , Loss : 0.7859227061271667, Weights 1.018898844718933, bias : 0.09657181054353714  \n",
      "Steps : 1823 , Loss : 0.7859124541282654, Weights 1.0189694166183472, bias : 0.0965002179145813  \n",
      "Steps : 1824 , Loss : 0.7859025001525879, Weights 1.0190398693084717, bias : 0.0964287593960762  \n",
      "Steps : 1825 , Loss : 0.7858924269676208, Weights 1.0191102027893066, bias : 0.09635743498802185  \n",
      "Steps : 1826 , Loss : 0.7858824729919434, Weights 1.019180417060852, bias : 0.09628623723983765  \n",
      "Steps : 1827 , Loss : 0.7858725190162659, Weights 1.019250512123108, bias : 0.09621517360210419  \n",
      "Steps : 1828 , Loss : 0.7858625650405884, Weights 1.0193204879760742, bias : 0.09614424407482147  \n",
      "Steps : 1829 , Loss : 0.7858526706695557, Weights 1.019390344619751, bias : 0.0960734412074089  \n",
      "Steps : 1830 , Loss : 0.7858427166938782, Weights 1.0194600820541382, bias : 0.09600277245044708  \n",
      "Steps : 1831 , Loss : 0.7858330011367798, Weights 1.0195297002792358, bias : 0.09593223035335541  \n",
      "Steps : 1832 , Loss : 0.7858232855796814, Weights 1.019599199295044, bias : 0.09586182236671448  \n",
      "Steps : 1833 , Loss : 0.785813570022583, Weights 1.0196685791015625, bias : 0.0957915410399437  \n",
      "Steps : 1834 , Loss : 0.7858037352561951, Weights 1.0197378396987915, bias : 0.09572138637304306  \n",
      "Steps : 1835 , Loss : 0.7857940793037415, Weights 1.019806981086731, bias : 0.09565136581659317  \n",
      "Steps : 1836 , Loss : 0.7857844829559326, Weights 1.0198760032653809, bias : 0.09558147192001343  \n",
      "Steps : 1837 , Loss : 0.785774827003479, Weights 1.0199449062347412, bias : 0.09551170468330383  \n",
      "Steps : 1838 , Loss : 0.7857652306556702, Weights 1.020013689994812, bias : 0.09544207155704498  \n",
      "Steps : 1839 , Loss : 0.7857557535171509, Weights 1.0200823545455933, bias : 0.09537256509065628  \n",
      "Steps : 1840 , Loss : 0.7857462167739868, Weights 1.020150899887085, bias : 0.09530318528413773  \n",
      "Steps : 1841 , Loss : 0.7857367992401123, Weights 1.020219326019287, bias : 0.09523393213748932  \n",
      "Steps : 1842 , Loss : 0.7857273817062378, Weights 1.0202876329421997, bias : 0.09516480565071106  \n",
      "Steps : 1843 , Loss : 0.7857179045677185, Weights 1.0203558206558228, bias : 0.09509580582380295  \n",
      "Steps : 1844 , Loss : 0.7857086062431335, Weights 1.0204238891601562, bias : 0.09502693265676498  \n",
      "Steps : 1845 , Loss : 0.7856991291046143, Weights 1.0204918384552002, bias : 0.09495818614959717  \n",
      "Steps : 1846 , Loss : 0.7856900095939636, Weights 1.0205596685409546, bias : 0.0948895663022995  \n",
      "Steps : 1847 , Loss : 0.7856807112693787, Weights 1.0206273794174194, bias : 0.09482107311487198  \n",
      "Steps : 1848 , Loss : 0.7856714129447937, Weights 1.0206949710845947, bias : 0.0947527065873146  \n",
      "Steps : 1849 , Loss : 0.7856622338294983, Weights 1.0207624435424805, bias : 0.09468446671962738  \n",
      "Steps : 1850 , Loss : 0.7856530547142029, Weights 1.0208297967910767, bias : 0.0946163535118103  \n",
      "Steps : 1851 , Loss : 0.7856439352035522, Weights 1.0208970308303833, bias : 0.09454836696386337  \n",
      "Steps : 1852 , Loss : 0.7856347560882568, Weights 1.0209641456604004, bias : 0.09448050707578659  \n",
      "Steps : 1853 , Loss : 0.7856257557868958, Weights 1.021031141281128, bias : 0.09441277384757996  \n",
      "Steps : 1854 , Loss : 0.7856167554855347, Weights 1.021098017692566, bias : 0.09434515982866287  \n",
      "Steps : 1855 , Loss : 0.7856075763702393, Weights 1.0211647748947144, bias : 0.09427767246961594  \n",
      "Steps : 1856 , Loss : 0.7855987548828125, Weights 1.0212314128875732, bias : 0.09421031177043915  \n",
      "Steps : 1857 , Loss : 0.7855898141860962, Weights 1.0212979316711426, bias : 0.09414307028055191  \n",
      "Steps : 1858 , Loss : 0.7855808734893799, Weights 1.0213643312454224, bias : 0.09407595545053482  \n",
      "Steps : 1859 , Loss : 0.7855720520019531, Weights 1.0214306116104126, bias : 0.09400896728038788  \n",
      "Steps : 1860 , Loss : 0.7855632305145264, Weights 1.0214968919754028, bias : 0.09394209831953049  \n",
      "Steps : 1861 , Loss : 0.7855543494224548, Weights 1.0215630531311035, bias : 0.09387535601854324  \n",
      "Steps : 1862 , Loss : 0.7855455279350281, Weights 1.0216290950775146, bias : 0.09380873292684555  \n",
      "Steps : 1863 , Loss : 0.7855368256568909, Weights 1.0216950178146362, bias : 0.093742236495018  \n",
      "Steps : 1864 , Loss : 0.7855280041694641, Weights 1.0217608213424683, bias : 0.09367585927248001  \n",
      "Steps : 1865 , Loss : 0.7855194211006165, Weights 1.0218265056610107, bias : 0.09360960870981216  \n",
      "Steps : 1866 , Loss : 0.7855107188224792, Weights 1.0218920707702637, bias : 0.09354347735643387  \n",
      "Steps : 1867 , Loss : 0.7855020761489868, Weights 1.021957516670227, bias : 0.09347746521234512  \n",
      "Steps : 1868 , Loss : 0.7854934930801392, Weights 1.0220228433609009, bias : 0.09341157972812653  \n",
      "Steps : 1869 , Loss : 0.7854848504066467, Weights 1.0220880508422852, bias : 0.09334581345319748  \n",
      "Steps : 1870 , Loss : 0.7854764461517334, Weights 1.0221531391143799, bias : 0.09328016638755798  \n",
      "Steps : 1871 , Loss : 0.7854678630828857, Weights 1.022218108177185, bias : 0.09321464598178864  \n",
      "Steps : 1872 , Loss : 0.7854594588279724, Weights 1.0222829580307007, bias : 0.09314924478530884  \n",
      "Steps : 1873 , Loss : 0.7854510545730591, Weights 1.0223476886749268, bias : 0.09308396279811859  \n",
      "Steps : 1874 , Loss : 0.7854425311088562, Weights 1.0224123001098633, bias : 0.0930188000202179  \n",
      "Steps : 1875 , Loss : 0.7854341268539429, Weights 1.0224767923355103, bias : 0.09295375645160675  \n",
      "Steps : 1876 , Loss : 0.7854259014129639, Weights 1.0225411653518677, bias : 0.09288883209228516  \n",
      "Steps : 1877 , Loss : 0.7854174971580505, Weights 1.0226054191589355, bias : 0.09282402694225311  \n",
      "Steps : 1878 , Loss : 0.7854092121124268, Weights 1.0226696729660034, bias : 0.09275934100151062  \n",
      "Steps : 1879 , Loss : 0.7854008674621582, Weights 1.0227338075637817, bias : 0.09269477427005768  \n",
      "Steps : 1880 , Loss : 0.7853925824165344, Weights 1.0227978229522705, bias : 0.09263032674789429  \n",
      "Steps : 1881 , Loss : 0.7853844165802002, Weights 1.0228617191314697, bias : 0.09256599843502045  \n",
      "Steps : 1882 , Loss : 0.7853763103485107, Weights 1.0229254961013794, bias : 0.09250178933143616  \n",
      "Steps : 1883 , Loss : 0.7853681445121765, Weights 1.0229891538619995, bias : 0.09243769943714142  \n",
      "Steps : 1884 , Loss : 0.7853600978851318, Weights 1.02305269241333, bias : 0.09237372875213623  \n",
      "Steps : 1885 , Loss : 0.7853518724441528, Weights 1.023116111755371, bias : 0.0923098772764206  \n",
      "Steps : 1886 , Loss : 0.7853438854217529, Weights 1.0231794118881226, bias : 0.09224613755941391  \n",
      "Steps : 1887 , Loss : 0.7853358387947083, Weights 1.0232425928115845, bias : 0.09218251705169678  \n",
      "Steps : 1888 , Loss : 0.7853277325630188, Weights 1.0233056545257568, bias : 0.0921190157532692  \n",
      "Steps : 1889 , Loss : 0.7853198051452637, Weights 1.0233685970306396, bias : 0.09205563366413116  \n",
      "Steps : 1890 , Loss : 0.7853118181228638, Weights 1.0234315395355225, bias : 0.09199236333370209  \n",
      "Steps : 1891 , Loss : 0.7853039503097534, Weights 1.0234943628311157, bias : 0.09192921221256256  \n",
      "Steps : 1892 , Loss : 0.7852960228919983, Weights 1.0235570669174194, bias : 0.09186618030071259  \n",
      "Steps : 1893 , Loss : 0.7852881550788879, Weights 1.0236196517944336, bias : 0.09180326014757156  \n",
      "Steps : 1894 , Loss : 0.7852802872657776, Weights 1.0236821174621582, bias : 0.09174045920372009  \n",
      "Steps : 1895 , Loss : 0.7852725386619568, Weights 1.0237444639205933, bias : 0.09167777001857758  \n",
      "Steps : 1896 , Loss : 0.7852647304534912, Weights 1.0238066911697388, bias : 0.09161520004272461  \n",
      "Steps : 1897 , Loss : 0.7852569818496704, Weights 1.0238687992095947, bias : 0.0915527418255806  \n",
      "Steps : 1898 , Loss : 0.7852492332458496, Weights 1.0239307880401611, bias : 0.09149040281772614  \n",
      "Steps : 1899 , Loss : 0.7852415442466736, Weights 1.023992657661438, bias : 0.09142817556858063  \n",
      "Steps : 1900 , Loss : 0.7852339148521423, Weights 1.0240544080734253, bias : 0.09136606007814407  \n",
      "Steps : 1901 , Loss : 0.7852261066436768, Weights 1.0241161584854126, bias : 0.09130406379699707  \n",
      "Steps : 1902 , Loss : 0.7852186560630798, Weights 1.0241777896881104, bias : 0.09124217927455902  \n",
      "Steps : 1903 , Loss : 0.7852110266685486, Weights 1.0242393016815186, bias : 0.09118040651082993  \n",
      "Steps : 1904 , Loss : 0.7852034568786621, Weights 1.0243006944656372, bias : 0.09111875295639038  \n",
      "Steps : 1905 , Loss : 0.7851959466934204, Weights 1.0243619680404663, bias : 0.09105721116065979  \n",
      "Steps : 1906 , Loss : 0.7851884365081787, Weights 1.0244231224060059, bias : 0.09099578112363815  \n",
      "Steps : 1907 , Loss : 0.7851809859275818, Weights 1.0244841575622559, bias : 0.09093446284532547  \n",
      "Steps : 1908 , Loss : 0.7851734757423401, Weights 1.0245450735092163, bias : 0.09087325632572174  \n",
      "Steps : 1909 , Loss : 0.7851660847663879, Weights 1.0246058702468872, bias : 0.09081216901540756  \n",
      "Steps : 1910 , Loss : 0.7851586937904358, Weights 1.024666666984558, bias : 0.09075119346380234  \n",
      "Steps : 1911 , Loss : 0.7851513624191284, Weights 1.0247273445129395, bias : 0.09069032967090607  \n",
      "Steps : 1912 , Loss : 0.785144031047821, Weights 1.0247879028320312, bias : 0.09062957763671875  \n",
      "Steps : 1913 , Loss : 0.7851366400718689, Weights 1.0248483419418335, bias : 0.09056893736124039  \n",
      "Steps : 1914 , Loss : 0.7851293087005615, Weights 1.0249086618423462, bias : 0.09050840884447098  \n",
      "Steps : 1915 , Loss : 0.7851220965385437, Weights 1.0249688625335693, bias : 0.09044799208641052  \n",
      "Steps : 1916 , Loss : 0.7851147651672363, Weights 1.025028944015503, bias : 0.09038768708705902  \n",
      "Steps : 1917 , Loss : 0.7851075530052185, Weights 1.025088906288147, bias : 0.09032749384641647  \n",
      "Steps : 1918 , Loss : 0.7851004004478455, Weights 1.025148868560791, bias : 0.09026741236448288  \n",
      "Steps : 1919 , Loss : 0.7850932478904724, Weights 1.0252087116241455, bias : 0.09020743519067764  \n",
      "Steps : 1920 , Loss : 0.7850860357284546, Weights 1.0252684354782104, bias : 0.09014756977558136  \n",
      "Steps : 1921 , Loss : 0.7850790619850159, Weights 1.0253280401229858, bias : 0.09008781611919403  \n",
      "Steps : 1922 , Loss : 0.785071849822998, Weights 1.0253875255584717, bias : 0.09002817422151566  \n",
      "Steps : 1923 , Loss : 0.7850648760795593, Weights 1.025446891784668, bias : 0.08996864408254623  \n",
      "Steps : 1924 , Loss : 0.785057783126831, Weights 1.0255061388015747, bias : 0.08990921825170517  \n",
      "Steps : 1925 , Loss : 0.7850508093833923, Weights 1.0255653858184814, bias : 0.08984990417957306  \n",
      "Steps : 1926 , Loss : 0.7850437760353088, Weights 1.0256245136260986, bias : 0.0897907018661499  \n",
      "Steps : 1927 , Loss : 0.7850368022918701, Weights 1.0256835222244263, bias : 0.0897316038608551  \n",
      "Steps : 1928 , Loss : 0.7850298881530762, Weights 1.0257424116134644, bias : 0.08967261761426926  \n",
      "Steps : 1929 , Loss : 0.785023033618927, Weights 1.025801181793213, bias : 0.08961373567581177  \n",
      "Steps : 1930 , Loss : 0.7850161194801331, Weights 1.0258598327636719, bias : 0.08955496549606323  \n",
      "Steps : 1931 , Loss : 0.7850092053413391, Weights 1.0259183645248413, bias : 0.08949629962444305  \n",
      "Steps : 1932 , Loss : 0.7850022912025452, Weights 1.0259768962860107, bias : 0.08943774551153183  \n",
      "Steps : 1933 , Loss : 0.7849954962730408, Weights 1.0260353088378906, bias : 0.08937929570674896  \n",
      "Steps : 1934 , Loss : 0.7849887013435364, Weights 1.026093602180481, bias : 0.08932095766067505  \n",
      "Steps : 1935 , Loss : 0.7849819660186768, Weights 1.0261517763137817, bias : 0.08926272392272949  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 1936 , Loss : 0.7849752902984619, Weights 1.026209831237793, bias : 0.08920460194349289  \n",
      "Steps : 1937 , Loss : 0.7849685549736023, Weights 1.0262677669525146, bias : 0.08914658427238464  \n",
      "Steps : 1938 , Loss : 0.7849617600440979, Weights 1.0263257026672363, bias : 0.08908867090940475  \n",
      "Steps : 1939 , Loss : 0.7849551439285278, Weights 1.0263835191726685, bias : 0.08903086930513382  \n",
      "Steps : 1940 , Loss : 0.7849485278129578, Weights 1.026441216468811, bias : 0.08897317200899124  \n",
      "Steps : 1941 , Loss : 0.7849418520927429, Weights 1.026498794555664, bias : 0.08891557902097702  \n",
      "Steps : 1942 , Loss : 0.7849352955818176, Weights 1.0265562534332275, bias : 0.08885809034109116  \n",
      "Steps : 1943 , Loss : 0.7849286794662476, Weights 1.0266135931015015, bias : 0.08880071341991425  \n",
      "Steps : 1944 , Loss : 0.7849220037460327, Weights 1.0266709327697754, bias : 0.08874344080686569  \n",
      "Steps : 1945 , Loss : 0.7849156260490417, Weights 1.0267281532287598, bias : 0.0886862725019455  \n",
      "Steps : 1946 , Loss : 0.7849090695381165, Weights 1.0267852544784546, bias : 0.08862920850515366  \n",
      "Steps : 1947 , Loss : 0.7849026322364807, Weights 1.0268422365188599, bias : 0.08857224881649017  \n",
      "Steps : 1948 , Loss : 0.7848962545394897, Weights 1.0268990993499756, bias : 0.08851539343595505  \n",
      "Steps : 1949 , Loss : 0.7848896980285645, Weights 1.0269558429718018, bias : 0.08845864236354828  \n",
      "Steps : 1950 , Loss : 0.7848832607269287, Weights 1.027012586593628, bias : 0.08840199559926987  \n",
      "Steps : 1951 , Loss : 0.7848768830299377, Weights 1.0270692110061646, bias : 0.08834545314311981  \n",
      "Steps : 1952 , Loss : 0.784870445728302, Weights 1.0271257162094116, bias : 0.08828901499509811  \n",
      "Steps : 1953 , Loss : 0.7848641872406006, Weights 1.0271821022033691, bias : 0.08823268115520477  \n",
      "Steps : 1954 , Loss : 0.7848578095436096, Weights 1.027238368988037, bias : 0.08817645162343979  \n",
      "Steps : 1955 , Loss : 0.7848514318466187, Weights 1.027294635772705, bias : 0.08812032639980316  \n",
      "Steps : 1956 , Loss : 0.7848452925682068, Weights 1.0273507833480835, bias : 0.08806430548429489  \n",
      "Steps : 1957 , Loss : 0.7848390340805054, Weights 1.0274068117141724, bias : 0.08800838887691498  \n",
      "Steps : 1958 , Loss : 0.7848325967788696, Weights 1.0274627208709717, bias : 0.08795257657766342  \n",
      "Steps : 1959 , Loss : 0.7848265171051025, Weights 1.0275185108184814, bias : 0.08789686113595963  \n",
      "Steps : 1960 , Loss : 0.7848202586174011, Weights 1.0275741815567017, bias : 0.08784125000238419  \n",
      "Steps : 1961 , Loss : 0.7848141193389893, Weights 1.0276298522949219, bias : 0.0877857431769371  \n",
      "Steps : 1962 , Loss : 0.7848079800605774, Weights 1.0276854038238525, bias : 0.08773034065961838  \n",
      "Steps : 1963 , Loss : 0.7848019599914551, Weights 1.0277408361434937, bias : 0.08767503499984741  \n",
      "Steps : 1964 , Loss : 0.7847958207130432, Weights 1.0277961492538452, bias : 0.0876198336482048  \n",
      "Steps : 1965 , Loss : 0.7847897410392761, Weights 1.0278513431549072, bias : 0.08756473660469055  \n",
      "Steps : 1966 , Loss : 0.7847836017608643, Weights 1.0279065370559692, bias : 0.08750973641872406  \n",
      "Steps : 1967 , Loss : 0.7847775220870972, Weights 1.0279616117477417, bias : 0.08745484054088593  \n",
      "Steps : 1968 , Loss : 0.7847715616226196, Weights 1.0280165672302246, bias : 0.08740004152059555  \n",
      "Steps : 1969 , Loss : 0.7847655415534973, Weights 1.028071403503418, bias : 0.08734534680843353  \n",
      "Steps : 1970 , Loss : 0.7847597002983093, Weights 1.0281261205673218, bias : 0.08729074895381927  \n",
      "Steps : 1971 , Loss : 0.7847536206245422, Weights 1.0281808376312256, bias : 0.08723625540733337  \n",
      "Steps : 1972 , Loss : 0.7847477197647095, Weights 1.0282354354858398, bias : 0.08718185871839523  \n",
      "Steps : 1973 , Loss : 0.7847418189048767, Weights 1.0282899141311646, bias : 0.08712756633758545  \n",
      "Steps : 1974 , Loss : 0.7847358584403992, Weights 1.0283442735671997, bias : 0.08707337081432343  \n",
      "Steps : 1975 , Loss : 0.784730076789856, Weights 1.0283986330032349, bias : 0.08701927959918976  \n",
      "Steps : 1976 , Loss : 0.7847241759300232, Weights 1.0284528732299805, bias : 0.08696528524160385  \n",
      "Steps : 1977 , Loss : 0.78471839427948, Weights 1.0285069942474365, bias : 0.0869113877415657  \n",
      "Steps : 1978 , Loss : 0.784712553024292, Weights 1.028560996055603, bias : 0.08685759454965591  \n",
      "Steps : 1979 , Loss : 0.7847068309783936, Weights 1.02861487865448, bias : 0.08680389821529388  \n",
      "Steps : 1980 , Loss : 0.7847009897232056, Weights 1.028668761253357, bias : 0.08675029873847961  \n",
      "Steps : 1981 , Loss : 0.7846952080726624, Weights 1.0287225246429443, bias : 0.0866967961192131  \n",
      "Steps : 1982 , Loss : 0.7846895456314087, Weights 1.0287761688232422, bias : 0.08664339780807495  \n",
      "Steps : 1983 , Loss : 0.7846838235855103, Weights 1.0288296937942505, bias : 0.08659009635448456  \n",
      "Steps : 1984 , Loss : 0.7846782207489014, Weights 1.0288832187652588, bias : 0.08653689175844193  \n",
      "Steps : 1985 , Loss : 0.7846724987030029, Weights 1.0289366245269775, bias : 0.08648378401994705  \n",
      "Steps : 1986 , Loss : 0.7846668362617493, Weights 1.0289899110794067, bias : 0.08643077313899994  \n",
      "Steps : 1987 , Loss : 0.7846611142158508, Weights 1.0290430784225464, bias : 0.08637785911560059  \n",
      "Steps : 1988 , Loss : 0.7846556305885315, Weights 1.0290961265563965, bias : 0.08632504194974899  \n",
      "Steps : 1989 , Loss : 0.7846499681472778, Weights 1.0291491746902466, bias : 0.08627232909202576  \n",
      "Steps : 1990 , Loss : 0.7846444845199585, Weights 1.0292021036148071, bias : 0.08621971309185028  \n",
      "Steps : 1991 , Loss : 0.7846388220787048, Weights 1.0292549133300781, bias : 0.08616719394922256  \n",
      "Steps : 1992 , Loss : 0.7846333384513855, Weights 1.0293076038360596, bias : 0.08611476421356201  \n",
      "Steps : 1993 , Loss : 0.7846278548240662, Weights 1.029360294342041, bias : 0.08606243133544922  \n",
      "Steps : 1994 , Loss : 0.7846223711967468, Weights 1.029412865638733, bias : 0.08601019531488419  \n",
      "Steps : 1995 , Loss : 0.7846169471740723, Weights 1.0294653177261353, bias : 0.08595805615186691  \n",
      "Steps : 1996 , Loss : 0.7846114635467529, Weights 1.029517650604248, bias : 0.0859060138463974  \n",
      "Steps : 1997 , Loss : 0.7846060991287231, Weights 1.0295699834823608, bias : 0.08585406839847565  \n",
      "Steps : 1998 , Loss : 0.7846006751060486, Weights 1.029622197151184, bias : 0.08580221980810165  \n",
      "Steps : 1999 , Loss : 0.784595251083374, Weights 1.0296742916107178, bias : 0.08575046807527542  \n",
      "Steps : 2000 , Loss : 0.784589946269989, Weights 1.029726266860962, bias : 0.08569880574941635  \n",
      "Steps : 2001 , Loss : 0.7845845222473145, Weights 1.029778242111206, bias : 0.08564724028110504  \n",
      "Steps : 2002 , Loss : 0.7845791578292847, Weights 1.0298300981521606, bias : 0.08559577167034149  \n",
      "Steps : 2003 , Loss : 0.7845739126205444, Weights 1.0298818349838257, bias : 0.0855443999171257  \n",
      "Steps : 2004 , Loss : 0.7845684885978699, Weights 1.0299334526062012, bias : 0.08549311757087708  \n",
      "Steps : 2005 , Loss : 0.7845633029937744, Weights 1.0299850702285767, bias : 0.08544193208217621  \n",
      "Steps : 2006 , Loss : 0.784558117389679, Weights 1.0300365686416626, bias : 0.0853908434510231  \n",
      "Steps : 2007 , Loss : 0.784552812576294, Weights 1.030087947845459, bias : 0.08533984422683716  \n",
      "Steps : 2008 , Loss : 0.7845475673675537, Weights 1.0301392078399658, bias : 0.08528894186019897  \n",
      "Steps : 2009 , Loss : 0.7845423817634583, Weights 1.0301904678344727, bias : 0.08523812890052795  \n",
      "Steps : 2010 , Loss : 0.7845371961593628, Weights 1.03024160861969, bias : 0.0851874127984047  \n",
      "Steps : 2011 , Loss : 0.7845320701599121, Weights 1.0302926301956177, bias : 0.0851367861032486  \n",
      "Steps : 2012 , Loss : 0.7845270037651062, Weights 1.0303435325622559, bias : 0.08508625626564026  \n",
      "Steps : 2013 , Loss : 0.7845218181610107, Weights 1.030394434928894, bias : 0.08503581583499908  \n",
      "Steps : 2014 , Loss : 0.7845167517662048, Weights 1.0304452180862427, bias : 0.08498547226190567  \n",
      "Steps : 2015 , Loss : 0.7845115661621094, Weights 1.0304958820343018, bias : 0.08493521809577942  \n",
      "Steps : 2016 , Loss : 0.7845065593719482, Weights 1.0305464267730713, bias : 0.08488506078720093  \n",
      "Steps : 2017 , Loss : 0.7845014929771423, Weights 1.0305969715118408, bias : 0.0848349928855896  \n",
      "Steps : 2018 , Loss : 0.784496545791626, Weights 1.0306473970413208, bias : 0.08478501439094543  \n",
      "Steps : 2019 , Loss : 0.7844914197921753, Weights 1.0306977033615112, bias : 0.08473513275384903  \n",
      "Steps : 2020 , Loss : 0.7844864726066589, Weights 1.030747890472412, bias : 0.08468534052371979  \n",
      "Steps : 2021 , Loss : 0.7844815254211426, Weights 1.030798077583313, bias : 0.08463563770055771  \n",
      "Steps : 2022 , Loss : 0.7844764590263367, Weights 1.0308481454849243, bias : 0.08458603173494339  \n",
      "Steps : 2023 , Loss : 0.7844715714454651, Weights 1.030898094177246, bias : 0.08453651517629623  \n",
      "Steps : 2024 , Loss : 0.784466564655304, Weights 1.0309480428695679, bias : 0.08448708802461624  \n",
      "Steps : 2025 , Loss : 0.7844616770744324, Weights 1.0309978723526, bias : 0.08443775027990341  \n",
      "Steps : 2026 , Loss : 0.7844567894935608, Weights 1.0310475826263428, bias : 0.08438850939273834  \n",
      "Steps : 2027 , Loss : 0.7844519019126892, Weights 1.031097173690796, bias : 0.08433935791254044  \n",
      "Steps : 2028 , Loss : 0.7844470739364624, Weights 1.031146764755249, bias : 0.08429029583930969  \n",
      "Steps : 2029 , Loss : 0.7844421863555908, Weights 1.0311962366104126, bias : 0.08424132317304611  \n",
      "Steps : 2030 , Loss : 0.7844374179840088, Weights 1.0312455892562866, bias : 0.0841924399137497  \n",
      "Steps : 2031 , Loss : 0.7844326496124268, Weights 1.0312949419021606, bias : 0.08414364606142044  \n",
      "Steps : 2032 , Loss : 0.7844278812408447, Weights 1.0313441753387451, bias : 0.08409494161605835  \n",
      "Steps : 2033 , Loss : 0.7844229936599731, Weights 1.03139328956604, bias : 0.08404632657766342  \n",
      "Steps : 2034 , Loss : 0.7844183444976807, Weights 1.0314422845840454, bias : 0.08399780094623566  \n",
      "Steps : 2035 , Loss : 0.7844135761260986, Weights 1.0314912796020508, bias : 0.08394936472177505  \n",
      "Steps : 2036 , Loss : 0.7844087481498718, Weights 1.0315401554107666, bias : 0.08390101790428162  \n",
      "Steps : 2037 , Loss : 0.7844041585922241, Weights 1.0315889120101929, bias : 0.08385276049375534  \n",
      "Steps : 2038 , Loss : 0.7843993902206421, Weights 1.0316376686096191, bias : 0.08380459249019623  \n",
      "Steps : 2039 , Loss : 0.7843947410583496, Weights 1.0316863059997559, bias : 0.08375651389360428  \n",
      "Steps : 2040 , Loss : 0.7843900918960571, Weights 1.031734824180603, bias : 0.08370852470397949  \n",
      "Steps : 2041 , Loss : 0.7843855023384094, Weights 1.0317832231521606, bias : 0.08366062492132187  \n",
      "Steps : 2042 , Loss : 0.7843807935714722, Weights 1.0318316221237183, bias : 0.08361280709505081  \n",
      "Steps : 2043 , Loss : 0.7843762040138245, Weights 1.0318799018859863, bias : 0.08356507867574692  \n",
      "Steps : 2044 , Loss : 0.7843716144561768, Weights 1.0319280624389648, bias : 0.08351743966341019  \n",
      "Steps : 2045 , Loss : 0.7843670845031738, Weights 1.0319762229919434, bias : 0.08346989005804062  \n",
      "Steps : 2046 , Loss : 0.7843624949455261, Weights 1.0320242643356323, bias : 0.08342242985963821  \n",
      "Steps : 2047 , Loss : 0.7843579649925232, Weights 1.0320721864700317, bias : 0.08337505161762238  \n",
      "Steps : 2048 , Loss : 0.784353494644165, Weights 1.0321201086044312, bias : 0.0833277627825737  \n",
      "Steps : 2049 , Loss : 0.7843489050865173, Weights 1.032167911529541, bias : 0.08328056335449219  \n",
      "Steps : 2050 , Loss : 0.784344494342804, Weights 1.0322155952453613, bias : 0.08323344588279724  \n",
      "Steps : 2051 , Loss : 0.7843399047851562, Weights 1.0322632789611816, bias : 0.08318641781806946  \n",
      "Steps : 2052 , Loss : 0.7843354940414429, Weights 1.0323108434677124, bias : 0.08313947170972824  \n",
      "Steps : 2053 , Loss : 0.7843310832977295, Weights 1.0323582887649536, bias : 0.08309261500835419  \n",
      "Steps : 2054 , Loss : 0.7843266129493713, Weights 1.0324056148529053, bias : 0.0830458477139473  \n",
      "Steps : 2055 , Loss : 0.784322202205658, Weights 1.032452940940857, bias : 0.08299916237592697  \n",
      "Steps : 2056 , Loss : 0.7843177914619446, Weights 1.032500147819519, bias : 0.08295256644487381  \n",
      "Steps : 2057 , Loss : 0.784313440322876, Weights 1.0325472354888916, bias : 0.08290605247020721  \n",
      "Steps : 2058 , Loss : 0.7843090891838074, Weights 1.0325943231582642, bias : 0.08285962790250778  \n",
      "Steps : 2059 , Loss : 0.784304678440094, Weights 1.0326412916183472, bias : 0.08281328529119492  \n",
      "Steps : 2060 , Loss : 0.7843003869056702, Weights 1.0326881408691406, bias : 0.08276703208684921  \n",
      "Steps : 2061 , Loss : 0.7842960953712463, Weights 1.032734990119934, bias : 0.08272086083889008  \n",
      "Steps : 2062 , Loss : 0.7842917442321777, Weights 1.032781720161438, bias : 0.0826747715473175  \n",
      "Steps : 2063 , Loss : 0.7842875123023987, Weights 1.0328283309936523, bias : 0.0826287716627121  \n",
      "Steps : 2064 , Loss : 0.7842832207679749, Weights 1.0328749418258667, bias : 0.08258285373449326  \n",
      "Steps : 2065 , Loss : 0.784278929233551, Weights 1.0329214334487915, bias : 0.08253701776266098  \n",
      "Steps : 2066 , Loss : 0.7842747569084167, Weights 1.0329678058624268, bias : 0.08249127119779587  \n",
      "Steps : 2067 , Loss : 0.7842704653739929, Weights 1.033014178276062, bias : 0.08244560658931732  \n",
      "Steps : 2068 , Loss : 0.7842662930488586, Weights 1.0330604314804077, bias : 0.08240002393722534  \n",
      "Steps : 2069 , Loss : 0.7842620611190796, Weights 1.0331065654754639, bias : 0.08235453069210052  \n",
      "Steps : 2070 , Loss : 0.7842578887939453, Weights 1.03315269947052, bias : 0.08230911940336227  \n",
      "Steps : 2071 , Loss : 0.784253716468811, Weights 1.0331987142562866, bias : 0.08226379007101059  \n",
      "Steps : 2072 , Loss : 0.7842495441436768, Weights 1.0332446098327637, bias : 0.08221854269504547  \n",
      "Steps : 2073 , Loss : 0.7842453718185425, Weights 1.0332905054092407, bias : 0.08217337727546692  \n",
      "Steps : 2074 , Loss : 0.784241259098053, Weights 1.0333362817764282, bias : 0.08212830126285553  \n",
      "Steps : 2075 , Loss : 0.7842372059822083, Weights 1.0333819389343262, bias : 0.0820833072066307  \n",
      "Steps : 2076 , Loss : 0.7842330932617188, Weights 1.0334275960922241, bias : 0.08203839510679245  \n",
      "Steps : 2077 , Loss : 0.7842289805412292, Weights 1.0334731340408325, bias : 0.08199356496334076  \n",
      "Steps : 2078 , Loss : 0.7842249274253845, Weights 1.0335185527801514, bias : 0.08194881677627563  \n",
      "Steps : 2079 , Loss : 0.7842208743095398, Weights 1.0335639715194702, bias : 0.08190415054559708  \n",
      "Steps : 2080 , Loss : 0.7842169404029846, Weights 1.0336092710494995, bias : 0.08185956627130508  \n",
      "Steps : 2081 , Loss : 0.7842128872871399, Weights 1.0336544513702393, bias : 0.08181506395339966  \n",
      "Steps : 2082 , Loss : 0.7842088937759399, Weights 1.033699631690979, bias : 0.0817706435918808  \n",
      "Steps : 2083 , Loss : 0.7842048406600952, Weights 1.0337446928024292, bias : 0.0817263051867485  \n",
      "Steps : 2084 , Loss : 0.78420090675354, Weights 1.0337896347045898, bias : 0.08168204873800278  \n",
      "Steps : 2085 , Loss : 0.7841968536376953, Weights 1.0338345766067505, bias : 0.08163787424564362  \n",
      "Steps : 2086 , Loss : 0.7841929793357849, Weights 1.0338793992996216, bias : 0.08159378170967102  \n",
      "Steps : 2087 , Loss : 0.784188985824585, Weights 1.0339241027832031, bias : 0.08154977113008499  \n",
      "Steps : 2088 , Loss : 0.7841851115226746, Weights 1.0339688062667847, bias : 0.08150583505630493  \n",
      "Steps : 2089 , Loss : 0.7841811180114746, Weights 1.0340133905410767, bias : 0.08146198093891144  \n",
      "Steps : 2090 , Loss : 0.7841772437095642, Weights 1.034057855606079, bias : 0.08141820877790451  \n",
      "Steps : 2091 , Loss : 0.7841734290122986, Weights 1.0341023206710815, bias : 0.08137451857328415  \n",
      "Steps : 2092 , Loss : 0.784169614315033, Weights 1.0341466665267944, bias : 0.08133091032505035  \n",
      "Steps : 2093 , Loss : 0.784165620803833, Weights 1.0341910123825073, bias : 0.08128737658262253  \n",
      "Steps : 2094 , Loss : 0.7841618061065674, Weights 1.0342352390289307, bias : 0.08124392479658127  \n",
      "Steps : 2095 , Loss : 0.7841579914093018, Weights 1.0342793464660645, bias : 0.08120055496692657  \n",
      "Steps : 2096 , Loss : 0.7841541767120361, Weights 1.0343234539031982, bias : 0.08115726709365845  \n",
      "Steps : 2097 , Loss : 0.7841503620147705, Weights 1.0343674421310425, bias : 0.08111405372619629  \n",
      "Steps : 2098 , Loss : 0.7841465473175049, Weights 1.0344113111495972, bias : 0.0810709223151207  \n",
      "Steps : 2099 , Loss : 0.7841429114341736, Weights 1.0344551801681519, bias : 0.08102787286043167  \n",
      "Steps : 2100 , Loss : 0.784139096736908, Weights 1.034498929977417, bias : 0.08098489791154861  \n",
      "Steps : 2101 , Loss : 0.7841354608535767, Weights 1.0345425605773926, bias : 0.08094200491905212  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2102 , Loss : 0.784131646156311, Weights 1.0345861911773682, bias : 0.0808991864323616  \n",
      "Steps : 2103 , Loss : 0.7841278910636902, Weights 1.0346297025680542, bias : 0.08085644990205765  \n",
      "Steps : 2104 , Loss : 0.7841241359710693, Weights 1.0346732139587402, bias : 0.08081378787755966  \n",
      "Steps : 2105 , Loss : 0.784120500087738, Weights 1.0347166061401367, bias : 0.08077120780944824  \n",
      "Steps : 2106 , Loss : 0.784116804599762, Weights 1.0347598791122437, bias : 0.08072870969772339  \n",
      "Steps : 2107 , Loss : 0.7841131687164307, Weights 1.0348031520843506, bias : 0.0806862860918045  \n",
      "Steps : 2108 , Loss : 0.7841094732284546, Weights 1.034846305847168, bias : 0.08064393699169159  \n",
      "Steps : 2109 , Loss : 0.7841058969497681, Weights 1.0348893404006958, bias : 0.08060166984796524  \n",
      "Steps : 2110 , Loss : 0.7841023206710815, Weights 1.0349323749542236, bias : 0.08055947721004486  \n",
      "Steps : 2111 , Loss : 0.7840986847877502, Weights 1.034975290298462, bias : 0.08051736652851105  \n",
      "Steps : 2112 , Loss : 0.784095048904419, Weights 1.0350182056427002, bias : 0.0804753303527832  \n",
      "Steps : 2113 , Loss : 0.7840914726257324, Weights 1.035061001777649, bias : 0.08043337613344193  \n",
      "Steps : 2114 , Loss : 0.7840879559516907, Weights 1.035103678703308, bias : 0.08039149641990662  \n",
      "Steps : 2115 , Loss : 0.7840843200683594, Weights 1.0351463556289673, bias : 0.08034969121217728  \n",
      "Steps : 2116 , Loss : 0.7840808033943176, Weights 1.035188913345337, bias : 0.0803079679608345  \n",
      "Steps : 2117 , Loss : 0.7840771675109863, Weights 1.035231351852417, bias : 0.0802663192152977  \n",
      "Steps : 2118 , Loss : 0.7840736508369446, Weights 1.035273790359497, bias : 0.08022474497556686  \n",
      "Steps : 2119 , Loss : 0.7840701341629028, Weights 1.0353161096572876, bias : 0.080183245241642  \n",
      "Steps : 2120 , Loss : 0.7840666174888611, Weights 1.0353584289550781, bias : 0.0801418274641037  \n",
      "Steps : 2121 , Loss : 0.7840632796287537, Weights 1.035400629043579, bias : 0.08010048419237137  \n",
      "Steps : 2122 , Loss : 0.7840597629547119, Weights 1.0354427099227905, bias : 0.08005921542644501  \n",
      "Steps : 2123 , Loss : 0.7840562462806702, Weights 1.035484790802002, bias : 0.08001802116632462  \n",
      "Steps : 2124 , Loss : 0.7840527296066284, Weights 1.0355267524719238, bias : 0.07997690141201019  \n",
      "Steps : 2125 , Loss : 0.784049391746521, Weights 1.0355687141418457, bias : 0.07993585616350174  \n",
      "Steps : 2126 , Loss : 0.7840459942817688, Weights 1.035610556602478, bias : 0.07989489287137985  \n",
      "Steps : 2127 , Loss : 0.7840425968170166, Weights 1.0356522798538208, bias : 0.07985400408506393  \n",
      "Steps : 2128 , Loss : 0.7840391397476196, Weights 1.0356940031051636, bias : 0.07981318980455399  \n",
      "Steps : 2129 , Loss : 0.7840358018875122, Weights 1.0357356071472168, bias : 0.07977245002985  \n",
      "Steps : 2130 , Loss : 0.7840324640274048, Weights 1.03577721118927, bias : 0.079731784760952  \n",
      "Steps : 2131 , Loss : 0.7840290665626526, Weights 1.0358186960220337, bias : 0.07969119399785995  \n",
      "Steps : 2132 , Loss : 0.7840255498886108, Weights 1.0358600616455078, bias : 0.07965067774057388  \n",
      "Steps : 2133 , Loss : 0.7840223908424377, Weights 1.035901427268982, bias : 0.07961023598909378  \n",
      "Steps : 2134 , Loss : 0.784018874168396, Weights 1.0359426736831665, bias : 0.07956986874341965  \n",
      "Steps : 2135 , Loss : 0.7840155959129333, Weights 1.035983920097351, bias : 0.07952957600355148  \n",
      "Steps : 2136 , Loss : 0.7840123176574707, Weights 1.036025047302246, bias : 0.07948935776948929  \n",
      "Steps : 2137 , Loss : 0.7840090394020081, Weights 1.0360660552978516, bias : 0.07944921404123306  \n",
      "Steps : 2138 , Loss : 0.7840057611465454, Weights 1.036107063293457, bias : 0.0794091448187828  \n",
      "Steps : 2139 , Loss : 0.7840025424957275, Weights 1.036147952079773, bias : 0.07936915010213852  \n",
      "Steps : 2140 , Loss : 0.7839992642402649, Weights 1.0361888408660889, bias : 0.0793292224407196  \n",
      "Steps : 2141 , Loss : 0.7839959859848022, Weights 1.0362296104431152, bias : 0.07928936928510666  \n",
      "Steps : 2142 , Loss : 0.7839927673339844, Weights 1.036270260810852, bias : 0.07924959063529968  \n",
      "Steps : 2143 , Loss : 0.7839895486831665, Weights 1.0363109111785889, bias : 0.07920988649129868  \n",
      "Steps : 2144 , Loss : 0.7839863300323486, Weights 1.0363514423370361, bias : 0.07917025685310364  \n",
      "Steps : 2145 , Loss : 0.7839831709861755, Weights 1.0363919734954834, bias : 0.07913069427013397  \n",
      "Steps : 2146 , Loss : 0.7839799523353577, Weights 1.0364323854446411, bias : 0.07909120619297028  \n",
      "Steps : 2147 , Loss : 0.7839767336845398, Weights 1.0364726781845093, bias : 0.07905179262161255  \n",
      "Steps : 2148 , Loss : 0.7839736938476562, Weights 1.0365129709243774, bias : 0.07901245355606079  \n",
      "Steps : 2149 , Loss : 0.7839704751968384, Weights 1.036553144454956, bias : 0.0789731815457344  \n",
      "Steps : 2150 , Loss : 0.7839672565460205, Weights 1.0365933179855347, bias : 0.07893398404121399  \n",
      "Steps : 2151 , Loss : 0.7839641571044922, Weights 1.0366333723068237, bias : 0.07889486104249954  \n",
      "Steps : 2152 , Loss : 0.7839610576629639, Weights 1.0366734266281128, bias : 0.07885580509901047  \n",
      "Steps : 2153 , Loss : 0.7839578986167908, Weights 1.0367133617401123, bias : 0.07881682366132736  \n",
      "Steps : 2154 , Loss : 0.7839547991752625, Weights 1.0367531776428223, bias : 0.07877791672945023  \n",
      "Steps : 2155 , Loss : 0.7839516997337341, Weights 1.0367929935455322, bias : 0.07873907685279846  \n",
      "Steps : 2156 , Loss : 0.7839486598968506, Weights 1.0368326902389526, bias : 0.07870031148195267  \n",
      "Steps : 2157 , Loss : 0.783945620059967, Weights 1.036872386932373, bias : 0.07866161316633224  \n",
      "Steps : 2158 , Loss : 0.7839425802230835, Weights 1.036911964416504, bias : 0.07862298935651779  \n",
      "Steps : 2159 , Loss : 0.7839395403862, Weights 1.0369514226913452, bias : 0.07858443260192871  \n",
      "Steps : 2160 , Loss : 0.7839364409446716, Weights 1.0369908809661865, bias : 0.0785459503531456  \n",
      "Steps : 2161 , Loss : 0.7839334011077881, Weights 1.0370302200317383, bias : 0.07850753515958786  \n",
      "Steps : 2162 , Loss : 0.7839304208755493, Weights 1.03706955909729, bias : 0.07846919447183609  \n",
      "Steps : 2163 , Loss : 0.7839273810386658, Weights 1.0371087789535522, bias : 0.07843092083930969  \n",
      "Steps : 2164 , Loss : 0.783924400806427, Weights 1.0371479988098145, bias : 0.07839272171258926  \n",
      "Steps : 2165 , Loss : 0.7839214205741882, Weights 1.037187099456787, bias : 0.07835458964109421  \n",
      "Steps : 2166 , Loss : 0.7839184403419495, Weights 1.0372260808944702, bias : 0.07831653207540512  \n",
      "Steps : 2167 , Loss : 0.7839155793190002, Weights 1.0372650623321533, bias : 0.0782785415649414  \n",
      "Steps : 2168 , Loss : 0.7839125990867615, Weights 1.0373039245605469, bias : 0.07824061810970306  \n",
      "Steps : 2169 , Loss : 0.7839096188545227, Weights 1.0373427867889404, bias : 0.07820276916027069  \n",
      "Steps : 2170 , Loss : 0.7839066982269287, Weights 1.0373815298080444, bias : 0.07816498726606369  \n",
      "Steps : 2171 , Loss : 0.7839038372039795, Weights 1.0374202728271484, bias : 0.07812727242708206  \n",
      "Steps : 2172 , Loss : 0.7839009165763855, Weights 1.037458896636963, bias : 0.0780896320939064  \n",
      "Steps : 2173 , Loss : 0.783897876739502, Weights 1.0374975204467773, bias : 0.07805205881595612  \n",
      "Steps : 2174 , Loss : 0.7838950157165527, Weights 1.0375360250473022, bias : 0.0780145525932312  \n",
      "Steps : 2175 , Loss : 0.7838922142982483, Weights 1.0375744104385376, bias : 0.07797711342573166  \n",
      "Steps : 2176 , Loss : 0.7838894128799438, Weights 1.037612795829773, bias : 0.07793974876403809  \n",
      "Steps : 2177 , Loss : 0.7838864326477051, Weights 1.0376510620117188, bias : 0.07790245115756989  \n",
      "Steps : 2178 , Loss : 0.7838836908340454, Weights 1.0376893281936646, bias : 0.07786522060632706  \n",
      "Steps : 2179 , Loss : 0.7838808298110962, Weights 1.0377274751663208, bias : 0.0778280571103096  \n",
      "Steps : 2180 , Loss : 0.7838780283927917, Weights 1.037765622138977, bias : 0.07779096066951752  \n",
      "Steps : 2181 , Loss : 0.7838751077651978, Weights 1.0378036499023438, bias : 0.0777539387345314  \n",
      "Steps : 2182 , Loss : 0.7838723659515381, Weights 1.0378416776657104, bias : 0.07771698385477066  \n",
      "Steps : 2183 , Loss : 0.7838695645332336, Weights 1.0378795862197876, bias : 0.07768009603023529  \n",
      "Steps : 2184 , Loss : 0.7838667035102844, Weights 1.0379173755645752, bias : 0.07764327526092529  \n",
      "Steps : 2185 , Loss : 0.7838640809059143, Weights 1.0379551649093628, bias : 0.07760652154684067  \n",
      "Steps : 2186 , Loss : 0.7838612198829651, Weights 1.0379928350448608, bias : 0.07756983488798141  \n",
      "Steps : 2187 , Loss : 0.7838584780693054, Weights 1.0380305051803589, bias : 0.07753321528434753  \n",
      "Steps : 2188 , Loss : 0.7838557362556458, Weights 1.0380680561065674, bias : 0.07749666273593903  \n",
      "Steps : 2189 , Loss : 0.7838530540466309, Weights 1.0381056070327759, bias : 0.07746017724275589  \n",
      "Steps : 2190 , Loss : 0.7838503122329712, Weights 1.0381430387496948, bias : 0.07742375880479813  \n",
      "Steps : 2191 , Loss : 0.7838475704193115, Weights 1.0381804704666138, bias : 0.07738740742206573  \n",
      "Steps : 2192 , Loss : 0.7838448286056519, Weights 1.0382177829742432, bias : 0.07735112309455872  \n",
      "Steps : 2193 , Loss : 0.783842146396637, Weights 1.0382550954818726, bias : 0.07731490582227707  \n",
      "Steps : 2194 , Loss : 0.7838394045829773, Weights 1.0382922887802124, bias : 0.0772787556052208  \n",
      "Steps : 2195 , Loss : 0.783836841583252, Weights 1.0383293628692627, bias : 0.07724267244338989  \n",
      "Steps : 2196 , Loss : 0.7838340997695923, Weights 1.038366436958313, bias : 0.07720665633678436  \n",
      "Steps : 2197 , Loss : 0.7838314771652222, Weights 1.0384033918380737, bias : 0.0771707072854042  \n",
      "Steps : 2198 , Loss : 0.7838289141654968, Weights 1.0384403467178345, bias : 0.07713481783866882  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2199 , Loss : 0.7838261127471924, Weights 1.0384771823883057, bias : 0.07709899544715881  \n",
      "Steps : 2200 , Loss : 0.783823549747467, Weights 1.0385140180587769, bias : 0.07706324011087418  \n",
      "Steps : 2201 , Loss : 0.7838208675384521, Weights 1.0385507345199585, bias : 0.07702755182981491  \n",
      "Steps : 2202 , Loss : 0.7838183045387268, Weights 1.0385874509811401, bias : 0.07699193060398102  \n",
      "Steps : 2203 , Loss : 0.7838157415390015, Weights 1.0386240482330322, bias : 0.0769563764333725  \n",
      "Steps : 2204 , Loss : 0.7838131189346313, Weights 1.0386606454849243, bias : 0.07692088186740875  \n",
      "Steps : 2205 , Loss : 0.783810555934906, Weights 1.0386971235275269, bias : 0.07688545435667038  \n",
      "Steps : 2206 , Loss : 0.7838079929351807, Weights 1.0387336015701294, bias : 0.07685009390115738  \n",
      "Steps : 2207 , Loss : 0.7838053703308105, Weights 1.0387699604034424, bias : 0.07681479305028915  \n",
      "Steps : 2208 , Loss : 0.7838029265403748, Weights 1.0388063192367554, bias : 0.0767795592546463  \n",
      "Steps : 2209 , Loss : 0.7838003635406494, Weights 1.0388425588607788, bias : 0.07674439251422882  \n",
      "Steps : 2210 , Loss : 0.7837977409362793, Weights 1.0388786792755127, bias : 0.07670929282903671  \n",
      "Steps : 2211 , Loss : 0.783795177936554, Weights 1.0389147996902466, bias : 0.07667425274848938  \n",
      "Steps : 2212 , Loss : 0.7837926745414734, Weights 1.038950800895691, bias : 0.07663927972316742  \n",
      "Steps : 2213 , Loss : 0.783790111541748, Weights 1.0389868021011353, bias : 0.07660436630249023  \n",
      "Steps : 2214 , Loss : 0.7837876677513123, Weights 1.03902268409729, bias : 0.07656951993703842  \n",
      "Steps : 2215 , Loss : 0.7837851643562317, Weights 1.0390585660934448, bias : 0.07653474062681198  \n",
      "Steps : 2216 , Loss : 0.7837827205657959, Weights 1.03909432888031, bias : 0.07650002092123032  \n",
      "Steps : 2217 , Loss : 0.7837802171707153, Weights 1.0391300916671753, bias : 0.07646536827087402  \n",
      "Steps : 2218 , Loss : 0.7837777733802795, Weights 1.039165735244751, bias : 0.0764307752251625  \n",
      "Steps : 2219 , Loss : 0.783775269985199, Weights 1.0392013788223267, bias : 0.07639624923467636  \n",
      "Steps : 2220 , Loss : 0.783772885799408, Weights 1.0392369031906128, bias : 0.07636178284883499  \n",
      "Steps : 2221 , Loss : 0.7837704420089722, Weights 1.039272427558899, bias : 0.076327383518219  \n",
      "Steps : 2222 , Loss : 0.7837679386138916, Weights 1.0393078327178955, bias : 0.07629304379224777  \n",
      "Steps : 2223 , Loss : 0.7837655544281006, Weights 1.039343237876892, bias : 0.07625877112150192  \n",
      "Steps : 2224 , Loss : 0.7837631106376648, Weights 1.0393785238265991, bias : 0.07622455805540085  \n",
      "Steps : 2225 , Loss : 0.783760666847229, Weights 1.0394138097763062, bias : 0.07619041204452515  \n",
      "Steps : 2226 , Loss : 0.7837584018707275, Weights 1.0394489765167236, bias : 0.07615632563829422  \n",
      "Steps : 2227 , Loss : 0.7837559580802917, Weights 1.0394841432571411, bias : 0.07612229883670807  \n",
      "Steps : 2228 , Loss : 0.783753514289856, Weights 1.039519190788269, bias : 0.07608833909034729  \n",
      "Steps : 2229 , Loss : 0.7837512493133545, Weights 1.039554238319397, bias : 0.07605443894863129  \n",
      "Steps : 2230 , Loss : 0.7837488055229187, Weights 1.0395891666412354, bias : 0.07602060586214066  \n",
      "Steps : 2231 , Loss : 0.7837464809417725, Weights 1.0396240949630737, bias : 0.0759868323802948  \n",
      "Steps : 2232 , Loss : 0.7837440371513367, Weights 1.0396589040756226, bias : 0.07595311850309372  \n",
      "Steps : 2233 , Loss : 0.7837417721748352, Weights 1.0396937131881714, bias : 0.07591947168111801  \n",
      "Steps : 2234 , Loss : 0.783739447593689, Weights 1.0397284030914307, bias : 0.07588588446378708  \n",
      "Steps : 2235 , Loss : 0.7837370038032532, Weights 1.03976309299469, bias : 0.07585235685110092  \n",
      "Steps : 2236 , Loss : 0.7837347388267517, Weights 1.0397976636886597, bias : 0.07581888884305954  \n",
      "Steps : 2237 , Loss : 0.783732533454895, Weights 1.0398322343826294, bias : 0.07578548789024353  \n",
      "Steps : 2238 , Loss : 0.7837302684783936, Weights 1.0398666858673096, bias : 0.0757521465420723  \n",
      "Steps : 2239 , Loss : 0.7837279438972473, Weights 1.0399011373519897, bias : 0.07571886479854584  \n",
      "Steps : 2240 , Loss : 0.7837255597114563, Weights 1.0399354696273804, bias : 0.07568564265966415  \n",
      "Steps : 2241 , Loss : 0.7837233543395996, Weights 1.039969801902771, bias : 0.07565248012542725  \n",
      "Steps : 2242 , Loss : 0.7837211489677429, Weights 1.040004014968872, bias : 0.07561938464641571  \n",
      "Steps : 2243 , Loss : 0.7837187051773071, Weights 1.0400382280349731, bias : 0.07558634877204895  \n",
      "Steps : 2244 , Loss : 0.7837165594100952, Weights 1.0400723218917847, bias : 0.07555337250232697  \n",
      "Steps : 2245 , Loss : 0.7837142944335938, Weights 1.0401064157485962, bias : 0.07552045583724976  \n",
      "Steps : 2246 , Loss : 0.7837121486663818, Weights 1.0401403903961182, bias : 0.07548759877681732  \n",
      "Steps : 2247 , Loss : 0.7837098836898804, Weights 1.0401743650436401, bias : 0.07545480132102966  \n",
      "Steps : 2248 , Loss : 0.7837076783180237, Weights 1.0402082204818726, bias : 0.07542206346988678  \n",
      "Steps : 2249 , Loss : 0.783705472946167, Weights 1.040242075920105, bias : 0.07538938522338867  \n",
      "Steps : 2250 , Loss : 0.7837033271789551, Weights 1.0402758121490479, bias : 0.07535676658153534  \n",
      "Steps : 2251 , Loss : 0.7837011218070984, Weights 1.0403095483779907, bias : 0.07532420754432678  \n",
      "Steps : 2252 , Loss : 0.7836988568305969, Weights 1.040343165397644, bias : 0.075291708111763  \n",
      "Steps : 2253 , Loss : 0.7836966514587402, Weights 1.0403767824172974, bias : 0.075259268283844  \n",
      "Steps : 2254 , Loss : 0.7836944460868835, Weights 1.0404102802276611, bias : 0.07522688806056976  \n",
      "Steps : 2255 , Loss : 0.7836923003196716, Weights 1.040443778038025, bias : 0.07519456744194031  \n",
      "Steps : 2256 , Loss : 0.7836902141571045, Weights 1.0404771566390991, bias : 0.07516230642795563  \n",
      "Steps : 2257 , Loss : 0.7836880683898926, Weights 1.0405105352401733, bias : 0.07513010501861572  \n",
      "Steps : 2258 , Loss : 0.7836859822273254, Weights 1.040543794631958, bias : 0.0750979632139206  \n",
      "Steps : 2259 , Loss : 0.7836837768554688, Weights 1.0405770540237427, bias : 0.07506588101387024  \n",
      "Steps : 2260 , Loss : 0.7836816310882568, Weights 1.0406101942062378, bias : 0.07503385841846466  \n",
      "Steps : 2261 , Loss : 0.7836796045303345, Weights 1.040643334388733, bias : 0.07500189542770386  \n",
      "Steps : 2262 , Loss : 0.7836774587631226, Weights 1.0406763553619385, bias : 0.07496999204158783  \n",
      "Steps : 2263 , Loss : 0.7836753726005554, Weights 1.040709376335144, bias : 0.07493814826011658  \n",
      "Steps : 2264 , Loss : 0.7836732268333435, Weights 1.04074227809906, bias : 0.0749063640832901  \n",
      "Steps : 2265 , Loss : 0.7836712598800659, Weights 1.040775179862976, bias : 0.0748746320605278  \n",
      "Steps : 2266 , Loss : 0.783669114112854, Weights 1.0408079624176025, bias : 0.07484295964241028  \n",
      "Steps : 2267 , Loss : 0.7836669683456421, Weights 1.040840744972229, bias : 0.07481134682893753  \n",
      "Steps : 2268 , Loss : 0.7836649417877197, Weights 1.0408735275268555, bias : 0.07477979362010956  \n",
      "Steps : 2269 , Loss : 0.7836628556251526, Weights 1.0409061908721924, bias : 0.07474830001592636  \n",
      "Steps : 2270 , Loss : 0.783660888671875, Weights 1.0409388542175293, bias : 0.07471685856580734  \n",
      "Steps : 2271 , Loss : 0.7836587429046631, Weights 1.0409713983535767, bias : 0.0746854767203331  \n",
      "Steps : 2272 , Loss : 0.7836567163467407, Weights 1.041003942489624, bias : 0.07465415447950363  \n",
      "Steps : 2273 , Loss : 0.7836546897888184, Weights 1.0410363674163818, bias : 0.07462289184331894  \n",
      "Steps : 2274 , Loss : 0.783652663230896, Weights 1.0410687923431396, bias : 0.07459168136119843  \n",
      "Steps : 2275 , Loss : 0.7836507558822632, Weights 1.041101098060608, bias : 0.07456053048372269  \n",
      "Steps : 2276 , Loss : 0.783648669719696, Weights 1.0411334037780762, bias : 0.07452943921089172  \n",
      "Steps : 2277 , Loss : 0.7836467027664185, Weights 1.0411655902862549, bias : 0.07449840009212494  \n",
      "Steps : 2278 , Loss : 0.7836446166038513, Weights 1.0411977767944336, bias : 0.07446742057800293  \n",
      "Steps : 2279 , Loss : 0.7836427092552185, Weights 1.0412298440933228, bias : 0.0744365006685257  \n",
      "Steps : 2280 , Loss : 0.7836407423019409, Weights 1.041261911392212, bias : 0.07440563291311264  \n",
      "Steps : 2281 , Loss : 0.7836387753486633, Weights 1.0412938594818115, bias : 0.07437482476234436  \n",
      "Steps : 2282 , Loss : 0.7836368083953857, Weights 1.0413258075714111, bias : 0.07434406876564026  \n",
      "Steps : 2283 , Loss : 0.7836347818374634, Weights 1.0413576364517212, bias : 0.07431337237358093  \n",
      "Steps : 2284 , Loss : 0.7836329936981201, Weights 1.0413894653320312, bias : 0.07428272813558578  \n",
      "Steps : 2285 , Loss : 0.7836309671401978, Weights 1.0414212942123413, bias : 0.07425214350223541  \n",
      "Steps : 2286 , Loss : 0.7836290001869202, Weights 1.0414530038833618, bias : 0.07422161847352982  \n",
      "Steps : 2287 , Loss : 0.7836270332336426, Weights 1.0414847135543823, bias : 0.0741911455988884  \n",
      "Steps : 2288 , Loss : 0.7836251854896545, Weights 1.0415163040161133, bias : 0.07416073232889175  \n",
      "Steps : 2289 , Loss : 0.7836232781410217, Weights 1.0415478944778442, bias : 0.07413037121295929  \n",
      "Steps : 2290 , Loss : 0.7836213707923889, Weights 1.0415793657302856, bias : 0.0741000697016716  \n",
      "Steps : 2291 , Loss : 0.7836194038391113, Weights 1.041610836982727, bias : 0.07406982034444809  \n",
      "Steps : 2292 , Loss : 0.7836175560951233, Weights 1.041642189025879, bias : 0.07403962314128876  \n",
      "Steps : 2293 , Loss : 0.7836157083511353, Weights 1.0416735410690308, bias : 0.0740094855427742  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2294 , Loss : 0.7836137413978577, Weights 1.041704773902893, bias : 0.07397940009832382  \n",
      "Steps : 2295 , Loss : 0.7836118936538696, Weights 1.0417360067367554, bias : 0.07394937425851822  \n",
      "Steps : 2296 , Loss : 0.7836101055145264, Weights 1.0417672395706177, bias : 0.0739194005727768  \n",
      "Steps : 2297 , Loss : 0.7836081981658936, Weights 1.0417983531951904, bias : 0.07388947904109955  \n",
      "Steps : 2298 , Loss : 0.7836062908172607, Weights 1.0418294668197632, bias : 0.07385961711406708  \n",
      "Steps : 2299 , Loss : 0.7836045026779175, Weights 1.0418604612350464, bias : 0.07382980734109879  \n",
      "Steps : 2300 , Loss : 0.7836025953292847, Weights 1.0418914556503296, bias : 0.07380005717277527  \n",
      "Steps : 2301 , Loss : 0.7836007475852966, Weights 1.0419223308563232, bias : 0.07377035915851593  \n",
      "Steps : 2302 , Loss : 0.7835990190505981, Weights 1.041953206062317, bias : 0.07374071329832077  \n",
      "Steps : 2303 , Loss : 0.7835971117019653, Weights 1.041983962059021, bias : 0.07371111959218979  \n",
      "Steps : 2304 , Loss : 0.7835952639579773, Weights 1.042014718055725, bias : 0.07368158549070358  \n",
      "Steps : 2305 , Loss : 0.7835934162139893, Weights 1.0420453548431396, bias : 0.07365210354328156  \n",
      "Steps : 2306 , Loss : 0.7835916876792908, Weights 1.0420759916305542, bias : 0.0736226737499237  \n",
      "Steps : 2307 , Loss : 0.7835899591445923, Weights 1.0421066284179688, bias : 0.07359330356121063  \n",
      "Steps : 2308 , Loss : 0.7835880517959595, Weights 1.0421371459960938, bias : 0.07356398552656174  \n",
      "Steps : 2309 , Loss : 0.7835862636566162, Weights 1.0421676635742188, bias : 0.07353471964597702  \n",
      "Steps : 2310 , Loss : 0.7835845351219177, Weights 1.0421980619430542, bias : 0.07350550591945648  \n",
      "Steps : 2311 , Loss : 0.7835827469825745, Weights 1.0422284603118896, bias : 0.07347634434700012  \n",
      "Steps : 2312 , Loss : 0.783581018447876, Weights 1.0422587394714355, bias : 0.07344724237918854  \n",
      "Steps : 2313 , Loss : 0.7835792303085327, Weights 1.0422890186309814, bias : 0.07341819256544113  \n",
      "Steps : 2314 , Loss : 0.783577561378479, Weights 1.0423191785812378, bias : 0.0733891949057579  \n",
      "Steps : 2315 , Loss : 0.7835757732391357, Weights 1.0423493385314941, bias : 0.07336024940013885  \n",
      "Steps : 2316 , Loss : 0.7835740447044373, Weights 1.0423794984817505, bias : 0.07333135604858398  \n",
      "Steps : 2317 , Loss : 0.783572256565094, Weights 1.0424095392227173, bias : 0.07330251485109329  \n",
      "Steps : 2318 , Loss : 0.7835705280303955, Weights 1.042439579963684, bias : 0.07327372580766678  \n",
      "Steps : 2319 , Loss : 0.7835688591003418, Weights 1.0424695014953613, bias : 0.07324498891830444  \n",
      "Steps : 2320 , Loss : 0.7835671305656433, Weights 1.0424994230270386, bias : 0.07321631163358688  \n",
      "Steps : 2321 , Loss : 0.7835654616355896, Weights 1.0425292253494263, bias : 0.0731876865029335  \n",
      "Steps : 2322 , Loss : 0.7835636734962463, Weights 1.042559027671814, bias : 0.0731591135263443  \n",
      "Steps : 2323 , Loss : 0.7835620045661926, Weights 1.0425888299942017, bias : 0.07313059270381927  \n",
      "Steps : 2324 , Loss : 0.7835603356361389, Weights 1.0426185131072998, bias : 0.07310212403535843  \n",
      "Steps : 2325 , Loss : 0.7835586667060852, Weights 1.042648196220398, bias : 0.07307370752096176  \n",
      "Steps : 2326 , Loss : 0.7835569977760315, Weights 1.0426777601242065, bias : 0.07304534316062927  \n",
      "Steps : 2327 , Loss : 0.783555269241333, Weights 1.0427073240280151, bias : 0.07301703095436096  \n",
      "Steps : 2328 , Loss : 0.7835536003112793, Weights 1.0427367687225342, bias : 0.07298877090215683  \n",
      "Steps : 2329 , Loss : 0.7835519313812256, Weights 1.0427662134170532, bias : 0.07296056300401688  \n",
      "Steps : 2330 , Loss : 0.7835504412651062, Weights 1.0427956581115723, bias : 0.0729324072599411  \n",
      "Steps : 2331 , Loss : 0.7835487127304077, Weights 1.0428249835968018, bias : 0.0729043036699295  \n",
      "Steps : 2332 , Loss : 0.7835469841957092, Weights 1.0428543090820312, bias : 0.07287624478340149  \n",
      "Steps : 2333 , Loss : 0.7835453748703003, Weights 1.0428835153579712, bias : 0.07284823805093765  \n",
      "Steps : 2334 , Loss : 0.7835437655448914, Weights 1.0429127216339111, bias : 0.072820283472538  \n",
      "Steps : 2335 , Loss : 0.7835421562194824, Weights 1.0429418087005615, bias : 0.07279238104820251  \n",
      "Steps : 2336 , Loss : 0.7835404872894287, Weights 1.042970895767212, bias : 0.07276453077793121  \n",
      "Steps : 2337 , Loss : 0.783538818359375, Weights 1.0429999828338623, bias : 0.07273673266172409  \n",
      "Steps : 2338 , Loss : 0.7835372686386108, Weights 1.0430289506912231, bias : 0.07270898669958115  \n",
      "Steps : 2339 , Loss : 0.7835357785224915, Weights 1.043057918548584, bias : 0.07268129289150238  \n",
      "Steps : 2340 , Loss : 0.7835339903831482, Weights 1.0430867671966553, bias : 0.0726536437869072  \n",
      "Steps : 2341 , Loss : 0.7835323810577393, Weights 1.0431156158447266, bias : 0.07262604683637619  \n",
      "Steps : 2342 , Loss : 0.7835308909416199, Weights 1.0431443452835083, bias : 0.07259850203990936  \n",
      "Steps : 2343 , Loss : 0.7835293412208557, Weights 1.04317307472229, bias : 0.07257100939750671  \n",
      "Steps : 2344 , Loss : 0.783527672290802, Weights 1.0432018041610718, bias : 0.07254356145858765  \n",
      "Steps : 2345 , Loss : 0.7835263013839722, Weights 1.043230414390564, bias : 0.07251616567373276  \n",
      "Steps : 2346 , Loss : 0.7835246324539185, Weights 1.0432590246200562, bias : 0.07248882204294205  \n",
      "Steps : 2347 , Loss : 0.7835230231285095, Weights 1.0432875156402588, bias : 0.07246153056621552  \n",
      "Steps : 2348 , Loss : 0.7835214138031006, Weights 1.0433160066604614, bias : 0.07243428379297256  \n",
      "Steps : 2349 , Loss : 0.7835200428962708, Weights 1.043344497680664, bias : 0.07240708917379379  \n",
      "Steps : 2350 , Loss : 0.7835184335708618, Weights 1.0433728694915771, bias : 0.0723799467086792  \n",
      "Steps : 2351 , Loss : 0.7835168242454529, Weights 1.0434012413024902, bias : 0.07235285639762878  \n",
      "Steps : 2352 , Loss : 0.7835153341293335, Weights 1.0434294939041138, bias : 0.07232581079006195  \n",
      "Steps : 2353 , Loss : 0.7835137844085693, Weights 1.0434577465057373, bias : 0.0722988173365593  \n",
      "Steps : 2354 , Loss : 0.78351229429245, Weights 1.0434859991073608, bias : 0.07227186858654022  \n",
      "Steps : 2355 , Loss : 0.7835107445716858, Weights 1.0435141324996948, bias : 0.07224497199058533  \n",
      "Steps : 2356 , Loss : 0.7835093140602112, Weights 1.0435422658920288, bias : 0.07221812754869461  \n",
      "Steps : 2357 , Loss : 0.783507764339447, Weights 1.0435702800750732, bias : 0.07219132781028748  \n",
      "Steps : 2358 , Loss : 0.7835063338279724, Weights 1.0435982942581177, bias : 0.07216458022594452  \n",
      "Steps : 2359 , Loss : 0.7835047841072083, Weights 1.0436261892318726, bias : 0.07213788479566574  \n",
      "Steps : 2360 , Loss : 0.7835032939910889, Weights 1.0436540842056274, bias : 0.07211123406887054  \n",
      "Steps : 2361 , Loss : 0.7835018634796143, Weights 1.0436819791793823, bias : 0.07208463549613953  \n",
      "Steps : 2362 , Loss : 0.7835003733634949, Weights 1.0437097549438477, bias : 0.07205808162689209  \n",
      "Steps : 2363 , Loss : 0.7834988236427307, Weights 1.043737530708313, bias : 0.07203157991170883  \n",
      "Steps : 2364 , Loss : 0.7834974527359009, Weights 1.0437651872634888, bias : 0.07200512290000916  \n",
      "Steps : 2365 , Loss : 0.7834959626197815, Weights 1.0437928438186646, bias : 0.07197871804237366  \n",
      "Steps : 2366 , Loss : 0.7834944725036621, Weights 1.0438205003738403, bias : 0.07195235788822174  \n",
      "Steps : 2367 , Loss : 0.7834930419921875, Weights 1.0438480377197266, bias : 0.071926049888134  \n",
      "Steps : 2368 , Loss : 0.7834916114807129, Weights 1.0438755750656128, bias : 0.07189978659152985  \n",
      "Steps : 2369 , Loss : 0.7834901213645935, Weights 1.0439029932022095, bias : 0.07187357544898987  \n",
      "Steps : 2370 , Loss : 0.7834886908531189, Weights 1.0439304113388062, bias : 0.07184740900993347  \n",
      "Steps : 2371 , Loss : 0.7834872603416443, Weights 1.0439578294754028, bias : 0.07182129472494125  \n",
      "Steps : 2372 , Loss : 0.7834857702255249, Weights 1.04398512840271, bias : 0.07179522514343262  \n",
      "Steps : 2373 , Loss : 0.7834843993186951, Weights 1.044012427330017, bias : 0.07176920026540756  \n",
      "Steps : 2374 , Loss : 0.7834829688072205, Weights 1.0440396070480347, bias : 0.07174322754144669  \n",
      "Steps : 2375 , Loss : 0.7834815979003906, Weights 1.0440667867660522, bias : 0.07171729952096939  \n",
      "Steps : 2376 , Loss : 0.7834802269935608, Weights 1.0440939664840698, bias : 0.07169142365455627  \n",
      "Steps : 2377 , Loss : 0.783478856086731, Weights 1.0441210269927979, bias : 0.07166559249162674  \n",
      "Steps : 2378 , Loss : 0.7834774255752563, Weights 1.0441480875015259, bias : 0.07163980603218079  \n",
      "Steps : 2379 , Loss : 0.783475935459137, Weights 1.0441750288009644, bias : 0.07161407172679901  \n",
      "Steps : 2380 , Loss : 0.7834746837615967, Weights 1.0442019701004028, bias : 0.07158838212490082  \n",
      "Steps : 2381 , Loss : 0.7834731936454773, Weights 1.0442289113998413, bias : 0.0715627372264862  \n",
      "Steps : 2382 , Loss : 0.7834718227386475, Weights 1.0442557334899902, bias : 0.07153714448213577  \n",
      "Steps : 2383 , Loss : 0.783470630645752, Weights 1.0442825555801392, bias : 0.07151159644126892  \n",
      "Steps : 2384 , Loss : 0.7834690809249878, Weights 1.044309377670288, bias : 0.07148609310388565  \n",
      "Steps : 2385 , Loss : 0.7834677696228027, Weights 1.0443360805511475, bias : 0.07146063446998596  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2386 , Loss : 0.7834665179252625, Weights 1.0443627834320068, bias : 0.07143522799015045  \n",
      "Steps : 2387 , Loss : 0.7834651470184326, Weights 1.0443893671035767, bias : 0.07140986621379852  \n",
      "Steps : 2388 , Loss : 0.783463716506958, Weights 1.0444159507751465, bias : 0.07138454914093018  \n",
      "Steps : 2389 , Loss : 0.7834623456001282, Weights 1.0444425344467163, bias : 0.07135927677154541  \n",
      "Steps : 2390 , Loss : 0.7834611535072327, Weights 1.0444689989089966, bias : 0.07133405655622482  \n",
      "Steps : 2391 , Loss : 0.7834597826004028, Weights 1.0444954633712769, bias : 0.07130888104438782  \n",
      "Steps : 2392 , Loss : 0.7834583520889282, Weights 1.0445218086242676, bias : 0.0712837502360344  \n",
      "Steps : 2393 , Loss : 0.7834570407867432, Weights 1.0445481538772583, bias : 0.07125866413116455  \n",
      "Steps : 2394 , Loss : 0.7834557890892029, Weights 1.044574499130249, bias : 0.07123362272977829  \n",
      "Steps : 2395 , Loss : 0.783454418182373, Weights 1.0446007251739502, bias : 0.07120863348245621  \n",
      "Steps : 2396 , Loss : 0.7834531664848328, Weights 1.0446269512176514, bias : 0.0711836889386177  \n",
      "Steps : 2397 , Loss : 0.7834518551826477, Weights 1.0446531772613525, bias : 0.07115878909826279  \n",
      "Steps : 2398 , Loss : 0.7834504842758179, Weights 1.0446792840957642, bias : 0.07113393396139145  \n",
      "Steps : 2399 , Loss : 0.7834492325782776, Weights 1.0447053909301758, bias : 0.07110912352800369  \n",
      "Steps : 2400 , Loss : 0.7834478616714478, Weights 1.0447313785552979, bias : 0.07108435779809952  \n",
      "Steps : 2401 , Loss : 0.7834465503692627, Weights 1.04475736618042, bias : 0.07105963677167892  \n",
      "Steps : 2402 , Loss : 0.783445417881012, Weights 1.044783353805542, bias : 0.07103496044874191  \n",
      "Steps : 2403 , Loss : 0.7834440469741821, Weights 1.0448092222213745, bias : 0.07101032882928848  \n",
      "Steps : 2404 , Loss : 0.7834428548812866, Weights 1.044835090637207, bias : 0.07098574936389923  \n",
      "Steps : 2405 , Loss : 0.7834414839744568, Weights 1.0448609590530396, bias : 0.07096121460199356  \n",
      "Steps : 2406 , Loss : 0.783440351486206, Weights 1.0448867082595825, bias : 0.07093672454357147  \n",
      "Steps : 2407 , Loss : 0.783439040184021, Weights 1.0449124574661255, bias : 0.07091227918863297  \n",
      "Steps : 2408 , Loss : 0.7834376692771912, Weights 1.044938087463379, bias : 0.07088787853717804  \n",
      "Steps : 2409 , Loss : 0.7834365963935852, Weights 1.0449637174606323, bias : 0.0708635225892067  \n",
      "Steps : 2410 , Loss : 0.7834352850914001, Weights 1.0449893474578857, bias : 0.07083921134471893  \n",
      "Steps : 2411 , Loss : 0.7834340929985046, Weights 1.0450148582458496, bias : 0.07081494480371475  \n",
      "Steps : 2412 , Loss : 0.7834328413009644, Weights 1.0450403690338135, bias : 0.07079072296619415  \n",
      "Steps : 2413 , Loss : 0.7834316492080688, Weights 1.0450658798217773, bias : 0.07076654583215714  \n",
      "Steps : 2414 , Loss : 0.7834303379058838, Weights 1.0450912714004517, bias : 0.0707424134016037  \n",
      "Steps : 2415 , Loss : 0.7834290862083435, Weights 1.045116662979126, bias : 0.07071831822395325  \n",
      "Steps : 2416 , Loss : 0.7834279537200928, Weights 1.0451419353485107, bias : 0.07069426774978638  \n",
      "Steps : 2417 , Loss : 0.7834267616271973, Weights 1.0451672077178955, bias : 0.07067026197910309  \n",
      "Steps : 2418 , Loss : 0.7834254503250122, Weights 1.0451924800872803, bias : 0.07064630091190338  \n",
      "Steps : 2419 , Loss : 0.7834241986274719, Weights 1.0452176332473755, bias : 0.07062238454818726  \n",
      "Steps : 2420 , Loss : 0.7834230661392212, Weights 1.0452427864074707, bias : 0.07059851288795471  \n",
      "Steps : 2421 , Loss : 0.7834217548370361, Weights 1.045267939567566, bias : 0.07057468593120575  \n",
      "Steps : 2422 , Loss : 0.7834208011627197, Weights 1.0452929735183716, bias : 0.07055090367794037  \n",
      "Steps : 2423 , Loss : 0.7834194898605347, Weights 1.0453180074691772, bias : 0.07052716612815857  \n",
      "Steps : 2424 , Loss : 0.7834182977676392, Weights 1.0453429222106934, bias : 0.07050346583127975  \n",
      "Steps : 2425 , Loss : 0.7834171056747437, Weights 1.0453678369522095, bias : 0.07047981023788452  \n",
      "Steps : 2426 , Loss : 0.7834159731864929, Weights 1.0453927516937256, bias : 0.07045619934797287  \n",
      "Steps : 2427 , Loss : 0.7834148406982422, Weights 1.0454175472259521, bias : 0.0704326331615448  \n",
      "Steps : 2428 , Loss : 0.7834135293960571, Weights 1.0454423427581787, bias : 0.07040911167860031  \n",
      "Steps : 2429 , Loss : 0.7834124565124512, Weights 1.0454671382904053, bias : 0.07038562744855881  \n",
      "Steps : 2430 , Loss : 0.7834112644195557, Weights 1.0454918146133423, bias : 0.07036218792200089  \n",
      "Steps : 2431 , Loss : 0.7834101319313049, Weights 1.0455164909362793, bias : 0.07033879309892654  \n",
      "Steps : 2432 , Loss : 0.7834089398384094, Weights 1.0455411672592163, bias : 0.07031544297933578  \n",
      "Steps : 2433 , Loss : 0.7834079265594482, Weights 1.0455657243728638, bias : 0.07029213011264801  \n",
      "Steps : 2434 , Loss : 0.783406674861908, Weights 1.0455902814865112, bias : 0.07026886194944382  \n",
      "Steps : 2435 , Loss : 0.7834054827690125, Weights 1.0456148386001587, bias : 0.0702456384897232  \n",
      "Steps : 2436 , Loss : 0.783404529094696, Weights 1.0456392765045166, bias : 0.07022245973348618  \n",
      "Steps : 2437 , Loss : 0.7834033370018005, Weights 1.0456637144088745, bias : 0.07019931823015213  \n",
      "Steps : 2438 , Loss : 0.7834022641181946, Weights 1.0456880331039429, bias : 0.07017622143030167  \n",
      "Steps : 2439 , Loss : 0.7834010124206543, Weights 1.0457123517990112, bias : 0.07015316933393478  \n",
      "Steps : 2440 , Loss : 0.7833999395370483, Weights 1.0457366704940796, bias : 0.07013015449047089  \n",
      "Steps : 2441 , Loss : 0.7833987474441528, Weights 1.0457608699798584, bias : 0.07010718435049057  \n",
      "Steps : 2442 , Loss : 0.7833976745605469, Weights 1.0457850694656372, bias : 0.07008425891399384  \n",
      "Steps : 2443 , Loss : 0.7833966016769409, Weights 1.045809268951416, bias : 0.07006137073040009  \n",
      "Steps : 2444 , Loss : 0.783395528793335, Weights 1.0458333492279053, bias : 0.07003852725028992  \n",
      "Steps : 2445 , Loss : 0.7833943367004395, Weights 1.0458574295043945, bias : 0.07001572847366333  \n",
      "Steps : 2446 , Loss : 0.7833932638168335, Weights 1.0458815097808838, bias : 0.06999296694993973  \n",
      "Steps : 2447 , Loss : 0.7833921909332275, Weights 1.0459054708480835, bias : 0.06997025012969971  \n",
      "Steps : 2448 , Loss : 0.7833911180496216, Weights 1.0459294319152832, bias : 0.06994757056236267  \n",
      "Steps : 2449 , Loss : 0.7833900451660156, Weights 1.045953392982483, bias : 0.06992493569850922  \n",
      "Steps : 2450 , Loss : 0.7833889722824097, Weights 1.045977234840393, bias : 0.06990234553813934  \n",
      "Steps : 2451 , Loss : 0.7833878993988037, Weights 1.0460010766983032, bias : 0.06987979263067245  \n",
      "Steps : 2452 , Loss : 0.783386766910553, Weights 1.0460249185562134, bias : 0.06985728442668915  \n",
      "Steps : 2453 , Loss : 0.7833857536315918, Weights 1.046048641204834, bias : 0.06983481347560883  \n",
      "Steps : 2454 , Loss : 0.7833846807479858, Weights 1.0460723638534546, bias : 0.06981238722801208  \n",
      "Steps : 2455 , Loss : 0.7833836078643799, Weights 1.0460960865020752, bias : 0.06978999823331833  \n",
      "Steps : 2456 , Loss : 0.7833825945854187, Weights 1.0461196899414062, bias : 0.06976765394210815  \n",
      "Steps : 2457 , Loss : 0.7833815217018127, Weights 1.0461432933807373, bias : 0.06974534690380096  \n",
      "Steps : 2458 , Loss : 0.7833804488182068, Weights 1.0461667776107788, bias : 0.06972308456897736  \n",
      "Steps : 2459 , Loss : 0.7833793759346008, Weights 1.0461902618408203, bias : 0.06970085948705673  \n",
      "Steps : 2460 , Loss : 0.7833783030509949, Weights 1.0462137460708618, bias : 0.06967867910861969  \n",
      "Steps : 2461 , Loss : 0.7833772897720337, Weights 1.0462371110916138, bias : 0.06965653598308563  \n",
      "Steps : 2462 , Loss : 0.7833763957023621, Weights 1.0462604761123657, bias : 0.06963443756103516  \n",
      "Steps : 2463 , Loss : 0.7833752632141113, Weights 1.0462838411331177, bias : 0.06961237639188766  \n",
      "Steps : 2464 , Loss : 0.7833742499351501, Weights 1.04630708694458, bias : 0.06959035247564316  \n",
      "Steps : 2465 , Loss : 0.783373236656189, Weights 1.0463303327560425, bias : 0.06956837326288223  \n",
      "Steps : 2466 , Loss : 0.783372163772583, Weights 1.0463535785675049, bias : 0.06954643130302429  \n",
      "Steps : 2467 , Loss : 0.7833712697029114, Weights 1.0463767051696777, bias : 0.06952453404664993  \n",
      "Steps : 2468 , Loss : 0.7833701968193054, Weights 1.0463998317718506, bias : 0.06950267404317856  \n",
      "Steps : 2469 , Loss : 0.783369243144989, Weights 1.0464229583740234, bias : 0.06948085129261017  \n",
      "Steps : 2470 , Loss : 0.7833681702613831, Weights 1.0464459657669067, bias : 0.06945907324552536  \n",
      "Steps : 2471 , Loss : 0.7833671569824219, Weights 1.04646897315979, bias : 0.06943733245134354  \n",
      "Steps : 2472 , Loss : 0.7833660840988159, Weights 1.0464919805526733, bias : 0.0694156289100647  \n",
      "Steps : 2473 , Loss : 0.7833651900291443, Weights 1.046514868736267, bias : 0.06939397007226944  \n",
      "Steps : 2474 , Loss : 0.7833641767501831, Weights 1.0465377569198608, bias : 0.06937234848737717  \n",
      "Steps : 2475 , Loss : 0.7833631038665771, Weights 1.0465606451034546, bias : 0.06935076415538788  \n",
      "Steps : 2476 , Loss : 0.7833622694015503, Weights 1.0465834140777588, bias : 0.06932922452688217  \n",
      "Steps : 2477 , Loss : 0.7833612561225891, Weights 1.046606183052063, bias : 0.06930772215127945  \n",
      "Steps : 2478 , Loss : 0.7833602428436279, Weights 1.0466289520263672, bias : 0.06928625702857971  \n",
      "Steps : 2479 , Loss : 0.7833592295646667, Weights 1.0466516017913818, bias : 0.06926483660936356  \n",
      "Steps : 2480 , Loss : 0.7833583950996399, Weights 1.0466742515563965, bias : 0.06924345344305038  \n",
      "Steps : 2481 , Loss : 0.7833573818206787, Weights 1.0466969013214111, bias : 0.0692221075296402  \n",
      "Steps : 2482 , Loss : 0.7833564281463623, Weights 1.0467194318771362, bias : 0.069200798869133  \n",
      "Steps : 2483 , Loss : 0.7833554148674011, Weights 1.0467419624328613, bias : 0.06917953491210938  \n",
      "Steps : 2484 , Loss : 0.7833544015884399, Weights 1.0467644929885864, bias : 0.06915830820798874  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2485 , Loss : 0.7833535671234131, Weights 1.046786904335022, bias : 0.06913711875677109  \n",
      "Steps : 2486 , Loss : 0.7833526730537415, Weights 1.0468093156814575, bias : 0.06911596655845642  \n",
      "Steps : 2487 , Loss : 0.7833516001701355, Weights 1.046831727027893, bias : 0.06909485161304474  \n",
      "Steps : 2488 , Loss : 0.7833507061004639, Weights 1.046854019165039, bias : 0.06907378137111664  \n",
      "Steps : 2489 , Loss : 0.7833497524261475, Weights 1.046876311302185, bias : 0.06905274838209152  \n",
      "Steps : 2490 , Loss : 0.7833488583564758, Weights 1.046898603439331, bias : 0.06903175264596939  \n",
      "Steps : 2491 , Loss : 0.7833479046821594, Weights 1.046920895576477, bias : 0.06901079416275024  \n",
      "Steps : 2492 , Loss : 0.7833468914031982, Weights 1.0469430685043335, bias : 0.06898987293243408  \n",
      "Steps : 2493 , Loss : 0.7833459973335266, Weights 1.04696524143219, bias : 0.0689689889550209  \n",
      "Steps : 2494 , Loss : 0.7833450436592102, Weights 1.0469874143600464, bias : 0.06894814968109131  \n",
      "Steps : 2495 , Loss : 0.7833442687988281, Weights 1.0470094680786133, bias : 0.0689273476600647  \n",
      "Steps : 2496 , Loss : 0.7833432555198669, Weights 1.0470315217971802, bias : 0.06890658289194107  \n",
      "Steps : 2497 , Loss : 0.7833423018455505, Weights 1.047053575515747, bias : 0.06888585537672043  \n",
      "Steps : 2498 , Loss : 0.7833413481712341, Weights 1.0470755100250244, bias : 0.06886516511440277  \n",
      "Steps : 2499 , Loss : 0.7833406329154968, Weights 1.0470974445343018, bias : 0.0688445121049881  \n",
      "Steps : 2500 , Loss : 0.7833396196365356, Weights 1.047119379043579, bias : 0.06882389634847641  \n",
      "Steps : 2501 , Loss : 0.7833387851715088, Weights 1.047141194343567, bias : 0.0688033178448677  \n",
      "Steps : 2502 , Loss : 0.7833378314971924, Weights 1.0471630096435547, bias : 0.06878277659416199  \n",
      "Steps : 2503 , Loss : 0.7833369374275208, Weights 1.0471848249435425, bias : 0.06876227259635925  \n",
      "Steps : 2504 , Loss : 0.7833361029624939, Weights 1.0472065210342407, bias : 0.0687418058514595  \n",
      "Steps : 2505 , Loss : 0.7833352088928223, Weights 1.047228217124939, bias : 0.06872137635946274  \n",
      "Steps : 2506 , Loss : 0.7833342552185059, Weights 1.0472499132156372, bias : 0.06870098412036896  \n",
      "Steps : 2507 , Loss : 0.783333420753479, Weights 1.047271490097046, bias : 0.06868062913417816  \n",
      "Steps : 2508 , Loss : 0.7833325266838074, Weights 1.0472930669784546, bias : 0.06866031140089035  \n",
      "Steps : 2509 , Loss : 0.7833316326141357, Weights 1.0473146438598633, bias : 0.06864003092050552  \n",
      "Steps : 2510 , Loss : 0.7833307385444641, Weights 1.0473361015319824, bias : 0.06861978769302368  \n",
      "Steps : 2511 , Loss : 0.7833299040794373, Weights 1.0473575592041016, bias : 0.06859958171844482  \n",
      "Steps : 2512 , Loss : 0.7833290100097656, Weights 1.0473790168762207, bias : 0.06857941299676895  \n",
      "Steps : 2513 , Loss : 0.7833282351493835, Weights 1.0474004745483398, bias : 0.06855928152799606  \n",
      "Steps : 2514 , Loss : 0.7833273410797119, Weights 1.0474218130111694, bias : 0.06853918731212616  \n",
      "Steps : 2515 , Loss : 0.7833263874053955, Weights 1.047443151473999, bias : 0.06851913034915924  \n",
      "Steps : 2516 , Loss : 0.7833256721496582, Weights 1.0474644899368286, bias : 0.0684991106390953  \n",
      "Steps : 2517 , Loss : 0.7833247184753418, Weights 1.0474857091903687, bias : 0.06847912818193436  \n",
      "Steps : 2518 , Loss : 0.7833238244056702, Weights 1.0475069284439087, bias : 0.06845918297767639  \n",
      "Steps : 2519 , Loss : 0.7833230495452881, Weights 1.0475281476974487, bias : 0.06843927502632141  \n",
      "Steps : 2520 , Loss : 0.783322274684906, Weights 1.0475492477416992, bias : 0.06841940432786942  \n",
      "Steps : 2521 , Loss : 0.7833213806152344, Weights 1.0475703477859497, bias : 0.0683995708823204  \n",
      "Steps : 2522 , Loss : 0.7833205461502075, Weights 1.0475914478302002, bias : 0.06837976723909378  \n",
      "Steps : 2523 , Loss : 0.7833197116851807, Weights 1.0476124286651611, bias : 0.06836000084877014  \n",
      "Steps : 2524 , Loss : 0.7833188772201538, Weights 1.047633409500122, bias : 0.06834027171134949  \n",
      "Steps : 2525 , Loss : 0.7833179235458374, Weights 1.047654390335083, bias : 0.06832057982683182  \n",
      "Steps : 2526 , Loss : 0.7833172678947449, Weights 1.047675371170044, bias : 0.06830092519521713  \n",
      "Steps : 2527 , Loss : 0.7833163738250732, Weights 1.0476962327957153, bias : 0.06828130781650543  \n",
      "Steps : 2528 , Loss : 0.7833155989646912, Weights 1.0477170944213867, bias : 0.06826172769069672  \n",
      "Steps : 2529 , Loss : 0.7833147644996643, Weights 1.047737956047058, bias : 0.06824217736721039  \n",
      "Steps : 2530 , Loss : 0.7833139300346375, Weights 1.04775869846344, bias : 0.06822266429662704  \n",
      "Steps : 2531 , Loss : 0.7833131551742554, Weights 1.0477794408798218, bias : 0.06820318847894669  \n",
      "Steps : 2532 , Loss : 0.7833123207092285, Weights 1.0478001832962036, bias : 0.06818374991416931  \n",
      "Steps : 2533 , Loss : 0.7833116054534912, Weights 1.047820806503296, bias : 0.06816434860229492  \n",
      "Steps : 2534 , Loss : 0.7833107113838196, Weights 1.0478414297103882, bias : 0.06814497709274292  \n",
      "Steps : 2535 , Loss : 0.7833099365234375, Weights 1.0478620529174805, bias : 0.0681256428360939  \n",
      "Steps : 2536 , Loss : 0.7833091616630554, Weights 1.0478826761245728, bias : 0.06810634583234787  \n",
      "Steps : 2537 , Loss : 0.7833083868026733, Weights 1.0479031801223755, bias : 0.06808708608150482  \n",
      "Steps : 2538 , Loss : 0.7833075523376465, Weights 1.0479236841201782, bias : 0.06806785613298416  \n",
      "Steps : 2539 , Loss : 0.7833068370819092, Weights 1.047944188117981, bias : 0.06804866343736649  \n",
      "Steps : 2540 , Loss : 0.7833059430122375, Weights 1.0479645729064941, bias : 0.0680295079946518  \n",
      "Steps : 2541 , Loss : 0.7833052277565002, Weights 1.0479849576950073, bias : 0.06801038235425949  \n",
      "Steps : 2542 , Loss : 0.7833044528961182, Weights 1.0480053424835205, bias : 0.06799129396677017  \n",
      "Steps : 2543 , Loss : 0.7833036780357361, Weights 1.0480256080627441, bias : 0.06797224283218384  \n",
      "Steps : 2544 , Loss : 0.7833028435707092, Weights 1.0480458736419678, bias : 0.06795322149991989  \n",
      "Steps : 2545 , Loss : 0.7833021283149719, Weights 1.0480661392211914, bias : 0.06793423742055893  \n",
      "Steps : 2546 , Loss : 0.7833014130592346, Weights 1.048086404800415, bias : 0.06791529059410095  \n",
      "Steps : 2547 , Loss : 0.7833006381988525, Weights 1.0481065511703491, bias : 0.06789637356996536  \n",
      "Steps : 2548 , Loss : 0.7832996845245361, Weights 1.0481266975402832, bias : 0.06787749379873276  \n",
      "Steps : 2549 , Loss : 0.7832990884780884, Weights 1.0481468439102173, bias : 0.06785865128040314  \n",
      "Steps : 2550 , Loss : 0.7832983136177063, Weights 1.0481668710708618, bias : 0.0678398385643959  \n",
      "Steps : 2551 , Loss : 0.783297598361969, Weights 1.0481868982315063, bias : 0.06782106310129166  \n",
      "Steps : 2552 , Loss : 0.7832968235015869, Weights 1.0482069253921509, bias : 0.0678023248910904  \n",
      "Steps : 2553 , Loss : 0.7832959890365601, Weights 1.0482268333435059, bias : 0.06778361648321152  \n",
      "Steps : 2554 , Loss : 0.7832954525947571, Weights 1.0482467412948608, bias : 0.06776494532823563  \n",
      "Steps : 2555 , Loss : 0.7832944989204407, Weights 1.0482666492462158, bias : 0.06774630397558212  \n",
      "Steps : 2556 , Loss : 0.7832939028739929, Weights 1.0482865571975708, bias : 0.0677276998758316  \n",
      "Steps : 2557 , Loss : 0.7832930088043213, Weights 1.0483063459396362, bias : 0.06770913302898407  \n",
      "Steps : 2558 , Loss : 0.7832924127578735, Weights 1.0483261346817017, bias : 0.06769059598445892  \n",
      "Steps : 2559 , Loss : 0.7832916378974915, Weights 1.048345923423767, bias : 0.06767209619283676  \n",
      "Steps : 2560 , Loss : 0.7832910418510437, Weights 1.048365592956543, bias : 0.06765362620353699  \n",
      "Steps : 2561 , Loss : 0.7832902669906616, Weights 1.0483852624893188, bias : 0.0676351934671402  \n",
      "Steps : 2562 , Loss : 0.7832894325256348, Weights 1.0484049320220947, bias : 0.0676167905330658  \n",
      "Steps : 2563 , Loss : 0.7832886576652527, Weights 1.0484246015548706, bias : 0.06759842485189438  \n",
      "Steps : 2564 , Loss : 0.7832879424095154, Weights 1.048444151878357, bias : 0.06758008897304535  \n",
      "Steps : 2565 , Loss : 0.7832873463630676, Weights 1.0484637022018433, bias : 0.0675617903470993  \n",
      "Steps : 2566 , Loss : 0.7832865715026855, Weights 1.0484832525253296, bias : 0.06754352152347565  \n",
      "Steps : 2567 , Loss : 0.7832858562469482, Weights 1.0485026836395264, bias : 0.06752528995275497  \n",
      "Steps : 2568 , Loss : 0.7832852005958557, Weights 1.0485221147537231, bias : 0.06750708818435669  \n",
      "Steps : 2569 , Loss : 0.7832844257354736, Weights 1.04854154586792, bias : 0.06748892366886139  \n",
      "Steps : 2570 , Loss : 0.7832837700843811, Weights 1.0485609769821167, bias : 0.06747078895568848  \n",
      "Steps : 2571 , Loss : 0.7832830548286438, Weights 1.048580288887024, bias : 0.06745269149541855  \n",
      "Steps : 2572 , Loss : 0.7832823395729065, Weights 1.0485996007919312, bias : 0.06743462383747101  \n",
      "Steps : 2573 , Loss : 0.783281683921814, Weights 1.0486189126968384, bias : 0.06741659343242645  \n",
      "Steps : 2574 , Loss : 0.7832809686660767, Weights 1.048638105392456, bias : 0.06739859282970428  \n",
      "Steps : 2575 , Loss : 0.7832802534103394, Weights 1.0486572980880737, bias : 0.0673806220293045  \n",
      "Steps : 2576 , Loss : 0.7832796573638916, Weights 1.0486764907836914, bias : 0.06736268848180771  \n",
      "Steps : 2577 , Loss : 0.7832789421081543, Weights 1.048695683479309, bias : 0.0673447847366333  \n",
      "Steps : 2578 , Loss : 0.783278226852417, Weights 1.0487147569656372, bias : 0.06732691824436188  \n",
      "Steps : 2579 , Loss : 0.7832775712013245, Weights 1.0487338304519653, bias : 0.06730908155441284  \n",
      "Steps : 2580 , Loss : 0.7832767963409424, Weights 1.0487529039382935, bias : 0.0672912746667862  \n",
      "Steps : 2581 , Loss : 0.7832762002944946, Weights 1.0487719774246216, bias : 0.06727350503206253  \n",
      "Steps : 2582 , Loss : 0.7832755446434021, Weights 1.0487909317016602, bias : 0.06725576519966125  \n",
      "Steps : 2583 , Loss : 0.7832748293876648, Weights 1.0488098859786987, bias : 0.06723806262016296  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2584 , Loss : 0.783274233341217, Weights 1.0488288402557373, bias : 0.06722038984298706  \n",
      "Steps : 2585 , Loss : 0.7832735180854797, Weights 1.0488476753234863, bias : 0.06720274686813354  \n",
      "Steps : 2586 , Loss : 0.7832728624343872, Weights 1.0488665103912354, bias : 0.06718514114618301  \n",
      "Steps : 2587 , Loss : 0.7832721471786499, Weights 1.0488853454589844, bias : 0.06716756522655487  \n",
      "Steps : 2588 , Loss : 0.7832714915275574, Weights 1.0489041805267334, bias : 0.06715001910924911  \n",
      "Steps : 2589 , Loss : 0.7832708954811096, Weights 1.0489228963851929, bias : 0.06713250279426575  \n",
      "Steps : 2590 , Loss : 0.7832702398300171, Weights 1.0489416122436523, bias : 0.06711502373218536  \n",
      "Steps : 2591 , Loss : 0.7832695841789246, Weights 1.0489603281021118, bias : 0.06709757447242737  \n",
      "Steps : 2592 , Loss : 0.7832688689231873, Weights 1.0489790439605713, bias : 0.06708015501499176  \n",
      "Steps : 2593 , Loss : 0.78326815366745, Weights 1.0489976406097412, bias : 0.06706277281045914  \n",
      "Steps : 2594 , Loss : 0.7832674980163574, Weights 1.0490162372589111, bias : 0.0670454204082489  \n",
      "Steps : 2595 , Loss : 0.7832669019699097, Weights 1.049034833908081, bias : 0.06702809780836105  \n",
      "Steps : 2596 , Loss : 0.7832663059234619, Weights 1.0490533113479614, bias : 0.0670108050107956  \n",
      "Steps : 2597 , Loss : 0.7832657098770142, Weights 1.0490717887878418, bias : 0.06699354946613312  \n",
      "Steps : 2598 , Loss : 0.7832650542259216, Weights 1.0490902662277222, bias : 0.06697632372379303  \n",
      "Steps : 2599 , Loss : 0.7832644581794739, Weights 1.0491087436676025, bias : 0.06695912778377533  \n",
      "Steps : 2600 , Loss : 0.7832638025283813, Weights 1.0491271018981934, bias : 0.06694196164608002  \n",
      "Steps : 2601 , Loss : 0.7832631468772888, Weights 1.0491454601287842, bias : 0.06692483276128769  \n",
      "Steps : 2602 , Loss : 0.7832624912261963, Weights 1.049163818359375, bias : 0.06690773367881775  \n",
      "Steps : 2603 , Loss : 0.7832619547843933, Weights 1.0491821765899658, bias : 0.0668906643986702  \n",
      "Steps : 2604 , Loss : 0.783261239528656, Weights 1.049200415611267, bias : 0.06687362492084503  \n",
      "Steps : 2605 , Loss : 0.7832606434822083, Weights 1.0492186546325684, bias : 0.06685661524534225  \n",
      "Steps : 2606 , Loss : 0.7832600474357605, Weights 1.0492368936538696, bias : 0.06683964282274246  \n",
      "Steps : 2607 , Loss : 0.7832594513893127, Weights 1.049255132675171, bias : 0.06682270020246506  \n",
      "Steps : 2608 , Loss : 0.7832587957382202, Weights 1.0492732524871826, bias : 0.06680578738451004  \n",
      "Steps : 2609 , Loss : 0.7832581996917725, Weights 1.0492913722991943, bias : 0.06678890436887741  \n",
      "Steps : 2610 , Loss : 0.7832576036453247, Weights 1.049309492111206, bias : 0.06677205115556717  \n",
      "Steps : 2611 , Loss : 0.783257007598877, Weights 1.0493274927139282, bias : 0.06675522774457932  \n",
      "Steps : 2612 , Loss : 0.7832563519477844, Weights 1.0493454933166504, bias : 0.06673843413591385  \n",
      "Steps : 2613 , Loss : 0.7832557559013367, Weights 1.0493634939193726, bias : 0.06672167778015137  \n",
      "Steps : 2614 , Loss : 0.7832551598548889, Weights 1.0493814945220947, bias : 0.06670495122671127  \n",
      "Steps : 2615 , Loss : 0.7832545638084412, Weights 1.0493993759155273, bias : 0.06668825447559357  \n",
      "Steps : 2616 , Loss : 0.7832539081573486, Weights 1.04941725730896, bias : 0.06667158752679825  \n",
      "Steps : 2617 , Loss : 0.7832534313201904, Weights 1.0494351387023926, bias : 0.06665495038032532  \n",
      "Steps : 2618 , Loss : 0.7832527160644531, Weights 1.0494530200958252, bias : 0.06663834303617477  \n",
      "Steps : 2619 , Loss : 0.7832521200180054, Weights 1.0494707822799683, bias : 0.06662176549434662  \n",
      "Steps : 2620 , Loss : 0.7832515835762024, Weights 1.0494885444641113, bias : 0.06660521775484085  \n",
      "Steps : 2621 , Loss : 0.7832509875297546, Weights 1.0495063066482544, bias : 0.06658869981765747  \n",
      "Steps : 2622 , Loss : 0.7832504510879517, Weights 1.0495240688323975, bias : 0.06657221168279648  \n",
      "Steps : 2623 , Loss : 0.7832497954368591, Weights 1.049541711807251, bias : 0.06655575335025787  \n",
      "Steps : 2624 , Loss : 0.7832491993904114, Weights 1.0495593547821045, bias : 0.06653932482004166  \n",
      "Steps : 2625 , Loss : 0.7832486629486084, Weights 1.049576997756958, bias : 0.06652292609214783  \n",
      "Steps : 2626 , Loss : 0.7832480072975159, Weights 1.0495946407318115, bias : 0.06650655716657639  \n",
      "Steps : 2627 , Loss : 0.7832475304603577, Weights 1.0496121644973755, bias : 0.06649021804332733  \n",
      "Steps : 2628 , Loss : 0.7832469344139099, Weights 1.0496296882629395, bias : 0.06647391617298126  \n",
      "Steps : 2629 , Loss : 0.7832462787628174, Weights 1.0496472120285034, bias : 0.06645764410495758  \n",
      "Steps : 2630 , Loss : 0.7832458019256592, Weights 1.0496647357940674, bias : 0.06644139438867569  \n",
      "Steps : 2631 , Loss : 0.7832452654838562, Weights 1.0496821403503418, bias : 0.06642517447471619  \n",
      "Steps : 2632 , Loss : 0.7832447290420532, Weights 1.0496995449066162, bias : 0.06640898436307907  \n",
      "Steps : 2633 , Loss : 0.7832440733909607, Weights 1.0497169494628906, bias : 0.06639282405376434  \n",
      "Steps : 2634 , Loss : 0.7832435369491577, Weights 1.049734354019165, bias : 0.066376693546772  \n",
      "Steps : 2635 , Loss : 0.7832430005073547, Weights 1.04975163936615, bias : 0.06636059284210205  \n",
      "Steps : 2636 , Loss : 0.7832423448562622, Weights 1.0497689247131348, bias : 0.06634452193975449  \n",
      "Steps : 2637 , Loss : 0.783241868019104, Weights 1.0497862100601196, bias : 0.06632848083972931  \n",
      "Steps : 2638 , Loss : 0.7832412719726562, Weights 1.0498034954071045, bias : 0.06631246954202652  \n",
      "Steps : 2639 , Loss : 0.783240795135498, Weights 1.0498206615447998, bias : 0.06629648804664612  \n",
      "Steps : 2640 , Loss : 0.7832401990890503, Weights 1.0498378276824951, bias : 0.0662805363535881  \n",
      "Steps : 2641 , Loss : 0.7832396030426025, Weights 1.0498549938201904, bias : 0.06626461446285248  \n",
      "Steps : 2642 , Loss : 0.7832390666007996, Weights 1.0498721599578857, bias : 0.06624872237443924  \n",
      "Steps : 2643 , Loss : 0.7832385897636414, Weights 1.0498892068862915, bias : 0.06623286008834839  \n",
      "Steps : 2644 , Loss : 0.7832380533218384, Weights 1.0499062538146973, bias : 0.06621702760457993  \n",
      "Steps : 2645 , Loss : 0.7832374572753906, Weights 1.049923300743103, bias : 0.06620122492313385  \n",
      "Steps : 2646 , Loss : 0.7832369804382324, Weights 1.0499403476715088, bias : 0.06618544459342957  \n",
      "Steps : 2647 , Loss : 0.7832363843917847, Weights 1.049957275390625, bias : 0.06616969406604767  \n",
      "Steps : 2648 , Loss : 0.7832358479499817, Weights 1.0499742031097412, bias : 0.06615397334098816  \n",
      "Steps : 2649 , Loss : 0.7832353115081787, Weights 1.0499911308288574, bias : 0.06613828241825104  \n",
      "Steps : 2650 , Loss : 0.7832348346710205, Weights 1.0500080585479736, bias : 0.0661226212978363  \n",
      "Steps : 2651 , Loss : 0.7832343578338623, Weights 1.0500248670578003, bias : 0.06610698997974396  \n",
      "Steps : 2652 , Loss : 0.7832338213920593, Weights 1.050041675567627, bias : 0.066091388463974  \n",
      "Steps : 2653 , Loss : 0.7832331657409668, Weights 1.0500584840774536, bias : 0.06607580929994583  \n",
      "Steps : 2654 , Loss : 0.7832327485084534, Weights 1.0500752925872803, bias : 0.06606025993824005  \n",
      "Steps : 2655 , Loss : 0.7832322716712952, Weights 1.0500919818878174, bias : 0.06604474037885666  \n",
      "Steps : 2656 , Loss : 0.7832316756248474, Weights 1.0501086711883545, bias : 0.06602925062179565  \n",
      "Steps : 2657 , Loss : 0.7832311987876892, Weights 1.0501253604888916, bias : 0.06601379066705704  \n",
      "Steps : 2658 , Loss : 0.7832306623458862, Weights 1.0501420497894287, bias : 0.06599835306406021  \n",
      "Steps : 2659 , Loss : 0.783230185508728, Weights 1.0501586198806763, bias : 0.06598294526338577  \n",
      "Steps : 2660 , Loss : 0.783229649066925, Weights 1.0501751899719238, bias : 0.06596756726503372  \n",
      "Steps : 2661 , Loss : 0.7832290530204773, Weights 1.0501917600631714, bias : 0.06595221906900406  \n",
      "Steps : 2662 , Loss : 0.7832286357879639, Weights 1.050208330154419, bias : 0.06593690067529678  \n",
      "Steps : 2663 , Loss : 0.7832280993461609, Weights 1.050224781036377, bias : 0.0659216046333313  \n",
      "Steps : 2664 , Loss : 0.7832276821136475, Weights 1.050241231918335, bias : 0.0659063383936882  \n",
      "Steps : 2665 , Loss : 0.7832271456718445, Weights 1.050257682800293, bias : 0.06589110195636749  \n",
      "Steps : 2666 , Loss : 0.7832266092300415, Weights 1.050274133682251, bias : 0.06587589532136917  \n",
      "Steps : 2667 , Loss : 0.7832260727882385, Weights 1.0502904653549194, bias : 0.06586071103811264  \n",
      "Steps : 2668 , Loss : 0.7832256555557251, Weights 1.050306797027588, bias : 0.0658455565571785  \n",
      "Steps : 2669 , Loss : 0.7832251191139221, Weights 1.0503231287002563, bias : 0.06583043187856674  \n",
      "Steps : 2670 , Loss : 0.7832246422767639, Weights 1.0503394603729248, bias : 0.06581532955169678  \n",
      "Steps : 2671 , Loss : 0.7832241654396057, Weights 1.0503556728363037, bias : 0.0658002570271492  \n",
      "Steps : 2672 , Loss : 0.7832236289978027, Weights 1.0503718852996826, bias : 0.06578521430492401  \n",
      "Steps : 2673 , Loss : 0.7832232117652893, Weights 1.0503880977630615, bias : 0.06577020138502121  \n",
      "Steps : 2674 , Loss : 0.7832225561141968, Weights 1.0504043102264404, bias : 0.0657552108168602  \n",
      "Steps : 2675 , Loss : 0.7832221388816833, Weights 1.0504205226898193, bias : 0.06574025005102158  \n",
      "Steps : 2676 , Loss : 0.7832217216491699, Weights 1.0504366159439087, bias : 0.06572531908750534  \n",
      "Steps : 2677 , Loss : 0.7832213044166565, Weights 1.050452709197998, bias : 0.0657104104757309  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2678 , Loss : 0.7832207679748535, Weights 1.0504688024520874, bias : 0.06569553166627884  \n",
      "Steps : 2679 , Loss : 0.7832202911376953, Weights 1.0504848957061768, bias : 0.06568068265914917  \n",
      "Steps : 2680 , Loss : 0.7832198143005371, Weights 1.0505008697509766, bias : 0.06566585600376129  \n",
      "Steps : 2681 , Loss : 0.7832192778587341, Weights 1.0505168437957764, bias : 0.0656510591506958  \n",
      "Steps : 2682 , Loss : 0.7832188606262207, Weights 1.0505328178405762, bias : 0.0656362846493721  \n",
      "Steps : 2683 , Loss : 0.7832183837890625, Weights 1.050548791885376, bias : 0.06562153995037079  \n",
      "Steps : 2684 , Loss : 0.7832179069519043, Weights 1.0505646467208862, bias : 0.06560682505369186  \n",
      "Steps : 2685 , Loss : 0.7832174897193909, Weights 1.0505805015563965, bias : 0.06559213250875473  \n",
      "Steps : 2686 , Loss : 0.7832169532775879, Weights 1.0505963563919067, bias : 0.06557746976613998  \n",
      "Steps : 2687 , Loss : 0.7832164764404297, Weights 1.050612211227417, bias : 0.06556283682584763  \n",
      "Steps : 2688 , Loss : 0.7832159996032715, Weights 1.0506279468536377, bias : 0.06554822623729706  \n",
      "Steps : 2689 , Loss : 0.7832156419754028, Weights 1.0506436824798584, bias : 0.06553364545106888  \n",
      "Steps : 2690 , Loss : 0.7832151651382446, Weights 1.050659418106079, bias : 0.06551908701658249  \n",
      "Steps : 2691 , Loss : 0.7832146286964417, Weights 1.0506751537322998, bias : 0.06550455838441849  \n",
      "Steps : 2692 , Loss : 0.7832142114639282, Weights 1.0506908893585205, bias : 0.06549005210399628  \n",
      "Steps : 2693 , Loss : 0.78321373462677, Weights 1.0507065057754517, bias : 0.06547557562589645  \n",
      "Steps : 2694 , Loss : 0.7832133769989014, Weights 1.0507221221923828, bias : 0.06546112895011902  \n",
      "Steps : 2695 , Loss : 0.7832128405570984, Weights 1.050737738609314, bias : 0.06544670462608337  \n",
      "Steps : 2696 , Loss : 0.783212423324585, Weights 1.0507533550262451, bias : 0.06543231010437012  \n",
      "Steps : 2697 , Loss : 0.783211886882782, Weights 1.0507688522338867, bias : 0.06541793793439865  \n",
      "Steps : 2698 , Loss : 0.7832115292549133, Weights 1.0507843494415283, bias : 0.06540359556674957  \n",
      "Steps : 2699 , Loss : 0.7832110524177551, Weights 1.05079984664917, bias : 0.06538927555084229  \n",
      "Steps : 2700 , Loss : 0.7832105755805969, Weights 1.0508153438568115, bias : 0.06537498533725739  \n",
      "Steps : 2701 , Loss : 0.7832101583480835, Weights 1.0508308410644531, bias : 0.06536071747541428  \n",
      "Steps : 2702 , Loss : 0.7832098603248596, Weights 1.0508462190628052, bias : 0.06534647941589355  \n",
      "Steps : 2703 , Loss : 0.7832093238830566, Weights 1.0508615970611572, bias : 0.06533226370811462  \n",
      "Steps : 2704 , Loss : 0.7832089066505432, Weights 1.0508769750595093, bias : 0.06531807780265808  \n",
      "Steps : 2705 , Loss : 0.7832084894180298, Weights 1.0508923530578613, bias : 0.06530391424894333  \n",
      "Steps : 2706 , Loss : 0.7832080125808716, Weights 1.0509076118469238, bias : 0.06528978049755096  \n",
      "Steps : 2707 , Loss : 0.7832075357437134, Weights 1.0509228706359863, bias : 0.06527566909790039  \n",
      "Steps : 2708 , Loss : 0.7832071185112, Weights 1.0509381294250488, bias : 0.0652615875005722  \n",
      "Steps : 2709 , Loss : 0.7832067012786865, Weights 1.0509533882141113, bias : 0.06524752825498581  \n",
      "Steps : 2710 , Loss : 0.7832063436508179, Weights 1.0509686470031738, bias : 0.0652334913611412  \n",
      "Steps : 2711 , Loss : 0.7832058072090149, Weights 1.0509837865829468, bias : 0.06521948426961899  \n",
      "Steps : 2712 , Loss : 0.7832053899765015, Weights 1.0509989261627197, bias : 0.06520549952983856  \n",
      "Steps : 2713 , Loss : 0.783204972743988, Weights 1.0510140657424927, bias : 0.06519154459238052  \n",
      "Steps : 2714 , Loss : 0.7832045555114746, Weights 1.0510292053222656, bias : 0.06517761200666428  \n",
      "Steps : 2715 , Loss : 0.7832041382789612, Weights 1.051044225692749, bias : 0.06516370922327042  \n",
      "Steps : 2716 , Loss : 0.7832037210464478, Weights 1.0510592460632324, bias : 0.06514982879161835  \n",
      "Steps : 2717 , Loss : 0.7832033038139343, Weights 1.0510742664337158, bias : 0.06513597071170807  \n",
      "Steps : 2718 , Loss : 0.7832029461860657, Weights 1.0510892868041992, bias : 0.06512214243412018  \n",
      "Steps : 2719 , Loss : 0.7832025289535522, Weights 1.0511043071746826, bias : 0.06510833650827408  \n",
      "Steps : 2720 , Loss : 0.783202052116394, Weights 1.0511192083358765, bias : 0.06509456038475037  \n",
      "Steps : 2721 , Loss : 0.7832016944885254, Weights 1.0511341094970703, bias : 0.06508080661296844  \n",
      "Steps : 2722 , Loss : 0.783201277256012, Weights 1.0511490106582642, bias : 0.06506707519292831  \n",
      "Steps : 2723 , Loss : 0.7832008600234985, Weights 1.051163911819458, bias : 0.06505337357521057  \n",
      "Steps : 2724 , Loss : 0.7832005023956299, Weights 1.0511786937713623, bias : 0.06503969430923462  \n",
      "Steps : 2725 , Loss : 0.7832000851631165, Weights 1.0511934757232666, bias : 0.06502603739500046  \n",
      "Steps : 2726 , Loss : 0.7831996083259583, Weights 1.051208257675171, bias : 0.06501241028308868  \n",
      "Steps : 2727 , Loss : 0.7831993103027344, Weights 1.0512230396270752, bias : 0.0649988055229187  \n",
      "Steps : 2728 , Loss : 0.7831988334655762, Weights 1.0512378215789795, bias : 0.0649852305650711  \n",
      "Steps : 2729 , Loss : 0.7831984162330627, Weights 1.0512524843215942, bias : 0.0649716779589653  \n",
      "Steps : 2730 , Loss : 0.7831981182098389, Weights 1.051267147064209, bias : 0.06495814770460129  \n",
      "Steps : 2731 , Loss : 0.7831976413726807, Weights 1.0512818098068237, bias : 0.06494463980197906  \n",
      "Steps : 2732 , Loss : 0.7831971645355225, Weights 1.0512964725494385, bias : 0.06493116170167923  \n",
      "Steps : 2733 , Loss : 0.7831968665122986, Weights 1.0513111352920532, bias : 0.06491770595312119  \n",
      "Steps : 2734 , Loss : 0.7831965088844299, Weights 1.0513256788253784, bias : 0.06490427255630493  \n",
      "Steps : 2735 , Loss : 0.783195972442627, Weights 1.0513402223587036, bias : 0.06489086896181107  \n",
      "Steps : 2736 , Loss : 0.7831956744194031, Weights 1.0513547658920288, bias : 0.06487748771905899  \n",
      "Steps : 2737 , Loss : 0.7831952571868896, Weights 1.051369309425354, bias : 0.0648641288280487  \n",
      "Steps : 2738 , Loss : 0.783194899559021, Weights 1.0513837337493896, bias : 0.06485079973936081  \n",
      "Steps : 2739 , Loss : 0.7831944823265076, Weights 1.0513981580734253, bias : 0.0648374930024147  \n",
      "Steps : 2740 , Loss : 0.7831941246986389, Weights 1.051412582397461, bias : 0.06482420861721039  \n",
      "Steps : 2741 , Loss : 0.783193826675415, Weights 1.0514270067214966, bias : 0.06481094658374786  \n",
      "Steps : 2742 , Loss : 0.7831933498382568, Weights 1.0514414310455322, bias : 0.06479771435260773  \n",
      "Steps : 2743 , Loss : 0.783193051815033, Weights 1.0514557361602783, bias : 0.06478450447320938  \n",
      "Steps : 2744 , Loss : 0.7831925749778748, Weights 1.0514700412750244, bias : 0.06477131694555283  \n",
      "Steps : 2745 , Loss : 0.7831922173500061, Weights 1.0514843463897705, bias : 0.06475815176963806  \n",
      "Steps : 2746 , Loss : 0.7831919193267822, Weights 1.0514986515045166, bias : 0.06474501639604568  \n",
      "Steps : 2747 , Loss : 0.783191442489624, Weights 1.0515129566192627, bias : 0.0647319033741951  \n",
      "Steps : 2748 , Loss : 0.7831910848617554, Weights 1.0515271425247192, bias : 0.0647188127040863  \n",
      "Steps : 2749 , Loss : 0.7831907868385315, Weights 1.0515413284301758, bias : 0.0647057443857193  \n",
      "Steps : 2750 , Loss : 0.7831903100013733, Weights 1.0515555143356323, bias : 0.06469269841909409  \n",
      "Steps : 2751 , Loss : 0.7831900119781494, Weights 1.0515697002410889, bias : 0.06467968225479126  \n",
      "Steps : 2752 , Loss : 0.7831895351409912, Weights 1.0515838861465454, bias : 0.06466668844223022  \n",
      "Steps : 2753 , Loss : 0.7831892371177673, Weights 1.0515979528427124, bias : 0.06465371698141098  \n",
      "Steps : 2754 , Loss : 0.7831889390945435, Weights 1.0516120195388794, bias : 0.06464076787233353  \n",
      "Steps : 2755 , Loss : 0.7831884622573853, Weights 1.0516260862350464, bias : 0.06462784111499786  \n",
      "Steps : 2756 , Loss : 0.7831881642341614, Weights 1.0516401529312134, bias : 0.06461493670940399  \n",
      "Steps : 2757 , Loss : 0.7831878662109375, Weights 1.0516542196273804, bias : 0.06460206210613251  \n",
      "Steps : 2758 , Loss : 0.7831873893737793, Weights 1.0516681671142578, bias : 0.06458920985460281  \n",
      "Steps : 2759 , Loss : 0.7831870913505554, Weights 1.0516821146011353, bias : 0.06457637995481491  \n",
      "Steps : 2760 , Loss : 0.7831867337226868, Weights 1.0516960620880127, bias : 0.0645635724067688  \n",
      "Steps : 2761 , Loss : 0.7831863164901733, Weights 1.0517100095748901, bias : 0.06455078721046448  \n",
      "Steps : 2762 , Loss : 0.7831860184669495, Weights 1.051723837852478, bias : 0.06453802436590195  \n",
      "Steps : 2763 , Loss : 0.783185601234436, Weights 1.051737666130066, bias : 0.0645252913236618  \n",
      "Steps : 2764 , Loss : 0.7831852436065674, Weights 1.0517514944076538, bias : 0.06451258063316345  \n",
      "Steps : 2765 , Loss : 0.7831849455833435, Weights 1.0517653226852417, bias : 0.06449989229440689  \n",
      "Steps : 2766 , Loss : 0.7831846475601196, Weights 1.0517791509628296, bias : 0.06448722630739212  \n",
      "Steps : 2767 , Loss : 0.7831842303276062, Weights 1.051792860031128, bias : 0.06447458267211914  \n",
      "Steps : 2768 , Loss : 0.7831838726997375, Weights 1.0518065690994263, bias : 0.06446196138858795  \n",
      "Steps : 2769 , Loss : 0.7831835150718689, Weights 1.0518202781677246, bias : 0.06444936245679855  \n",
      "Steps : 2770 , Loss : 0.783183217048645, Weights 1.051833987236023, bias : 0.06443678587675095  \n",
      "Steps : 2771 , Loss : 0.7831828594207764, Weights 1.0518476963043213, bias : 0.06442423164844513  \n",
      "Steps : 2772 , Loss : 0.7831825017929077, Weights 1.05186128616333, bias : 0.0644116997718811  \n",
      "Steps : 2773 , Loss : 0.7831822037696838, Weights 1.0518748760223389, bias : 0.06439919769763947  \n",
      "Steps : 2774 , Loss : 0.78318190574646, Weights 1.0518884658813477, bias : 0.06438671797513962  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2775 , Loss : 0.7831814289093018, Weights 1.0519020557403564, bias : 0.06437426060438156  \n",
      "Steps : 2776 , Loss : 0.7831811308860779, Weights 1.0519156455993652, bias : 0.0643618255853653  \n",
      "Steps : 2777 , Loss : 0.783180832862854, Weights 1.0519291162490845, bias : 0.06434941291809082  \n",
      "Steps : 2778 , Loss : 0.7831804752349854, Weights 1.0519425868988037, bias : 0.06433702260255814  \n",
      "Steps : 2779 , Loss : 0.7831800580024719, Weights 1.051956057548523, bias : 0.06432465463876724  \n",
      "Steps : 2780 , Loss : 0.7831798791885376, Weights 1.0519695281982422, bias : 0.06431230902671814  \n",
      "Steps : 2781 , Loss : 0.7831794619560242, Weights 1.0519829988479614, bias : 0.06429998576641083  \n",
      "Steps : 2782 , Loss : 0.7831791639328003, Weights 1.0519964694976807, bias : 0.0642876848578453  \n",
      "Steps : 2783 , Loss : 0.7831788659095764, Weights 1.0520098209381104, bias : 0.06427540630102158  \n",
      "Steps : 2784 , Loss : 0.7831785082817078, Weights 1.05202317237854, bias : 0.06426315009593964  \n",
      "Steps : 2785 , Loss : 0.7831780910491943, Weights 1.0520365238189697, bias : 0.06425091624259949  \n",
      "Steps : 2786 , Loss : 0.7831777930259705, Weights 1.0520498752593994, bias : 0.06423870474100113  \n",
      "Steps : 2787 , Loss : 0.7831774950027466, Weights 1.052063226699829, bias : 0.06422651559114456  \n",
      "Steps : 2788 , Loss : 0.7831771969795227, Weights 1.0520764589309692, bias : 0.06421434879302979  \n",
      "Steps : 2789 , Loss : 0.7831768989562988, Weights 1.0520896911621094, bias : 0.0642022043466568  \n",
      "Steps : 2790 , Loss : 0.783176600933075, Weights 1.0521029233932495, bias : 0.0641900822520256  \n",
      "Steps : 2791 , Loss : 0.7831762433052063, Weights 1.0521161556243896, bias : 0.0641779825091362  \n",
      "Steps : 2792 , Loss : 0.7831758856773376, Weights 1.0521293878555298, bias : 0.06416590511798859  \n",
      "Steps : 2793 , Loss : 0.7831755876541138, Weights 1.0521425008773804, bias : 0.06415385007858276  \n",
      "Steps : 2794 , Loss : 0.7831752896308899, Weights 1.052155613899231, bias : 0.06414181739091873  \n",
      "Steps : 2795 , Loss : 0.783174991607666, Weights 1.0521687269210815, bias : 0.06412980705499649  \n",
      "Steps : 2796 , Loss : 0.7831747531890869, Weights 1.0521818399429321, bias : 0.06411781907081604  \n",
      "Steps : 2797 , Loss : 0.7831743359565735, Weights 1.0521949529647827, bias : 0.06410585343837738  \n",
      "Steps : 2798 , Loss : 0.7831740379333496, Weights 1.0522079467773438, bias : 0.06409390270709991  \n",
      "Steps : 2799 , Loss : 0.783173680305481, Weights 1.0522209405899048, bias : 0.06408197432756424  \n",
      "Steps : 2800 , Loss : 0.7831734418869019, Weights 1.0522339344024658, bias : 0.06407006829977036  \n",
      "Steps : 2801 , Loss : 0.783173143863678, Weights 1.0522469282150269, bias : 0.06405818462371826  \n",
      "Steps : 2802 , Loss : 0.7831727862358093, Weights 1.052259922027588, bias : 0.06404632329940796  \n",
      "Steps : 2803 , Loss : 0.7831724286079407, Weights 1.0522727966308594, bias : 0.06403448432683945  \n",
      "Steps : 2804 , Loss : 0.7831721305847168, Weights 1.0522856712341309, bias : 0.06402266770601273  \n",
      "Steps : 2805 , Loss : 0.7831718921661377, Weights 1.0522985458374023, bias : 0.0640108734369278  \n",
      "Steps : 2806 , Loss : 0.783171534538269, Weights 1.0523114204406738, bias : 0.06399910151958466  \n",
      "Steps : 2807 , Loss : 0.7831712365150452, Weights 1.0523242950439453, bias : 0.0639873519539833  \n",
      "Steps : 2808 , Loss : 0.7831709384918213, Weights 1.0523371696472168, bias : 0.06397562474012375  \n",
      "Steps : 2809 , Loss : 0.7831706404685974, Weights 1.0523499250411987, bias : 0.06396391242742538  \n",
      "Steps : 2810 , Loss : 0.7831704020500183, Weights 1.0523626804351807, bias : 0.06395222246646881  \n",
      "Steps : 2811 , Loss : 0.7831699848175049, Weights 1.0523754358291626, bias : 0.06394055485725403  \n",
      "Steps : 2812 , Loss : 0.7831698656082153, Weights 1.0523881912231445, bias : 0.06392890959978104  \n",
      "Steps : 2813 , Loss : 0.7831693887710571, Weights 1.0524009466171265, bias : 0.06391728669404984  \n",
      "Steps : 2814 , Loss : 0.7831692695617676, Weights 1.0524135828018188, bias : 0.06390568614006042  \n",
      "Steps : 2815 , Loss : 0.7831688523292542, Weights 1.0524262189865112, bias : 0.0638941079378128  \n",
      "Steps : 2816 , Loss : 0.783168613910675, Weights 1.0524388551712036, bias : 0.06388254463672638  \n",
      "Steps : 2817 , Loss : 0.7831683158874512, Weights 1.052451491355896, bias : 0.06387100368738174  \n",
      "Steps : 2818 , Loss : 0.7831679582595825, Weights 1.0524641275405884, bias : 0.0638594850897789  \n",
      "Steps : 2819 , Loss : 0.7831677198410034, Weights 1.0524766445159912, bias : 0.06384798884391785  \n",
      "Steps : 2820 , Loss : 0.7831674218177795, Weights 1.052489161491394, bias : 0.06383651494979858  \n",
      "Steps : 2821 , Loss : 0.7831671833992004, Weights 1.0525016784667969, bias : 0.06382506340742111  \n",
      "Steps : 2822 , Loss : 0.7831668257713318, Weights 1.0525141954421997, bias : 0.06381362676620483  \n",
      "Steps : 2823 , Loss : 0.7831665873527527, Weights 1.0525267124176025, bias : 0.06380221247673035  \n",
      "Steps : 2824 , Loss : 0.7831662893295288, Weights 1.0525392293930054, bias : 0.06379082053899765  \n",
      "Steps : 2825 , Loss : 0.7831659913063049, Weights 1.0525516271591187, bias : 0.06377945095300674  \n",
      "Steps : 2826 , Loss : 0.7831655740737915, Weights 1.052564024925232, bias : 0.06376810371875763  \n",
      "Steps : 2827 , Loss : 0.783165454864502, Weights 1.0525764226913452, bias : 0.06375677138566971  \n",
      "Steps : 2828 , Loss : 0.7831651568412781, Weights 1.0525888204574585, bias : 0.06374546140432358  \n",
      "Steps : 2829 , Loss : 0.783164918422699, Weights 1.0526012182235718, bias : 0.06373417377471924  \n",
      "Steps : 2830 , Loss : 0.7831646203994751, Weights 1.052613615989685, bias : 0.06372290849685669  \n",
      "Steps : 2831 , Loss : 0.7831642627716064, Weights 1.0526258945465088, bias : 0.06371165812015533  \n",
      "Steps : 2832 , Loss : 0.7831640839576721, Weights 1.0526381731033325, bias : 0.06370043009519577  \n",
      "Steps : 2833 , Loss : 0.7831637859344482, Weights 1.0526504516601562, bias : 0.063689224421978  \n",
      "Steps : 2834 , Loss : 0.7831634283065796, Weights 1.05266273021698, bias : 0.06367804110050201  \n",
      "Steps : 2835 , Loss : 0.7831632494926453, Weights 1.0526750087738037, bias : 0.06366687268018723  \n",
      "Steps : 2836 , Loss : 0.7831630110740662, Weights 1.052687168121338, bias : 0.06365572661161423  \n",
      "Steps : 2837 , Loss : 0.7831627130508423, Weights 1.052699327468872, bias : 0.06364460289478302  \n",
      "Steps : 2838 , Loss : 0.7831623554229736, Weights 1.0527114868164062, bias : 0.0636335015296936  \n",
      "Steps : 2839 , Loss : 0.7831620573997498, Weights 1.0527236461639404, bias : 0.06362241506576538  \n",
      "Steps : 2840 , Loss : 0.7831618785858154, Weights 1.0527358055114746, bias : 0.06361135095357895  \n",
      "Steps : 2841 , Loss : 0.7831616401672363, Weights 1.0527479648590088, bias : 0.06360030919313431  \n",
      "Steps : 2842 , Loss : 0.7831613421440125, Weights 1.0527600049972534, bias : 0.06358928233385086  \n",
      "Steps : 2843 , Loss : 0.7831609845161438, Weights 1.052772045135498, bias : 0.0635782778263092  \n",
      "Steps : 2844 , Loss : 0.7831608057022095, Weights 1.0527840852737427, bias : 0.06356729567050934  \n",
      "Steps : 2845 , Loss : 0.7831605672836304, Weights 1.0527961254119873, bias : 0.06355632841587067  \n",
      "Steps : 2846 , Loss : 0.7831602692604065, Weights 1.052808165550232, bias : 0.06354538351297379  \n",
      "Steps : 2847 , Loss : 0.7831599712371826, Weights 1.052820086479187, bias : 0.0635344609618187  \n",
      "Steps : 2848 , Loss : 0.783159613609314, Weights 1.052832007408142, bias : 0.0635235607624054  \n",
      "Steps : 2849 , Loss : 0.7831594944000244, Weights 1.0528439283370972, bias : 0.06351267546415329  \n",
      "Steps : 2850 , Loss : 0.7831592559814453, Weights 1.0528558492660522, bias : 0.06350181251764297  \n",
      "Steps : 2851 , Loss : 0.7831589579582214, Weights 1.0528677701950073, bias : 0.06349097192287445  \n",
      "Steps : 2852 , Loss : 0.7831587195396423, Weights 1.0528796911239624, bias : 0.06348014622926712  \n",
      "Steps : 2853 , Loss : 0.7831584215164185, Weights 1.052891492843628, bias : 0.06346934288740158  \n",
      "Steps : 2854 , Loss : 0.7831582427024841, Weights 1.0529032945632935, bias : 0.06345856189727783  \n",
      "Steps : 2855 , Loss : 0.7831579446792603, Weights 1.052915096282959, bias : 0.06344779580831528  \n",
      "Steps : 2856 , Loss : 0.7831576466560364, Weights 1.0529268980026245, bias : 0.06343705207109451  \n",
      "Steps : 2857 , Loss : 0.7831573486328125, Weights 1.05293869972229, bias : 0.06342632323503494  \n",
      "Steps : 2858 , Loss : 0.7831571698188782, Weights 1.0529505014419556, bias : 0.06341561675071716  \n",
      "Steps : 2859 , Loss : 0.7831568717956543, Weights 1.0529621839523315, bias : 0.06340493261814117  \n",
      "Steps : 2860 , Loss : 0.7831567525863647, Weights 1.0529738664627075, bias : 0.06339426338672638  \n",
      "Steps : 2861 , Loss : 0.7831564545631409, Weights 1.0529855489730835, bias : 0.06338361650705338  \n",
      "Steps : 2862 , Loss : 0.783156156539917, Weights 1.0529972314834595, bias : 0.06337299197912216  \n",
      "Steps : 2863 , Loss : 0.7831559181213379, Weights 1.0530089139938354, bias : 0.06336238235235214  \n",
      "Steps : 2864 , Loss : 0.783155620098114, Weights 1.0530205965042114, bias : 0.06335179507732391  \n",
      "Steps : 2865 , Loss : 0.7831553816795349, Weights 1.0530321598052979, bias : 0.06334122270345688  \n",
      "Steps : 2866 , Loss : 0.7831551432609558, Weights 1.0530437231063843, bias : 0.06333067268133163  \n",
      "Steps : 2867 , Loss : 0.7831549644470215, Weights 1.0530552864074707, bias : 0.06332013756036758  \n",
      "Steps : 2868 , Loss : 0.7831546664237976, Weights 1.0530668497085571, bias : 0.06330962479114532  \n",
      "Steps : 2869 , Loss : 0.7831544280052185, Weights 1.0530784130096436, bias : 0.06329913437366486  \n",
      "Steps : 2870 , Loss : 0.7831542491912842, Weights 1.05308997631073, bias : 0.06328865885734558  \n",
      "Steps : 2871 , Loss : 0.7831540107727051, Weights 1.0531014204025269, bias : 0.0632782056927681  \n",
      "Steps : 2872 , Loss : 0.7831537127494812, Weights 1.0531128644943237, bias : 0.0632677674293518  \n",
      "Steps : 2873 , Loss : 0.7831535339355469, Weights 1.0531243085861206, bias : 0.06325735151767731  \n",
      "Steps : 2874 , Loss : 0.783153235912323, Weights 1.0531357526779175, bias : 0.063246950507164  \n",
      "Steps : 2875 , Loss : 0.7831530570983887, Weights 1.0531471967697144, bias : 0.06323657184839249  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2876 , Loss : 0.7831527590751648, Weights 1.0531586408615112, bias : 0.06322621554136276  \n",
      "Steps : 2877 , Loss : 0.7831525206565857, Weights 1.0531699657440186, bias : 0.06321587413549423  \n",
      "Steps : 2878 , Loss : 0.7831523418426514, Weights 1.0531812906265259, bias : 0.06320555508136749  \n",
      "Steps : 2879 , Loss : 0.7831521034240723, Weights 1.0531926155090332, bias : 0.06319525092840195  \n",
      "Steps : 2880 , Loss : 0.7831518650054932, Weights 1.0532039403915405, bias : 0.06318496912717819  \n",
      "Steps : 2881 , Loss : 0.7831516265869141, Weights 1.0532152652740479, bias : 0.06317470222711563  \n",
      "Steps : 2882 , Loss : 0.783151388168335, Weights 1.0532265901565552, bias : 0.06316445767879486  \n",
      "Steps : 2883 , Loss : 0.7831511497497559, Weights 1.053237795829773, bias : 0.06315422803163528  \n",
      "Steps : 2884 , Loss : 0.783150851726532, Weights 1.0532490015029907, bias : 0.0631440207362175  \n",
      "Steps : 2885 , Loss : 0.7831506133079529, Weights 1.0532602071762085, bias : 0.06313382834196091  \n",
      "Steps : 2886 , Loss : 0.7831504940986633, Weights 1.0532714128494263, bias : 0.0631236582994461  \n",
      "Steps : 2887 , Loss : 0.7831501960754395, Weights 1.053282618522644, bias : 0.0631135031580925  \n",
      "Steps : 2888 , Loss : 0.7831499576568604, Weights 1.0532938241958618, bias : 0.06310337036848068  \n",
      "Steps : 2889 , Loss : 0.783149778842926, Weights 1.05330491065979, bias : 0.06309325248003006  \n",
      "Steps : 2890 , Loss : 0.7831495404243469, Weights 1.0533159971237183, bias : 0.06308315694332123  \n",
      "Steps : 2891 , Loss : 0.783149242401123, Weights 1.0533270835876465, bias : 0.06307307630777359  \n",
      "Steps : 2892 , Loss : 0.7831491231918335, Weights 1.0533381700515747, bias : 0.06306301802396774  \n",
      "Steps : 2893 , Loss : 0.7831488251686096, Weights 1.053349256515503, bias : 0.06305297464132309  \n",
      "Steps : 2894 , Loss : 0.7831486463546753, Weights 1.0533603429794312, bias : 0.06304295361042023  \n",
      "Steps : 2895 , Loss : 0.7831483483314514, Weights 1.0533713102340698, bias : 0.06303294748067856  \n",
      "Steps : 2896 , Loss : 0.7831481695175171, Weights 1.0533822774887085, bias : 0.06302295625209808  \n",
      "Steps : 2897 , Loss : 0.7831479907035828, Weights 1.0533932447433472, bias : 0.0630129873752594  \n",
      "Steps : 2898 , Loss : 0.7831477522850037, Weights 1.0534042119979858, bias : 0.06300303339958191  \n",
      "Steps : 2899 , Loss : 0.7831475734710693, Weights 1.0534151792526245, bias : 0.06299310177564621  \n",
      "Steps : 2900 , Loss : 0.7831472754478455, Weights 1.0534261465072632, bias : 0.0629831850528717  \n",
      "Steps : 2901 , Loss : 0.7831470966339111, Weights 1.0534369945526123, bias : 0.06297329068183899  \n",
      "Steps : 2902 , Loss : 0.7831467986106873, Weights 1.0534478425979614, bias : 0.06296341121196747  \n",
      "Steps : 2903 , Loss : 0.7831466794013977, Weights 1.0534586906433105, bias : 0.06295354664325714  \n",
      "Steps : 2904 , Loss : 0.7831463813781738, Weights 1.0534695386886597, bias : 0.0629437044262886  \n",
      "Steps : 2905 , Loss : 0.7831462025642395, Weights 1.0534803867340088, bias : 0.06293387711048126  \n",
      "Steps : 2906 , Loss : 0.78314608335495, Weights 1.053491234779358, bias : 0.06292407214641571  \n",
      "Steps : 2907 , Loss : 0.7831457257270813, Weights 1.053502082824707, bias : 0.06291428208351135  \n",
      "Steps : 2908 , Loss : 0.7831456065177917, Weights 1.0535128116607666, bias : 0.06290450692176819  \n",
      "Steps : 2909 , Loss : 0.7831453680992126, Weights 1.0535235404968262, bias : 0.06289475411176682  \n",
      "Steps : 2910 , Loss : 0.7831451296806335, Weights 1.0535342693328857, bias : 0.06288501620292664  \n",
      "Steps : 2911 , Loss : 0.783145010471344, Weights 1.0535449981689453, bias : 0.06287530064582825  \n",
      "Steps : 2912 , Loss : 0.7831448316574097, Weights 1.0535557270050049, bias : 0.06286559998989105  \n",
      "Steps : 2913 , Loss : 0.7831445932388306, Weights 1.0535664558410645, bias : 0.06285591423511505  \n",
      "Steps : 2914 , Loss : 0.7831443548202515, Weights 1.0535770654678345, bias : 0.06284625083208084  \n",
      "Steps : 2915 , Loss : 0.7831442356109619, Weights 1.0535876750946045, bias : 0.06283660233020782  \n",
      "Steps : 2916 , Loss : 0.783143937587738, Weights 1.0535982847213745, bias : 0.0628269761800766  \n",
      "Steps : 2917 , Loss : 0.7831437587738037, Weights 1.0536088943481445, bias : 0.06281736493110657  \n",
      "Steps : 2918 , Loss : 0.7831436395645142, Weights 1.0536195039749146, bias : 0.06280776858329773  \n",
      "Steps : 2919 , Loss : 0.7831432819366455, Weights 1.0536301136016846, bias : 0.06279819458723068  \n",
      "Steps : 2920 , Loss : 0.783143162727356, Weights 1.053640604019165, bias : 0.06278863549232483  \n",
      "Steps : 2921 , Loss : 0.7831429839134216, Weights 1.0536510944366455, bias : 0.06277909129858017  \n",
      "Steps : 2922 , Loss : 0.7831426858901978, Weights 1.053661584854126, bias : 0.0627695694565773  \n",
      "Steps : 2923 , Loss : 0.7831425666809082, Weights 1.0536720752716064, bias : 0.06276006251573563  \n",
      "Steps : 2924 , Loss : 0.7831422686576843, Weights 1.053682565689087, bias : 0.06275057047605515  \n",
      "Steps : 2925 , Loss : 0.7831421494483948, Weights 1.0536930561065674, bias : 0.06274110078811646  \n",
      "Steps : 2926 , Loss : 0.7831419110298157, Weights 1.0537035465240479, bias : 0.06273164600133896  \n",
      "Steps : 2927 , Loss : 0.7831417322158813, Weights 1.0537139177322388, bias : 0.06272220611572266  \n",
      "Steps : 2928 , Loss : 0.783141553401947, Weights 1.0537242889404297, bias : 0.06271278858184814  \n",
      "Steps : 2929 , Loss : 0.7831413149833679, Weights 1.0537346601486206, bias : 0.06270338594913483  \n",
      "Steps : 2930 , Loss : 0.7831411957740784, Weights 1.0537450313568115, bias : 0.0626939982175827  \n",
      "Steps : 2931 , Loss : 0.783141016960144, Weights 1.0537554025650024, bias : 0.06268462538719177  \n",
      "Steps : 2932 , Loss : 0.7831407189369202, Weights 1.0537657737731934, bias : 0.06267527490854263  \n",
      "Steps : 2933 , Loss : 0.7831404805183411, Weights 1.0537761449813843, bias : 0.06266593933105469  \n",
      "Steps : 2934 , Loss : 0.7831404209136963, Weights 1.0537863969802856, bias : 0.06265661865472794  \n",
      "Steps : 2935 , Loss : 0.783140242099762, Weights 1.053796648979187, bias : 0.06264732033014297  \n",
      "Steps : 2936 , Loss : 0.7831400036811829, Weights 1.0538069009780884, bias : 0.06263803690671921  \n",
      "Steps : 2937 , Loss : 0.7831398248672485, Weights 1.0538171529769897, bias : 0.06262876838445663  \n",
      "Steps : 2938 , Loss : 0.7831394672393799, Weights 1.0538274049758911, bias : 0.06261951476335526  \n",
      "Steps : 2939 , Loss : 0.7831394076347351, Weights 1.0538376569747925, bias : 0.06261028349399567  \n",
      "Steps : 2940 , Loss : 0.783139169216156, Weights 1.0538477897644043, bias : 0.06260106712579727  \n",
      "Steps : 2941 , Loss : 0.7831391096115112, Weights 1.0538579225540161, bias : 0.06259186565876007  \n",
      "Steps : 2942 , Loss : 0.7831388711929321, Weights 1.053868055343628, bias : 0.06258267909288406  \n",
      "Steps : 2943 , Loss : 0.7831385731697083, Weights 1.0538781881332397, bias : 0.06257351487874985  \n",
      "Steps : 2944 , Loss : 0.7831385135650635, Weights 1.0538883209228516, bias : 0.06256436556577682  \n",
      "Steps : 2945 , Loss : 0.7831382751464844, Weights 1.0538984537124634, bias : 0.062555231153965  \n",
      "Steps : 2946 , Loss : 0.78313809633255, Weights 1.0539085865020752, bias : 0.06254611164331436  \n",
      "Steps : 2947 , Loss : 0.7831379771232605, Weights 1.0539186000823975, bias : 0.06253701448440552  \n",
      "Steps : 2948 , Loss : 0.7831377983093262, Weights 1.0539286136627197, bias : 0.06252793222665787  \n",
      "Steps : 2949 , Loss : 0.7831375598907471, Weights 1.053938627243042, bias : 0.06251886487007141  \n",
      "Steps : 2950 , Loss : 0.7831374406814575, Weights 1.0539486408233643, bias : 0.06250981241464615  \n",
      "Steps : 2951 , Loss : 0.7831372022628784, Weights 1.0539586544036865, bias : 0.06250078231096268  \n",
      "Steps : 2952 , Loss : 0.7831370830535889, Weights 1.0539686679840088, bias : 0.0624917633831501  \n",
      "Steps : 2953 , Loss : 0.7831368446350098, Weights 1.053978681564331, bias : 0.06248276308178902  \n",
      "Steps : 2954 , Loss : 0.7831366062164307, Weights 1.0539885759353638, bias : 0.06247377768158913  \n",
      "Steps : 2955 , Loss : 0.7831364274024963, Weights 1.0539984703063965, bias : 0.06246481090784073  \n",
      "Steps : 2956 , Loss : 0.7831363081932068, Weights 1.0540083646774292, bias : 0.062455859035253525  \n",
      "Steps : 2957 , Loss : 0.7831361293792725, Weights 1.054018259048462, bias : 0.06244692578911781  \n",
      "Steps : 2958 , Loss : 0.7831360101699829, Weights 1.0540281534194946, bias : 0.062438007444143295  \n",
      "Steps : 2959 , Loss : 0.7831358313560486, Weights 1.0540380477905273, bias : 0.06242910400032997  \n",
      "Steps : 2960 , Loss : 0.7831355929374695, Weights 1.05404794216156, bias : 0.06242021918296814  \n",
      "Steps : 2961 , Loss : 0.7831353545188904, Weights 1.0540577173233032, bias : 0.0624113492667675  \n",
      "Steps : 2962 , Loss : 0.7831352353096008, Weights 1.0540674924850464, bias : 0.06240249425172806  \n",
      "Steps : 2963 , Loss : 0.7831350564956665, Weights 1.0540772676467896, bias : 0.062393657863140106  \n",
      "Steps : 2964 , Loss : 0.783134937286377, Weights 1.0540870428085327, bias : 0.06238483637571335  \n",
      "Steps : 2965 , Loss : 0.7831347584724426, Weights 1.0540968179702759, bias : 0.062376029789447784  \n",
      "Steps : 2966 , Loss : 0.7831345200538635, Weights 1.054106593132019, bias : 0.06236724182963371  \n",
      "Steps : 2967 , Loss : 0.7831344604492188, Weights 1.0541163682937622, bias : 0.062358468770980835  \n",
      "Steps : 2968 , Loss : 0.7831342816352844, Weights 1.0541260242462158, bias : 0.06234971061348915  \n",
      "Steps : 2969 , Loss : 0.7831339836120605, Weights 1.0541356801986694, bias : 0.06234096735715866  \n",
      "Steps : 2970 , Loss : 0.7831339240074158, Weights 1.054145336151123, bias : 0.06233224272727966  \n",
      "Steps : 2971 , Loss : 0.783133864402771, Weights 1.0541549921035767, bias : 0.06232353299856186  \n",
      "Steps : 2972 , Loss : 0.7831336259841919, Weights 1.0541646480560303, bias : 0.06231483817100525  \n",
      "Steps : 2973 , Loss : 0.7831333875656128, Weights 1.0541743040084839, bias : 0.06230616196990013  \n",
      "Steps : 2974 , Loss : 0.7831330895423889, Weights 1.0541839599609375, bias : 0.06229750066995621  \n",
      "Steps : 2975 , Loss : 0.7831330895423889, Weights 1.0541934967041016, bias : 0.06228885427117348  \n",
      "Steps : 2976 , Loss : 0.783132791519165, Weights 1.0542030334472656, bias : 0.06228022277355194  \n",
      "Steps : 2977 , Loss : 0.7831327319145203, Weights 1.0542125701904297, bias : 0.0622716099023819  \n",
      "Steps : 2978 , Loss : 0.7831324934959412, Weights 1.0542221069335938, bias : 0.06226301193237305  \n",
      "Steps : 2979 , Loss : 0.7831323146820068, Weights 1.0542316436767578, bias : 0.06225442886352539  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 2980 , Loss : 0.7831321954727173, Weights 1.0542411804199219, bias : 0.06224586069583893  \n",
      "Steps : 2981 , Loss : 0.783132016658783, Weights 1.054250717163086, bias : 0.06223730742931366  \n",
      "Steps : 2982 , Loss : 0.7831318378448486, Weights 1.0542601346969604, bias : 0.06222877278923988  \n",
      "Steps : 2983 , Loss : 0.7831317186355591, Weights 1.054269552230835, bias : 0.0622202530503273  \n",
      "Steps : 2984 , Loss : 0.7831315398216248, Weights 1.0542789697647095, bias : 0.06221174821257591  \n",
      "Steps : 2985 , Loss : 0.78313148021698, Weights 1.054288387298584, bias : 0.06220325827598572  \n",
      "Steps : 2986 , Loss : 0.7831312417984009, Weights 1.0542978048324585, bias : 0.06219478324055672  \n",
      "Steps : 2987 , Loss : 0.7831311225891113, Weights 1.054307222366333, bias : 0.06218632310628891  \n",
      "Steps : 2988 , Loss : 0.783130943775177, Weights 1.0543166399002075, bias : 0.0621778778731823  \n",
      "Steps : 2989 , Loss : 0.7831308245658875, Weights 1.0543259382247925, bias : 0.062169451266527176  \n",
      "Steps : 2990 , Loss : 0.7831306457519531, Weights 1.0543352365493774, bias : 0.06216103956103325  \n",
      "Steps : 2991 , Loss : 0.7831304669380188, Weights 1.0543445348739624, bias : 0.062152642756700516  \n",
      "Steps : 2992 , Loss : 0.7831303477287292, Weights 1.0543538331985474, bias : 0.062144260853528976  \n",
      "Steps : 2993 , Loss : 0.7831301689147949, Weights 1.0543631315231323, bias : 0.06213589385151863  \n",
      "Steps : 2994 , Loss : 0.7831299304962158, Weights 1.0543724298477173, bias : 0.06212754175066948  \n",
      "Steps : 2995 , Loss : 0.783129870891571, Weights 1.0543817281723022, bias : 0.06211920455098152  \n",
      "Steps : 2996 , Loss : 0.7831296324729919, Weights 1.0543910264968872, bias : 0.06211088225245476  \n",
      "Steps : 2997 , Loss : 0.7831295728683472, Weights 1.0544002056121826, bias : 0.06210257485508919  \n",
      "Steps : 2998 , Loss : 0.7831294536590576, Weights 1.054409384727478, bias : 0.06209428235888481  \n",
      "Steps : 2999 , Loss : 0.7831292748451233, Weights 1.0544185638427734, bias : 0.06208600848913193  \n",
      "Steps : 3000 , Loss : 0.783129096031189, Weights 1.0544277429580688, bias : 0.06207774952054024  \n",
      "Steps : 3001 , Loss : 0.7831289768218994, Weights 1.0544369220733643, bias : 0.06206950545310974  \n",
      "Steps : 3002 , Loss : 0.7831287980079651, Weights 1.0544461011886597, bias : 0.06206127628684044  \n",
      "Steps : 3003 , Loss : 0.7831286787986755, Weights 1.054455280303955, bias : 0.06205306202173233  \n",
      "Steps : 3004 , Loss : 0.7831284999847412, Weights 1.054464340209961, bias : 0.062044862657785416  \n",
      "Steps : 3005 , Loss : 0.7831282615661621, Weights 1.0544734001159668, bias : 0.062036678194999695  \n",
      "Steps : 3006 , Loss : 0.7831282019615173, Weights 1.0544824600219727, bias : 0.06202850863337517  \n",
      "Steps : 3007 , Loss : 0.7831279635429382, Weights 1.0544915199279785, bias : 0.062020353972911835  \n",
      "Steps : 3008 , Loss : 0.7831279039382935, Weights 1.0545005798339844, bias : 0.062012214213609695  \n",
      "Steps : 3009 , Loss : 0.7831277251243591, Weights 1.0545096397399902, bias : 0.06200408935546875  \n",
      "Steps : 3010 , Loss : 0.7831276059150696, Weights 1.054518699645996, bias : 0.061995979398489  \n",
      "Steps : 3011 , Loss : 0.7831274271011353, Weights 1.054527759552002, bias : 0.06198788434267044  \n",
      "Steps : 3012 , Loss : 0.7831273078918457, Weights 1.0545367002487183, bias : 0.06197980418801308  \n",
      "Steps : 3013 , Loss : 0.7831271290779114, Weights 1.0545456409454346, bias : 0.06197173893451691  \n",
      "Steps : 3014 , Loss : 0.7831270098686218, Weights 1.0545545816421509, bias : 0.06196368858218193  \n",
      "Steps : 3015 , Loss : 0.7831268906593323, Weights 1.0545635223388672, bias : 0.06195564940571785  \n",
      "Steps : 3016 , Loss : 0.7831266522407532, Weights 1.0545724630355835, bias : 0.06194762513041496  \n",
      "Steps : 3017 , Loss : 0.7831265926361084, Weights 1.0545814037322998, bias : 0.06193961575627327  \n",
      "Steps : 3018 , Loss : 0.7831263542175293, Weights 1.0545903444290161, bias : 0.06193162128329277  \n",
      "Steps : 3019 , Loss : 0.7831262946128845, Weights 1.0545991659164429, bias : 0.061923641711473465  \n",
      "Steps : 3020 , Loss : 0.7831262350082397, Weights 1.0546079874038696, bias : 0.06191567704081535  \n",
      "Steps : 3021 , Loss : 0.7831259369850159, Weights 1.0546168088912964, bias : 0.061907727271318436  \n",
      "Steps : 3022 , Loss : 0.7831259369850159, Weights 1.0546256303787231, bias : 0.06189979240298271  \n",
      "Steps : 3023 , Loss : 0.7831257581710815, Weights 1.05463445186615, bias : 0.06189187243580818  \n",
      "Steps : 3024 , Loss : 0.7831255197525024, Weights 1.0546432733535767, bias : 0.061883967369794846  \n",
      "Steps : 3025 , Loss : 0.7831254601478577, Weights 1.0546520948410034, bias : 0.061876073479652405  \n",
      "Steps : 3026 , Loss : 0.7831252813339233, Weights 1.0546609163284302, bias : 0.06186819449067116  \n",
      "Steps : 3027 , Loss : 0.7831251621246338, Weights 1.0546696186065674, bias : 0.061860330402851105  \n",
      "Steps : 3028 , Loss : 0.7831249833106995, Weights 1.0546783208847046, bias : 0.061852481216192245  \n",
      "Steps : 3029 , Loss : 0.7831248641014099, Weights 1.0546870231628418, bias : 0.06184464693069458  \n",
      "Steps : 3030 , Loss : 0.7831246852874756, Weights 1.054695725440979, bias : 0.06183682754635811  \n",
      "Steps : 3031 , Loss : 0.7831246256828308, Weights 1.0547044277191162, bias : 0.06182902306318283  \n",
      "Steps : 3032 , Loss : 0.7831244468688965, Weights 1.0547131299972534, bias : 0.06182122975587845  \n",
      "Steps : 3033 , Loss : 0.7831243872642517, Weights 1.0547218322753906, bias : 0.06181345134973526  \n",
      "Steps : 3034 , Loss : 0.7831242084503174, Weights 1.0547305345535278, bias : 0.061805687844753265  \n",
      "Steps : 3035 , Loss : 0.7831240892410278, Weights 1.0547391176223755, bias : 0.061797939240932465  \n",
      "Steps : 3036 , Loss : 0.7831239104270935, Weights 1.0547477006912231, bias : 0.06179020553827286  \n",
      "Steps : 3037 , Loss : 0.7831239104270935, Weights 1.0547562837600708, bias : 0.061782483011484146  \n",
      "Steps : 3038 , Loss : 0.7831236124038696, Weights 1.0547648668289185, bias : 0.06177477538585663  \n",
      "Steps : 3039 , Loss : 0.7831234931945801, Weights 1.0547734498977661, bias : 0.061767082661390305  \n",
      "Steps : 3040 , Loss : 0.7831233739852905, Weights 1.0547820329666138, bias : 0.061759404838085175  \n",
      "Steps : 3041 , Loss : 0.7831231951713562, Weights 1.0547906160354614, bias : 0.06175173819065094  \n",
      "Steps : 3042 , Loss : 0.7831231951713562, Weights 1.054799199104309, bias : 0.0617440864443779  \n",
      "Steps : 3043 , Loss : 0.7831230163574219, Weights 1.0548076629638672, bias : 0.06173644959926605  \n",
      "Steps : 3044 , Loss : 0.7831228375434875, Weights 1.0548161268234253, bias : 0.0617288276553154  \n",
      "Steps : 3045 , Loss : 0.783122718334198, Weights 1.0548245906829834, bias : 0.06172121688723564  \n",
      "Steps : 3046 , Loss : 0.7831226587295532, Weights 1.0548330545425415, bias : 0.06171362102031708  \n",
      "Steps : 3047 , Loss : 0.7831225395202637, Weights 1.0548415184020996, bias : 0.06170604005455971  \n",
      "Steps : 3048 , Loss : 0.7831223011016846, Weights 1.0548499822616577, bias : 0.06169847398996353  \n",
      "Steps : 3049 , Loss : 0.7831222414970398, Weights 1.0548584461212158, bias : 0.06169091910123825  \n",
      "Steps : 3050 , Loss : 0.7831221222877502, Weights 1.054866909980774, bias : 0.061683379113674164  \n",
      "Steps : 3051 , Loss : 0.7831220030784607, Weights 1.0548752546310425, bias : 0.06167585402727127  \n",
      "Steps : 3052 , Loss : 0.7831218838691711, Weights 1.054883599281311, bias : 0.06166834011673927  \n",
      "Steps : 3053 , Loss : 0.7831217050552368, Weights 1.0548919439315796, bias : 0.06166084110736847  \n",
      "Steps : 3054 , Loss : 0.783121645450592, Weights 1.0549002885818481, bias : 0.06165335699915886  \n",
      "Steps : 3055 , Loss : 0.7831214666366577, Weights 1.0549086332321167, bias : 0.061645884066820145  \n",
      "Steps : 3056 , Loss : 0.7831213474273682, Weights 1.0549169778823853, bias : 0.061638426035642624  \n",
      "Steps : 3057 , Loss : 0.7831212878227234, Weights 1.0549253225326538, bias : 0.0616309829056263  \n",
      "Steps : 3058 , Loss : 0.7831211686134338, Weights 1.0549336671829224, bias : 0.061623550951480865  \n",
      "Steps : 3059 , Loss : 0.7831209301948547, Weights 1.0549418926239014, bias : 0.06161613389849663  \n",
      "Steps : 3060 , Loss : 0.78312087059021, Weights 1.0549501180648804, bias : 0.061608731746673584  \n",
      "Steps : 3061 , Loss : 0.7831207513809204, Weights 1.0549583435058594, bias : 0.061601340770721436  \n",
      "Steps : 3062 , Loss : 0.7831205725669861, Weights 1.0549665689468384, bias : 0.06159396469593048  \n",
      "Steps : 3063 , Loss : 0.7831203937530518, Weights 1.0549747943878174, bias : 0.06158659979701042  \n",
      "Steps : 3064 , Loss : 0.783120334148407, Weights 1.0549830198287964, bias : 0.061579249799251556  \n",
      "Steps : 3065 , Loss : 0.7831202745437622, Weights 1.0549912452697754, bias : 0.061571914702653885  \n",
      "Steps : 3066 , Loss : 0.7831200957298279, Weights 1.0549994707107544, bias : 0.06156459078192711  \n",
      "Steps : 3067 , Loss : 0.7831199765205383, Weights 1.0550076961517334, bias : 0.061557281762361526  \n",
      "Steps : 3068 , Loss : 0.783119797706604, Weights 1.0550158023834229, bias : 0.06154998391866684  \n",
      "Steps : 3069 , Loss : 0.7831197381019592, Weights 1.0550239086151123, bias : 0.06154270097613335  \n",
      "Steps : 3070 , Loss : 0.7831195592880249, Weights 1.0550320148468018, bias : 0.06153542920947075  \n",
      "Steps : 3071 , Loss : 0.7831195592880249, Weights 1.0550401210784912, bias : 0.061528172343969345  \n",
      "Steps : 3072 , Loss : 0.7831194996833801, Weights 1.0550482273101807, bias : 0.061520930379629135  \n",
      "Steps : 3073 , Loss : 0.783119261264801, Weights 1.0550563335418701, bias : 0.06151369959115982  \n",
      "Steps : 3074 , Loss : 0.7831192016601562, Weights 1.0550644397735596, bias : 0.0615064837038517  \n",
      "Steps : 3075 , Loss : 0.7831189632415771, Weights 1.055072546005249, bias : 0.061499278992414474  \n",
      "Steps : 3076 , Loss : 0.7831189632415771, Weights 1.055080533027649, bias : 0.06149208918213844  \n",
      "Steps : 3077 , Loss : 0.7831188440322876, Weights 1.0550885200500488, bias : 0.06148491054773331  \n",
      "Steps : 3078 , Loss : 0.783118724822998, Weights 1.0550965070724487, bias : 0.061477746814489365  \n",
      "Steps : 3079 , Loss : 0.7831186056137085, Weights 1.0551044940948486, bias : 0.06147059425711632  \n",
      "Steps : 3080 , Loss : 0.7831184267997742, Weights 1.0551124811172485, bias : 0.061463456600904465  \n",
      "Steps : 3081 , Loss : 0.7831183075904846, Weights 1.0551204681396484, bias : 0.06145633012056351  \n",
      "Steps : 3082 , Loss : 0.7831183075904846, Weights 1.0551284551620483, bias : 0.06144921854138374  \n",
      "Steps : 3083 , Loss : 0.7831181883811951, Weights 1.0551364421844482, bias : 0.061442118138074875  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3084 , Loss : 0.7831180691719055, Weights 1.0551444292068481, bias : 0.0614350326359272  \n",
      "Steps : 3085 , Loss : 0.7831178903579712, Weights 1.0551522970199585, bias : 0.06142795830965042  \n",
      "Steps : 3086 , Loss : 0.7831178307533264, Weights 1.0551601648330688, bias : 0.061420898884534836  \n",
      "Steps : 3087 , Loss : 0.7831177711486816, Weights 1.0551680326461792, bias : 0.061413850635290146  \n",
      "Steps : 3088 , Loss : 0.7831175923347473, Weights 1.0551759004592896, bias : 0.06140681356191635  \n",
      "Steps : 3089 , Loss : 0.7831175327301025, Weights 1.0551837682724, bias : 0.06139979138970375  \n",
      "Steps : 3090 , Loss : 0.7831173539161682, Weights 1.0551916360855103, bias : 0.061392780393362045  \n",
      "Steps : 3091 , Loss : 0.7831172943115234, Weights 1.0551995038986206, bias : 0.061385784298181534  \n",
      "Steps : 3092 , Loss : 0.7831170558929443, Weights 1.055207371711731, bias : 0.06137879937887192  \n",
      "Steps : 3093 , Loss : 0.7831170558929443, Weights 1.0552151203155518, bias : 0.061371829360723495  \n",
      "Steps : 3094 , Loss : 0.7831169366836548, Weights 1.0552228689193726, bias : 0.06136487051844597  \n",
      "Steps : 3095 , Loss : 0.7831167578697205, Weights 1.0552306175231934, bias : 0.06135792285203934  \n",
      "Steps : 3096 , Loss : 0.7831167578697205, Weights 1.0552383661270142, bias : 0.0613509900867939  \n",
      "Steps : 3097 , Loss : 0.7831165790557861, Weights 1.055246114730835, bias : 0.06134406849741936  \n",
      "Steps : 3098 , Loss : 0.7831164598464966, Weights 1.0552538633346558, bias : 0.06133716180920601  \n",
      "Steps : 3099 , Loss : 0.7831164002418518, Weights 1.0552616119384766, bias : 0.061330266296863556  \n",
      "Steps : 3100 , Loss : 0.7831162810325623, Weights 1.0552693605422974, bias : 0.061323381960392  \n",
      "Steps : 3101 , Loss : 0.7831161618232727, Weights 1.0552771091461182, bias : 0.061316512525081635  \n",
      "Steps : 3102 , Loss : 0.7831161022186279, Weights 1.0552847385406494, bias : 0.061309654265642166  \n",
      "Steps : 3103 , Loss : 0.7831159830093384, Weights 1.0552923679351807, bias : 0.06130280718207359  \n",
      "Steps : 3104 , Loss : 0.7831158638000488, Weights 1.055299997329712, bias : 0.061295974999666214  \n",
      "Steps : 3105 , Loss : 0.7831156849861145, Weights 1.0553076267242432, bias : 0.06128915399312973  \n",
      "Steps : 3106 , Loss : 0.7831156849861145, Weights 1.0553152561187744, bias : 0.06128234416246414  \n",
      "Steps : 3107 , Loss : 0.783115565776825, Weights 1.0553228855133057, bias : 0.06127554923295975  \n",
      "Steps : 3108 , Loss : 0.7831153869628906, Weights 1.055330514907837, bias : 0.06126876547932625  \n",
      "Steps : 3109 , Loss : 0.7831153273582458, Weights 1.0553381443023682, bias : 0.061261992901563644  \n",
      "Steps : 3110 , Loss : 0.7831152081489563, Weights 1.0553457736968994, bias : 0.061255235224962234  \n",
      "Steps : 3111 , Loss : 0.7831150889396667, Weights 1.0553532838821411, bias : 0.06124848872423172  \n",
      "Steps : 3112 , Loss : 0.783115029335022, Weights 1.0553607940673828, bias : 0.0612417533993721  \n",
      "Steps : 3113 , Loss : 0.7831149101257324, Weights 1.0553683042526245, bias : 0.061235032975673676  \n",
      "Steps : 3114 , Loss : 0.7831147909164429, Weights 1.0553758144378662, bias : 0.061228323727846146  \n",
      "Steps : 3115 , Loss : 0.7831146717071533, Weights 1.055383324623108, bias : 0.06122162565588951  \n",
      "Steps : 3116 , Loss : 0.7831146121025085, Weights 1.0553908348083496, bias : 0.06121494248509407  \n",
      "Steps : 3117 , Loss : 0.783114492893219, Weights 1.0553983449935913, bias : 0.061208270490169525  \n",
      "Steps : 3118 , Loss : 0.7831143736839294, Weights 1.055405855178833, bias : 0.061201609671115875  \n",
      "Steps : 3119 , Loss : 0.7831143140792847, Weights 1.0554133653640747, bias : 0.06119496002793312  \n",
      "Steps : 3120 , Loss : 0.7831142544746399, Weights 1.0554207563400269, bias : 0.06118832528591156  \n",
      "Steps : 3121 , Loss : 0.7831140756607056, Weights 1.055428147315979, bias : 0.061181701719760895  \n",
      "Steps : 3122 , Loss : 0.7831140160560608, Weights 1.0554355382919312, bias : 0.061175089329481125  \n",
      "Steps : 3123 , Loss : 0.7831138372421265, Weights 1.0554429292678833, bias : 0.06116848811507225  \n",
      "Steps : 3124 , Loss : 0.7831137776374817, Weights 1.0554503202438354, bias : 0.06116189807653427  \n",
      "Steps : 3125 , Loss : 0.7831137180328369, Weights 1.0554577112197876, bias : 0.061155322939157486  \n",
      "Steps : 3126 , Loss : 0.7831135392189026, Weights 1.0554651021957397, bias : 0.061148758977651596  \n",
      "Steps : 3127 , Loss : 0.7831134796142578, Weights 1.055472493171692, bias : 0.0611422061920166  \n",
      "Steps : 3128 , Loss : 0.783113420009613, Weights 1.055479884147644, bias : 0.0611356645822525  \n",
      "Steps : 3129 , Loss : 0.7831132411956787, Weights 1.0554871559143066, bias : 0.0611291378736496  \n",
      "Steps : 3130 , Loss : 0.7831132411956787, Weights 1.0554944276809692, bias : 0.06112262234091759  \n",
      "Steps : 3131 , Loss : 0.7831131219863892, Weights 1.0555016994476318, bias : 0.06111611798405647  \n",
      "Steps : 3132 , Loss : 0.7831130027770996, Weights 1.0555089712142944, bias : 0.061109624803066254  \n",
      "Steps : 3133 , Loss : 0.7831129431724548, Weights 1.055516242980957, bias : 0.06110314279794693  \n",
      "Steps : 3134 , Loss : 0.7831128835678101, Weights 1.0555235147476196, bias : 0.0610966719686985  \n",
      "Steps : 3135 , Loss : 0.7831127643585205, Weights 1.0555307865142822, bias : 0.06109021604061127  \n",
      "Steps : 3136 , Loss : 0.783112645149231, Weights 1.0555380582809448, bias : 0.06108377128839493  \n",
      "Steps : 3137 , Loss : 0.783112645149231, Weights 1.0555453300476074, bias : 0.061077337712049484  \n",
      "Steps : 3138 , Loss : 0.7831124663352966, Weights 1.05555260181427, bias : 0.061070915311574936  \n",
      "Steps : 3139 , Loss : 0.7831124067306519, Weights 1.055559754371643, bias : 0.06106450408697128  \n",
      "Steps : 3140 , Loss : 0.7831122875213623, Weights 1.0555669069290161, bias : 0.061058104038238525  \n",
      "Steps : 3141 , Loss : 0.7831121683120728, Weights 1.0555740594863892, bias : 0.06105171516537666  \n",
      "Steps : 3142 , Loss : 0.783112108707428, Weights 1.0555812120437622, bias : 0.061045341193675995  \n",
      "Steps : 3143 , Loss : 0.7831120491027832, Weights 1.0555883646011353, bias : 0.06103897839784622  \n",
      "Steps : 3144 , Loss : 0.7831119298934937, Weights 1.0555955171585083, bias : 0.061032626777887344  \n",
      "Steps : 3145 , Loss : 0.7831118702888489, Weights 1.0556026697158813, bias : 0.06102628633379936  \n",
      "Steps : 3146 , Loss : 0.7831117510795593, Weights 1.0556098222732544, bias : 0.061019957065582275  \n",
      "Steps : 3147 , Loss : 0.7831117510795593, Weights 1.0556169748306274, bias : 0.061013638973236084  \n",
      "Steps : 3148 , Loss : 0.7831116318702698, Weights 1.055624008178711, bias : 0.06100733205676079  \n",
      "Steps : 3149 , Loss : 0.7831113934516907, Weights 1.0556310415267944, bias : 0.06100103631615639  \n",
      "Steps : 3150 , Loss : 0.7831113934516907, Weights 1.055638074874878, bias : 0.06099475175142288  \n",
      "Steps : 3151 , Loss : 0.7831112742424011, Weights 1.0556451082229614, bias : 0.06098847836256027  \n",
      "Steps : 3152 , Loss : 0.7831112742424011, Weights 1.055652141571045, bias : 0.060982219874858856  \n",
      "Steps : 3153 , Loss : 0.7831110954284668, Weights 1.0556591749191284, bias : 0.060975972563028336  \n",
      "Steps : 3154 , Loss : 0.7831109762191772, Weights 1.055666208267212, bias : 0.06096973642706871  \n",
      "Steps : 3155 , Loss : 0.7831108570098877, Weights 1.0556732416152954, bias : 0.06096351146697998  \n",
      "Steps : 3156 , Loss : 0.7831109762191772, Weights 1.055680274963379, bias : 0.060957297682762146  \n",
      "Steps : 3157 , Loss : 0.7831107378005981, Weights 1.0556873083114624, bias : 0.06095109507441521  \n",
      "Steps : 3158 , Loss : 0.7831106781959534, Weights 1.0556942224502563, bias : 0.06094490364193916  \n",
      "Steps : 3159 , Loss : 0.7831106781959534, Weights 1.0557011365890503, bias : 0.060938723385334015  \n",
      "Steps : 3160 , Loss : 0.783110499382019, Weights 1.0557080507278442, bias : 0.06093255430459976  \n",
      "Steps : 3161 , Loss : 0.7831104397773743, Weights 1.0557149648666382, bias : 0.060926396399736404  \n",
      "Steps : 3162 , Loss : 0.7831103801727295, Weights 1.0557218790054321, bias : 0.06092024967074394  \n",
      "Steps : 3163 , Loss : 0.7831102609634399, Weights 1.055728793144226, bias : 0.060914114117622375  \n",
      "Steps : 3164 , Loss : 0.7831101417541504, Weights 1.05573570728302, bias : 0.060907989740371704  \n",
      "Steps : 3165 , Loss : 0.7831100225448608, Weights 1.055742621421814, bias : 0.06090187653899193  \n",
      "Steps : 3166 , Loss : 0.7831100225448608, Weights 1.055749535560608, bias : 0.06089577451348305  \n",
      "Steps : 3167 , Loss : 0.7831099033355713, Weights 1.0557564496994019, bias : 0.06088968366384506  \n",
      "Steps : 3168 , Loss : 0.7831099033355713, Weights 1.0557632446289062, bias : 0.06088360399007797  \n",
      "Steps : 3169 , Loss : 0.783109724521637, Weights 1.0557700395584106, bias : 0.06087753549218178  \n",
      "Steps : 3170 , Loss : 0.7831096649169922, Weights 1.055776834487915, bias : 0.06087147817015648  \n",
      "Steps : 3171 , Loss : 0.7831096053123474, Weights 1.0557836294174194, bias : 0.060865432024002075  \n",
      "Steps : 3172 , Loss : 0.7831094264984131, Weights 1.0557904243469238, bias : 0.06085939705371857  \n",
      "Steps : 3173 , Loss : 0.7831093668937683, Weights 1.0557972192764282, bias : 0.060853373259305954  \n",
      "Steps : 3174 , Loss : 0.7831094264984131, Weights 1.0558040142059326, bias : 0.060847360640764236  \n",
      "Steps : 3175 , Loss : 0.783109188079834, Weights 1.055810809135437, bias : 0.060841359198093414  \n",
      "Steps : 3176 , Loss : 0.7831091284751892, Weights 1.0558176040649414, bias : 0.06083536520600319  \n",
      "Steps : 3177 , Loss : 0.7831091284751892, Weights 1.0558243989944458, bias : 0.06082938238978386  \n",
      "Steps : 3178 , Loss : 0.7831089496612549, Weights 1.0558310747146606, bias : 0.060823410749435425  \n",
      "Steps : 3179 , Loss : 0.7831089496612549, Weights 1.0558377504348755, bias : 0.060817450284957886  \n",
      "Steps : 3180 , Loss : 0.7831088304519653, Weights 1.0558444261550903, bias : 0.06081150099635124  \n",
      "Steps : 3181 , Loss : 0.7831087708473206, Weights 1.0558511018753052, bias : 0.060805562883615494  \n",
      "Steps : 3182 , Loss : 0.7831087708473206, Weights 1.05585777759552, bias : 0.06079963594675064  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3183 , Loss : 0.7831085920333862, Weights 1.0558644533157349, bias : 0.06079372018575668  \n",
      "Steps : 3184 , Loss : 0.7831085324287415, Weights 1.0558711290359497, bias : 0.06078781560063362  \n",
      "Steps : 3185 , Loss : 0.7831085324287415, Weights 1.0558778047561646, bias : 0.060781922191381454  \n",
      "Steps : 3186 , Loss : 0.7831083536148071, Weights 1.0558844804763794, bias : 0.060776036232709885  \n",
      "Steps : 3187 , Loss : 0.7831083536148071, Weights 1.0558911561965942, bias : 0.06077016144990921  \n",
      "Steps : 3188 , Loss : 0.7831082344055176, Weights 1.0558977127075195, bias : 0.06076429784297943  \n",
      "Steps : 3189 , Loss : 0.7831082344055176, Weights 1.0559042692184448, bias : 0.06075844541192055  \n",
      "Steps : 3190 , Loss : 0.7831080555915833, Weights 1.0559108257293701, bias : 0.06075260415673256  \n",
      "Steps : 3191 , Loss : 0.7831079363822937, Weights 1.0559173822402954, bias : 0.060746774077415466  \n",
      "Steps : 3192 , Loss : 0.7831079363822937, Weights 1.0559239387512207, bias : 0.06074095517396927  \n",
      "Steps : 3193 , Loss : 0.7831078171730042, Weights 1.055930495262146, bias : 0.06073514744639397  \n",
      "Steps : 3194 , Loss : 0.7831077575683594, Weights 1.0559370517730713, bias : 0.06072934716939926  \n",
      "Steps : 3195 , Loss : 0.7831077575683594, Weights 1.0559436082839966, bias : 0.06072355806827545  \n",
      "Steps : 3196 , Loss : 0.783107578754425, Weights 1.0559501647949219, bias : 0.06071778014302254  \n",
      "Steps : 3197 , Loss : 0.7831075191497803, Weights 1.0559567213058472, bias : 0.06071201339364052  \n",
      "Steps : 3198 , Loss : 0.7831074595451355, Weights 1.0559632778167725, bias : 0.060706257820129395  \n",
      "Steps : 3199 , Loss : 0.7831073999404907, Weights 1.0559697151184082, bias : 0.06070050969719887  \n",
      "Steps : 3200 , Loss : 0.7831072807312012, Weights 1.055976152420044, bias : 0.060694772750139236  \n",
      "Steps : 3201 , Loss : 0.7831072211265564, Weights 1.0559825897216797, bias : 0.0606890469789505  \n",
      "Steps : 3202 , Loss : 0.7831070423126221, Weights 1.0559890270233154, bias : 0.06068333238363266  \n",
      "Steps : 3203 , Loss : 0.7831069827079773, Weights 1.0559954643249512, bias : 0.060677628964185715  \n",
      "Steps : 3204 , Loss : 0.7831069827079773, Weights 1.056001901626587, bias : 0.060671932995319366  \n",
      "Steps : 3205 , Loss : 0.7831069827079773, Weights 1.0560083389282227, bias : 0.060666248202323914  \n",
      "Steps : 3206 , Loss : 0.7831068634986877, Weights 1.0560147762298584, bias : 0.060660574585199356  \n",
      "Steps : 3207 , Loss : 0.7831067442893982, Weights 1.0560212135314941, bias : 0.060654912143945694  \n",
      "Steps : 3208 , Loss : 0.7831066846847534, Weights 1.0560276508331299, bias : 0.06064926087856293  \n",
      "Steps : 3209 , Loss : 0.7831066846847534, Weights 1.056033968925476, bias : 0.06064361706376076  \n",
      "Steps : 3210 , Loss : 0.7831065654754639, Weights 1.0560402870178223, bias : 0.06063798442482948  \n",
      "Steps : 3211 , Loss : 0.7831065654754639, Weights 1.0560466051101685, bias : 0.060632362961769104  \n",
      "Steps : 3212 , Loss : 0.7831064462661743, Weights 1.0560529232025146, bias : 0.06062675267457962  \n",
      "Steps : 3213 , Loss : 0.7831063866615295, Weights 1.0560592412948608, bias : 0.060621149837970734  \n",
      "Steps : 3214 , Loss : 0.7831062078475952, Weights 1.056065559387207, bias : 0.06061555817723274  \n",
      "Steps : 3215 , Loss : 0.7831062078475952, Weights 1.0560718774795532, bias : 0.060609977692365646  \n",
      "Steps : 3216 , Loss : 0.7831060886383057, Weights 1.0560781955718994, bias : 0.060604408383369446  \n",
      "Steps : 3217 , Loss : 0.7831060290336609, Weights 1.0560845136642456, bias : 0.06059884652495384  \n",
      "Steps : 3218 , Loss : 0.7831060290336609, Weights 1.0560908317565918, bias : 0.060593295842409134  \n",
      "Steps : 3219 , Loss : 0.7831059098243713, Weights 1.056097149848938, bias : 0.06058775633573532  \n",
      "Steps : 3220 , Loss : 0.7831059098243713, Weights 1.0561033487319946, bias : 0.060582224279642105  \n",
      "Steps : 3221 , Loss : 0.7831057906150818, Weights 1.0561095476150513, bias : 0.060576703399419785  \n",
      "Steps : 3222 , Loss : 0.7831056118011475, Weights 1.056115746498108, bias : 0.06057119369506836  \n",
      "Steps : 3223 , Loss : 0.7831056714057922, Weights 1.0561219453811646, bias : 0.06056569144129753  \n",
      "Steps : 3224 , Loss : 0.7831056118011475, Weights 1.0561281442642212, bias : 0.0605602003633976  \n",
      "Steps : 3225 , Loss : 0.7831054925918579, Weights 1.0561343431472778, bias : 0.06055472046136856  \n",
      "Steps : 3226 , Loss : 0.7831053733825684, Weights 1.0561405420303345, bias : 0.06054925173521042  \n",
      "Steps : 3227 , Loss : 0.7831053733825684, Weights 1.0561467409133911, bias : 0.060543790459632874  \n",
      "Steps : 3228 , Loss : 0.7831053137779236, Weights 1.0561529397964478, bias : 0.060538340359926224  \n",
      "Steps : 3229 , Loss : 0.7831051349639893, Weights 1.0561591386795044, bias : 0.06053290143609047  \n",
      "Steps : 3230 , Loss : 0.7831052541732788, Weights 1.056165337562561, bias : 0.06052746996283531  \n",
      "Steps : 3231 , Loss : 0.7831051349639893, Weights 1.0561714172363281, bias : 0.06052204966545105  \n",
      "Steps : 3232 , Loss : 0.7831050157546997, Weights 1.0561774969100952, bias : 0.060516636818647385  \n",
      "Steps : 3233 , Loss : 0.7831050157546997, Weights 1.0561835765838623, bias : 0.060511235147714615  \n",
      "Steps : 3234 , Loss : 0.7831048369407654, Weights 1.0561896562576294, bias : 0.06050584465265274  \n",
      "Steps : 3235 , Loss : 0.7831048369407654, Weights 1.0561957359313965, bias : 0.06050046160817146  \n",
      "Steps : 3236 , Loss : 0.7831048369407654, Weights 1.0562018156051636, bias : 0.06049508973956108  \n",
      "Steps : 3237 , Loss : 0.7831047177314758, Weights 1.0562078952789307, bias : 0.060489729046821594  \n",
      "Steps : 3238 , Loss : 0.7831047177314758, Weights 1.0562139749526978, bias : 0.060484375804662704  \n",
      "Steps : 3239 , Loss : 0.783104658126831, Weights 1.0562200546264648, bias : 0.06047903373837471  \n",
      "Steps : 3240 , Loss : 0.7831045389175415, Weights 1.056226134300232, bias : 0.06047369912266731  \n",
      "Steps : 3241 , Loss : 0.7831044793128967, Weights 1.056232213973999, bias : 0.06046837568283081  \n",
      "Steps : 3242 , Loss : 0.783104419708252, Weights 1.0562381744384766, bias : 0.060463063418865204  \n",
      "Steps : 3243 , Loss : 0.7831042408943176, Weights 1.056244134902954, bias : 0.060457758605480194  \n",
      "Steps : 3244 , Loss : 0.7831043004989624, Weights 1.0562500953674316, bias : 0.06045246496796608  \n",
      "Steps : 3245 , Loss : 0.7831042408943176, Weights 1.0562560558319092, bias : 0.06044717878103256  \n",
      "Steps : 3246 , Loss : 0.7831041812896729, Weights 1.0562620162963867, bias : 0.06044190376996994  \n",
      "Steps : 3247 , Loss : 0.7831041216850281, Weights 1.0562679767608643, bias : 0.060436639934778214  \n",
      "Steps : 3248 , Loss : 0.7831040024757385, Weights 1.0562739372253418, bias : 0.060431383550167084  \n",
      "Steps : 3249 , Loss : 0.7831039428710938, Weights 1.0562798976898193, bias : 0.06042613834142685  \n",
      "Steps : 3250 , Loss : 0.7831039428710938, Weights 1.0562858581542969, bias : 0.06042090058326721  \n",
      "Steps : 3251 , Loss : 0.7831037640571594, Weights 1.0562918186187744, bias : 0.06041567400097847  \n",
      "Steps : 3252 , Loss : 0.7831037044525146, Weights 1.056297779083252, bias : 0.060410454869270325  \n",
      "Steps : 3253 , Loss : 0.7831037044525146, Weights 1.0563037395477295, bias : 0.060405246913433075  \n",
      "Steps : 3254 , Loss : 0.7831036448478699, Weights 1.0563095808029175, bias : 0.06040005013346672  \n",
      "Steps : 3255 , Loss : 0.7831036448478699, Weights 1.0563154220581055, bias : 0.06039486080408096  \n",
      "Steps : 3256 , Loss : 0.7831034660339355, Weights 1.0563212633132935, bias : 0.0603896826505661  \n",
      "Steps : 3257 , Loss : 0.7831034660339355, Weights 1.0563271045684814, bias : 0.060384511947631836  \n",
      "Steps : 3258 , Loss : 0.7831034660339355, Weights 1.0563329458236694, bias : 0.060379352420568466  \n",
      "Steps : 3259 , Loss : 0.783103346824646, Weights 1.0563387870788574, bias : 0.06037420034408569  \n",
      "Steps : 3260 , Loss : 0.7831031680107117, Weights 1.0563446283340454, bias : 0.060369059443473816  \n",
      "Steps : 3261 , Loss : 0.7831031680107117, Weights 1.0563504695892334, bias : 0.060363925993442535  \n",
      "Steps : 3262 , Loss : 0.7831031680107117, Weights 1.0563563108444214, bias : 0.06035880371928215  \n",
      "Steps : 3263 , Loss : 0.7831031084060669, Weights 1.0563621520996094, bias : 0.06035368889570236  \n",
      "Steps : 3264 , Loss : 0.7831030488014221, Weights 1.0563679933547974, bias : 0.06034858524799347  \n",
      "Steps : 3265 , Loss : 0.7831029295921326, Weights 1.0563738346099854, bias : 0.06034348905086517  \n",
      "Steps : 3266 , Loss : 0.7831028699874878, Weights 1.0563795566558838, bias : 0.06033840402960777  \n",
      "Steps : 3267 , Loss : 0.7831028699874878, Weights 1.0563852787017822, bias : 0.06033332645893097  \n",
      "Steps : 3268 , Loss : 0.7831028699874878, Weights 1.0563910007476807, bias : 0.06032826006412506  \n",
      "Steps : 3269 , Loss : 0.783102810382843, Weights 1.056396722793579, bias : 0.06032320111989975  \n",
      "Steps : 3270 , Loss : 0.7831027507781982, Weights 1.0564024448394775, bias : 0.060318153351545334  \n",
      "Steps : 3271 , Loss : 0.7831025719642639, Weights 1.056408166885376, bias : 0.060313113033771515  \n",
      "Steps : 3272 , Loss : 0.7831025719642639, Weights 1.0564138889312744, bias : 0.06030808389186859  \n",
      "Steps : 3273 , Loss : 0.7831025123596191, Weights 1.0564196109771729, bias : 0.060303062200546265  \n",
      "Steps : 3274 , Loss : 0.7831025719642639, Weights 1.0564253330230713, bias : 0.060298047959804535  \n",
      "Steps : 3275 , Loss : 0.7831023335456848, Weights 1.0564310550689697, bias : 0.0602930448949337  \n",
      "Steps : 3276 , Loss : 0.78310227394104, Weights 1.0564367771148682, bias : 0.06028804928064346  \n",
      "Steps : 3277 , Loss : 0.78310227394104, Weights 1.0564424991607666, bias : 0.06028306484222412  \n",
      "Steps : 3278 , Loss : 0.7831022143363953, Weights 1.0564481019973755, bias : 0.060278087854385376  \n",
      "Steps : 3279 , Loss : 0.7831020951271057, Weights 1.0564537048339844, bias : 0.060273122042417526  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3280 , Loss : 0.7831020951271057, Weights 1.0564593076705933, bias : 0.06026816368103027  \n",
      "Steps : 3281 , Loss : 0.7831020951271057, Weights 1.0564649105072021, bias : 0.060263216495513916  \n",
      "Steps : 3282 , Loss : 0.7831019759178162, Weights 1.056470513343811, bias : 0.060258276760578156  \n",
      "Steps : 3283 , Loss : 0.7831018567085266, Weights 1.05647611618042, bias : 0.06025334447622299  \n",
      "Steps : 3284 , Loss : 0.7831019759178162, Weights 1.0564817190170288, bias : 0.060248423367738724  \n",
      "Steps : 3285 , Loss : 0.7831017971038818, Weights 1.0564873218536377, bias : 0.06024350970983505  \n",
      "Steps : 3286 , Loss : 0.7831017971038818, Weights 1.0564929246902466, bias : 0.06023860722780228  \n",
      "Steps : 3287 , Loss : 0.7831017374992371, Weights 1.0564985275268555, bias : 0.0602337121963501  \n",
      "Steps : 3288 , Loss : 0.7831016778945923, Weights 1.0565041303634644, bias : 0.060228824615478516  \n",
      "Steps : 3289 , Loss : 0.7831016778945923, Weights 1.0565097332000732, bias : 0.06022394821047783  \n",
      "Steps : 3290 , Loss : 0.783101499080658, Weights 1.0565152168273926, bias : 0.06021907925605774  \n",
      "Steps : 3291 , Loss : 0.783101499080658, Weights 1.056520700454712, bias : 0.060214217752218246  \n",
      "Steps : 3292 , Loss : 0.783101499080658, Weights 1.0565261840820312, bias : 0.06020936742424965  \n",
      "Steps : 3293 , Loss : 0.7831014394760132, Weights 1.0565316677093506, bias : 0.06020452454686165  \n",
      "Steps : 3294 , Loss : 0.7831013202667236, Weights 1.05653715133667, bias : 0.06019969284534454  \n",
      "Steps : 3295 , Loss : 0.7831012606620789, Weights 1.0565426349639893, bias : 0.060194868594408035  \n",
      "Steps : 3296 , Loss : 0.7831012010574341, Weights 1.0565481185913086, bias : 0.060190051794052124  \n",
      "Steps : 3297 , Loss : 0.7831012010574341, Weights 1.056553602218628, bias : 0.06018524616956711  \n",
      "Steps : 3298 , Loss : 0.7831012010574341, Weights 1.0565590858459473, bias : 0.06018044799566269  \n",
      "Steps : 3299 , Loss : 0.7831010222434998, Weights 1.0565645694732666, bias : 0.06017565727233887  \n",
      "Steps : 3300 , Loss : 0.7831010222434998, Weights 1.056570053100586, bias : 0.06017087772488594  \n",
      "Steps : 3301 , Loss : 0.7831010222434998, Weights 1.0565755367279053, bias : 0.06016610562801361  \n",
      "Steps : 3302 , Loss : 0.7831009030342102, Weights 1.056580901145935, bias : 0.06016134098172188  \n",
      "Steps : 3303 , Loss : 0.7831009030342102, Weights 1.0565862655639648, bias : 0.06015658751130104  \n",
      "Steps : 3304 , Loss : 0.7831008434295654, Weights 1.0565916299819946, bias : 0.0601518414914608  \n",
      "Steps : 3305 , Loss : 0.7831008434295654, Weights 1.0565969944000244, bias : 0.06014710292220116  \n",
      "Steps : 3306 , Loss : 0.7831007242202759, Weights 1.0566023588180542, bias : 0.06014237552881241  \n",
      "Steps : 3307 , Loss : 0.7831007242202759, Weights 1.056607723236084, bias : 0.06013765558600426  \n",
      "Steps : 3308 , Loss : 0.7831006050109863, Weights 1.0566130876541138, bias : 0.0601329430937767  \n",
      "Steps : 3309 , Loss : 0.7831006050109863, Weights 1.0566184520721436, bias : 0.060128241777420044  \n",
      "Steps : 3310 , Loss : 0.7831004858016968, Weights 1.0566238164901733, bias : 0.06012354791164398  \n",
      "Steps : 3311 , Loss : 0.783100426197052, Weights 1.0566291809082031, bias : 0.06011886149644852  \n",
      "Steps : 3312 , Loss : 0.7831003665924072, Weights 1.056634545326233, bias : 0.06011418253183365  \n",
      "Steps : 3313 , Loss : 0.783100426197052, Weights 1.0566399097442627, bias : 0.060109514743089676  \n",
      "Steps : 3314 , Loss : 0.7831003069877625, Weights 1.0566452741622925, bias : 0.0601048544049263  \n",
      "Steps : 3315 , Loss : 0.7831003665924072, Weights 1.0566505193710327, bias : 0.06010020151734352  \n",
      "Steps : 3316 , Loss : 0.7831001877784729, Weights 1.056655764579773, bias : 0.06009555980563164  \n",
      "Steps : 3317 , Loss : 0.7831001281738281, Weights 1.0566610097885132, bias : 0.06009092554450035  \n",
      "Steps : 3318 , Loss : 0.7831000685691833, Weights 1.0566662549972534, bias : 0.06008629873394966  \n",
      "Steps : 3319 , Loss : 0.7830999493598938, Weights 1.0566715002059937, bias : 0.06008167937397957  \n",
      "Steps : 3320 , Loss : 0.7831001281738281, Weights 1.0566767454147339, bias : 0.06007707118988037  \n",
      "Steps : 3321 , Loss : 0.7830999493598938, Weights 1.0566819906234741, bias : 0.06007247045636177  \n",
      "Steps : 3322 , Loss : 0.783099889755249, Weights 1.0566872358322144, bias : 0.06006787717342377  \n",
      "Steps : 3323 , Loss : 0.7830998301506042, Weights 1.0566924810409546, bias : 0.06006329134106636  \n",
      "Steps : 3324 , Loss : 0.7830998301506042, Weights 1.0566977262496948, bias : 0.06005871668457985  \n",
      "Steps : 3325 , Loss : 0.7830998301506042, Weights 1.056702971458435, bias : 0.060054149478673935  \n",
      "Steps : 3326 , Loss : 0.7830996513366699, Weights 1.0567082166671753, bias : 0.06004958972334862  \n",
      "Steps : 3327 , Loss : 0.7830996513366699, Weights 1.0567134618759155, bias : 0.0600450374186039  \n",
      "Steps : 3328 , Loss : 0.7830996513366699, Weights 1.0567185878753662, bias : 0.060040492564439774  \n",
      "Steps : 3329 , Loss : 0.7830995917320251, Weights 1.056723713874817, bias : 0.060035958886146545  \n",
      "Steps : 3330 , Loss : 0.7830995321273804, Weights 1.0567288398742676, bias : 0.060031432658433914  \n",
      "Steps : 3331 , Loss : 0.7830994129180908, Weights 1.0567339658737183, bias : 0.06002691388130188  \n",
      "Steps : 3332 , Loss : 0.7830994129180908, Weights 1.056739091873169, bias : 0.06002240255475044  \n",
      "Steps : 3333 , Loss : 0.7830995321273804, Weights 1.0567442178726196, bias : 0.0600179024040699  \n",
      "Steps : 3334 , Loss : 0.783099353313446, Weights 1.0567493438720703, bias : 0.060013409703969955  \n",
      "Steps : 3335 , Loss : 0.7830992937088013, Weights 1.056754469871521, bias : 0.06000892445445061  \n",
      "Steps : 3336 , Loss : 0.7830992937088013, Weights 1.0567595958709717, bias : 0.060004446655511856  \n",
      "Steps : 3337 , Loss : 0.7830992341041565, Weights 1.0567647218704224, bias : 0.0599999763071537  \n",
      "Steps : 3338 , Loss : 0.7830992341041565, Weights 1.056769847869873, bias : 0.059995513409376144  \n",
      "Steps : 3339 , Loss : 0.7830990552902222, Weights 1.0567749738693237, bias : 0.05999106168746948  \n",
      "Steps : 3340 , Loss : 0.7830990552902222, Weights 1.0567800998687744, bias : 0.05998661741614342  \n",
      "Steps : 3341 , Loss : 0.7830990552902222, Weights 1.0567851066589355, bias : 0.05998218059539795  \n",
      "Steps : 3342 , Loss : 0.7830989360809326, Weights 1.0567901134490967, bias : 0.05997775122523308  \n",
      "Steps : 3343 , Loss : 0.7830989956855774, Weights 1.0567951202392578, bias : 0.059973329305648804  \n",
      "Steps : 3344 , Loss : 0.7830987572669983, Weights 1.056800127029419, bias : 0.059968914836645126  \n",
      "Steps : 3345 , Loss : 0.7830988168716431, Weights 1.05680513381958, bias : 0.059964511543512344  \n",
      "Steps : 3346 , Loss : 0.7830987572669983, Weights 1.0568101406097412, bias : 0.05996011570096016  \n",
      "Steps : 3347 , Loss : 0.7830987572669983, Weights 1.0568151473999023, bias : 0.05995572730898857  \n",
      "Steps : 3348 , Loss : 0.7830987572669983, Weights 1.0568201541900635, bias : 0.05995134636759758  \n",
      "Steps : 3349 , Loss : 0.7830986976623535, Weights 1.0568251609802246, bias : 0.059946972876787186  \n",
      "Steps : 3350 , Loss : 0.783098578453064, Weights 1.0568301677703857, bias : 0.05994260683655739  \n",
      "Steps : 3351 , Loss : 0.783098578453064, Weights 1.0568351745605469, bias : 0.05993824824690819  \n",
      "Steps : 3352 , Loss : 0.783098578453064, Weights 1.056840181350708, bias : 0.05993390083312988  \n",
      "Steps : 3353 , Loss : 0.7830985188484192, Weights 1.0568451881408691, bias : 0.059929560869932175  \n",
      "Steps : 3354 , Loss : 0.7830984592437744, Weights 1.0568501949310303, bias : 0.059925228357315063  \n",
      "Steps : 3355 , Loss : 0.7830984592437744, Weights 1.0568550825119019, bias : 0.05992090329527855  \n",
      "Steps : 3356 , Loss : 0.7830983996391296, Weights 1.0568599700927734, bias : 0.05991658568382263  \n",
      "Steps : 3357 , Loss : 0.7830982804298401, Weights 1.056864857673645, bias : 0.05991227552294731  \n",
      "Steps : 3358 , Loss : 0.7830982804298401, Weights 1.0568697452545166, bias : 0.05990797281265259  \n",
      "Steps : 3359 , Loss : 0.7830982208251953, Weights 1.0568746328353882, bias : 0.05990367755293846  \n",
      "Steps : 3360 , Loss : 0.7830981612205505, Weights 1.0568795204162598, bias : 0.05989938974380493  \n",
      "Steps : 3361 , Loss : 0.7830981612205505, Weights 1.0568844079971313, bias : 0.0598951131105423  \n",
      "Steps : 3362 , Loss : 0.7830981612205505, Weights 1.056889295578003, bias : 0.05989084392786026  \n",
      "Steps : 3363 , Loss : 0.7830981612205505, Weights 1.0568941831588745, bias : 0.05988658219575882  \n",
      "Steps : 3364 , Loss : 0.783098042011261, Weights 1.056899070739746, bias : 0.059882327914237976  \n",
      "Steps : 3365 , Loss : 0.7830979824066162, Weights 1.0569039583206177, bias : 0.05987808108329773  \n",
      "Steps : 3366 , Loss : 0.7830979228019714, Weights 1.0569088459014893, bias : 0.05987384170293808  \n",
      "Steps : 3367 , Loss : 0.7830978631973267, Weights 1.0569137334823608, bias : 0.05986960977315903  \n",
      "Steps : 3368 , Loss : 0.7830979228019714, Weights 1.0569186210632324, bias : 0.05986538529396057  \n",
      "Steps : 3369 , Loss : 0.7830978631973267, Weights 1.0569233894348145, bias : 0.05986116826534271  \n",
      "Steps : 3370 , Loss : 0.7830977439880371, Weights 1.0569281578063965, bias : 0.05985695868730545  \n",
      "Steps : 3371 , Loss : 0.7830976843833923, Weights 1.0569329261779785, bias : 0.059852756559848785  \n",
      "Steps : 3372 , Loss : 0.7830976843833923, Weights 1.0569376945495605, bias : 0.05984856188297272  \n",
      "Steps : 3373 , Loss : 0.7830976843833923, Weights 1.0569424629211426, bias : 0.059844374656677246  \n",
      "Steps : 3374 , Loss : 0.7830976843833923, Weights 1.0569472312927246, bias : 0.05984019488096237  \n",
      "Steps : 3375 , Loss : 0.7830976843833923, Weights 1.0569519996643066, bias : 0.059836022555828094  \n",
      "Steps : 3376 , Loss : 0.783097505569458, Weights 1.0569567680358887, bias : 0.05983186140656471  \n",
      "Steps : 3377 , Loss : 0.783097505569458, Weights 1.0569615364074707, bias : 0.05982770770788193  \n",
      "Steps : 3378 , Loss : 0.783097505569458, Weights 1.0569663047790527, bias : 0.05982356145977974  \n",
      "Steps : 3379 , Loss : 0.7830973863601685, Weights 1.0569710731506348, bias : 0.05981942266225815  \n",
      "Steps : 3380 , Loss : 0.7830973863601685, Weights 1.0569758415222168, bias : 0.059815291315317154  \n",
      "Steps : 3381 , Loss : 0.7830973863601685, Weights 1.0569806098937988, bias : 0.05981116741895676  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3382 , Loss : 0.7830973267555237, Weights 1.0569853782653809, bias : 0.059807050973176956  \n",
      "Steps : 3383 , Loss : 0.7830972075462341, Weights 1.0569900274276733, bias : 0.05980294197797775  \n",
      "Steps : 3384 , Loss : 0.7830972075462341, Weights 1.0569946765899658, bias : 0.059798840433359146  \n",
      "Steps : 3385 , Loss : 0.7830972075462341, Weights 1.0569993257522583, bias : 0.059794746339321136  \n",
      "Steps : 3386 , Loss : 0.7830970883369446, Weights 1.0570039749145508, bias : 0.059790659695863724  \n",
      "Steps : 3387 , Loss : 0.7830970883369446, Weights 1.0570086240768433, bias : 0.05978658050298691  \n",
      "Steps : 3388 , Loss : 0.7830970883369446, Weights 1.0570132732391357, bias : 0.05978250876069069  \n",
      "Steps : 3389 , Loss : 0.7830970883369446, Weights 1.0570179224014282, bias : 0.05977844446897507  \n",
      "Steps : 3390 , Loss : 0.7830970287322998, Weights 1.0570225715637207, bias : 0.05977438762784004  \n",
      "Steps : 3391 , Loss : 0.7830970287322998, Weights 1.0570272207260132, bias : 0.059770338237285614  \n",
      "Steps : 3392 , Loss : 0.7830969095230103, Weights 1.0570318698883057, bias : 0.05976629629731178  \n",
      "Steps : 3393 , Loss : 0.7830969095230103, Weights 1.0570365190505981, bias : 0.05976226180791855  \n",
      "Steps : 3394 , Loss : 0.7830969095230103, Weights 1.0570411682128906, bias : 0.05975823476910591  \n",
      "Steps : 3395 , Loss : 0.7830968499183655, Weights 1.057045817375183, bias : 0.05975421145558357  \n",
      "Steps : 3396 , Loss : 0.7830966711044312, Weights 1.0570504665374756, bias : 0.05975019559264183  \n",
      "Steps : 3397 , Loss : 0.7830967903137207, Weights 1.057055115699768, bias : 0.059746187180280685  \n",
      "Steps : 3398 , Loss : 0.7830966711044312, Weights 1.057059645652771, bias : 0.05974218621850014  \n",
      "Steps : 3399 , Loss : 0.7830966711044312, Weights 1.057064175605774, bias : 0.059738192707300186  \n",
      "Steps : 3400 , Loss : 0.7830966711044312, Weights 1.0570687055587769, bias : 0.05973420664668083  \n",
      "Steps : 3401 , Loss : 0.7830966114997864, Weights 1.0570732355117798, bias : 0.059730228036642075  \n",
      "Steps : 3402 , Loss : 0.7830966114997864, Weights 1.0570777654647827, bias : 0.059726256877183914  \n",
      "Steps : 3403 , Loss : 0.7830964922904968, Weights 1.0570822954177856, bias : 0.05972229316830635  \n",
      "Steps : 3404 , Loss : 0.7830965518951416, Weights 1.0570868253707886, bias : 0.059718336910009384  \n",
      "Steps : 3405 , Loss : 0.7830964922904968, Weights 1.0570913553237915, bias : 0.059714388102293015  \n",
      "Steps : 3406 , Loss : 0.7830963134765625, Weights 1.0570958852767944, bias : 0.05971044674515724  \n",
      "Steps : 3407 , Loss : 0.7830963134765625, Weights 1.0571004152297974, bias : 0.059706512838602066  \n",
      "Steps : 3408 , Loss : 0.7830964922904968, Weights 1.0571049451828003, bias : 0.05970258638262749  \n",
      "Steps : 3409 , Loss : 0.7830963134765625, Weights 1.0571094751358032, bias : 0.059698667377233505  \n",
      "Steps : 3410 , Loss : 0.7830963134765625, Weights 1.0571140050888062, bias : 0.05969475582242012  \n",
      "Steps : 3411 , Loss : 0.7830962538719177, Weights 1.057118535041809, bias : 0.059690847992897034  \n",
      "Steps : 3412 , Loss : 0.7830961346626282, Weights 1.057123064994812, bias : 0.059686947613954544  \n",
      "Steps : 3413 , Loss : 0.7830961346626282, Weights 1.0571274757385254, bias : 0.05968305468559265  \n",
      "Steps : 3414 , Loss : 0.7830961346626282, Weights 1.0571318864822388, bias : 0.059679169207811356  \n",
      "Steps : 3415 , Loss : 0.7830961346626282, Weights 1.0571362972259521, bias : 0.05967529118061066  \n",
      "Steps : 3416 , Loss : 0.7830960154533386, Weights 1.0571407079696655, bias : 0.059671420603990555  \n",
      "Steps : 3417 , Loss : 0.7830960154533386, Weights 1.057145118713379, bias : 0.05966755747795105  \n",
      "Steps : 3418 , Loss : 0.7830960154533386, Weights 1.0571495294570923, bias : 0.05966370180249214  \n",
      "Steps : 3419 , Loss : 0.7830958366394043, Weights 1.0571539402008057, bias : 0.05965985357761383  \n",
      "Steps : 3420 , Loss : 0.7830958366394043, Weights 1.057158350944519, bias : 0.05965600907802582  \n",
      "Steps : 3421 , Loss : 0.7830959558486938, Weights 1.0571627616882324, bias : 0.0596521720290184  \n",
      "Steps : 3422 , Loss : 0.7830958366394043, Weights 1.0571671724319458, bias : 0.05964834243059158  \n",
      "Steps : 3423 , Loss : 0.7830957770347595, Weights 1.0571715831756592, bias : 0.05964452028274536  \n",
      "Steps : 3424 , Loss : 0.7830957174301147, Weights 1.0571759939193726, bias : 0.059640705585479736  \n",
      "Steps : 3425 , Loss : 0.7830957174301147, Weights 1.057180404663086, bias : 0.05963689833879471  \n",
      "Steps : 3426 , Loss : 0.7830957174301147, Weights 1.0571848154067993, bias : 0.05963309854269028  \n",
      "Steps : 3427 , Loss : 0.7830957174301147, Weights 1.0571892261505127, bias : 0.05962930619716644  \n",
      "Steps : 3428 , Loss : 0.7830957174301147, Weights 1.057193636894226, bias : 0.05962551757693291  \n",
      "Steps : 3429 , Loss : 0.7830955982208252, Weights 1.05719792842865, bias : 0.05962173640727997  \n",
      "Steps : 3430 , Loss : 0.7830955386161804, Weights 1.0572022199630737, bias : 0.059617962688207626  \n",
      "Steps : 3431 , Loss : 0.7830954790115356, Weights 1.0572065114974976, bias : 0.05961419641971588  \n",
      "Steps : 3432 , Loss : 0.7830954194068909, Weights 1.0572108030319214, bias : 0.05961043760180473  \n",
      "Steps : 3433 , Loss : 0.7830954790115356, Weights 1.0572150945663452, bias : 0.05960668623447418  \n",
      "Steps : 3434 , Loss : 0.7830954194068909, Weights 1.057219386100769, bias : 0.05960293859243393  \n",
      "Steps : 3435 , Loss : 0.7830954194068909, Weights 1.0572236776351929, bias : 0.059599198400974274  \n",
      "Steps : 3436 , Loss : 0.7830954194068909, Weights 1.0572279691696167, bias : 0.059595465660095215  \n",
      "Steps : 3437 , Loss : 0.7830953001976013, Weights 1.0572322607040405, bias : 0.05959174036979675  \n",
      "Steps : 3438 , Loss : 0.7830953001976013, Weights 1.0572365522384644, bias : 0.05958802253007889  \n",
      "Steps : 3439 , Loss : 0.7830952405929565, Weights 1.0572408437728882, bias : 0.05958431214094162  \n",
      "Steps : 3440 , Loss : 0.7830952405929565, Weights 1.057245135307312, bias : 0.05958060547709465  \n",
      "Steps : 3441 , Loss : 0.7830952405929565, Weights 1.0572494268417358, bias : 0.05957690626382828  \n",
      "Steps : 3442 , Loss : 0.7830951809883118, Weights 1.0572537183761597, bias : 0.0595732145011425  \n",
      "Steps : 3443 , Loss : 0.783095121383667, Weights 1.0572580099105835, bias : 0.05956953018903732  \n",
      "Steps : 3444 , Loss : 0.7830951809883118, Weights 1.0572621822357178, bias : 0.05956585332751274  \n",
      "Steps : 3445 , Loss : 0.783095121383667, Weights 1.057266354560852, bias : 0.05956218019127846  \n",
      "Steps : 3446 , Loss : 0.783095121383667, Weights 1.0572705268859863, bias : 0.05955851450562477  \n",
      "Steps : 3447 , Loss : 0.7830950021743774, Weights 1.0572746992111206, bias : 0.05955485627055168  \n",
      "Steps : 3448 , Loss : 0.7830950021743774, Weights 1.0572788715362549, bias : 0.05955120548605919  \n",
      "Steps : 3449 , Loss : 0.7830950021743774, Weights 1.0572830438613892, bias : 0.059547558426856995  \n",
      "Steps : 3450 , Loss : 0.7830949425697327, Weights 1.0572872161865234, bias : 0.0595439188182354  \n",
      "Steps : 3451 , Loss : 0.7830949425697327, Weights 1.0572913885116577, bias : 0.0595402866601944  \n",
      "Steps : 3452 , Loss : 0.7830948829650879, Weights 1.057295560836792, bias : 0.059536661952733994  \n",
      "Steps : 3453 , Loss : 0.7830947637557983, Weights 1.0572997331619263, bias : 0.05953304469585419  \n",
      "Steps : 3454 , Loss : 0.7830947637557983, Weights 1.0573039054870605, bias : 0.05952943116426468  \n",
      "Steps : 3455 , Loss : 0.7830947637557983, Weights 1.0573080778121948, bias : 0.05952582508325577  \n",
      "Steps : 3456 , Loss : 0.7830947637557983, Weights 1.057312250137329, bias : 0.059522226452827454  \n",
      "Steps : 3457 , Loss : 0.7830946445465088, Weights 1.0573164224624634, bias : 0.059518635272979736  \n",
      "Steps : 3458 , Loss : 0.7830946445465088, Weights 1.0573205947875977, bias : 0.05951504781842232  \n",
      "Steps : 3459 , Loss : 0.7830946445465088, Weights 1.057324767112732, bias : 0.059511467814445496  \n",
      "Steps : 3460 , Loss : 0.7830946445465088, Weights 1.0573289394378662, bias : 0.05950789526104927  \n",
      "Steps : 3461 , Loss : 0.7830946445465088, Weights 1.057332992553711, bias : 0.05950433015823364  \n",
      "Steps : 3462 , Loss : 0.7830946445465088, Weights 1.0573370456695557, bias : 0.05950076878070831  \n",
      "Steps : 3463 , Loss : 0.7830944657325745, Weights 1.0573410987854004, bias : 0.05949721485376358  \n",
      "Steps : 3464 , Loss : 0.7830946445465088, Weights 1.0573451519012451, bias : 0.059493668377399445  \n",
      "Steps : 3465 , Loss : 0.7830944657325745, Weights 1.0573492050170898, bias : 0.059490129351615906  \n",
      "Steps : 3466 , Loss : 0.7830944657325745, Weights 1.0573532581329346, bias : 0.059486594051122665  \n",
      "Steps : 3467 , Loss : 0.7830944657325745, Weights 1.0573573112487793, bias : 0.05948306620121002  \n",
      "Steps : 3468 , Loss : 0.7830944061279297, Weights 1.057361364364624, bias : 0.059479545801877975  \n",
      "Steps : 3469 , Loss : 0.7830943465232849, Weights 1.0573654174804688, bias : 0.05947602912783623  \n",
      "Steps : 3470 , Loss : 0.7830943465232849, Weights 1.0573694705963135, bias : 0.059472519904375076  \n",
      "Steps : 3471 , Loss : 0.7830943465232849, Weights 1.0573735237121582, bias : 0.05946901813149452  \n",
      "Steps : 3472 , Loss : 0.7830943465232849, Weights 1.057377576828003, bias : 0.059465523809194565  \n",
      "Steps : 3473 , Loss : 0.7830943465232849, Weights 1.0573816299438477, bias : 0.059462033212184906  \n",
      "Steps : 3474 , Loss : 0.7830943465232849, Weights 1.0573856830596924, bias : 0.059458550065755844  \n",
      "Steps : 3475 , Loss : 0.7830943465232849, Weights 1.057389736175537, bias : 0.05945507436990738  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3476 , Loss : 0.7830941677093506, Weights 1.0573937892913818, bias : 0.05945160239934921  \n",
      "Steps : 3477 , Loss : 0.7830941677093506, Weights 1.0573978424072266, bias : 0.05944813787937164  \n",
      "Steps : 3478 , Loss : 0.7830941677093506, Weights 1.0574017763137817, bias : 0.05944468080997467  \n",
      "Steps : 3479 , Loss : 0.7830941081047058, Weights 1.057405710220337, bias : 0.059441227465867996  \n",
      "Steps : 3480 , Loss : 0.783094048500061, Weights 1.057409644126892, bias : 0.05943778157234192  \n",
      "Steps : 3481 , Loss : 0.783094048500061, Weights 1.0574135780334473, bias : 0.05943434312939644  \n",
      "Steps : 3482 , Loss : 0.783094048500061, Weights 1.0574175119400024, bias : 0.05943090841174126  \n",
      "Steps : 3483 , Loss : 0.783094048500061, Weights 1.0574214458465576, bias : 0.05942748114466667  \n",
      "Steps : 3484 , Loss : 0.7830939292907715, Weights 1.0574253797531128, bias : 0.059424061328172684  \n",
      "Steps : 3485 , Loss : 0.7830939292907715, Weights 1.057429313659668, bias : 0.05942064896225929  \n",
      "Steps : 3486 , Loss : 0.7830939292907715, Weights 1.0574332475662231, bias : 0.0594172403216362  \n",
      "Steps : 3487 , Loss : 0.7830938696861267, Weights 1.0574371814727783, bias : 0.059413839131593704  \n",
      "Steps : 3488 , Loss : 0.7830938696861267, Weights 1.0574411153793335, bias : 0.05941044166684151  \n",
      "Steps : 3489 , Loss : 0.7830938696861267, Weights 1.0574450492858887, bias : 0.05940705165266991  \n",
      "Steps : 3490 , Loss : 0.7830938696861267, Weights 1.0574489831924438, bias : 0.0594036690890789  \n",
      "Steps : 3491 , Loss : 0.7830936908721924, Weights 1.057452917098999, bias : 0.0594002902507782  \n",
      "Steps : 3492 , Loss : 0.7830936908721924, Weights 1.0574568510055542, bias : 0.05939691886305809  \n",
      "Steps : 3493 , Loss : 0.7830936908721924, Weights 1.0574607849121094, bias : 0.05939355492591858  \n",
      "Steps : 3494 , Loss : 0.7830936312675476, Weights 1.0574647188186646, bias : 0.059390194714069366  \n",
      "Steps : 3495 , Loss : 0.7830936908721924, Weights 1.0574685335159302, bias : 0.05938684195280075  \n",
      "Steps : 3496 , Loss : 0.7830936312675476, Weights 1.0574723482131958, bias : 0.05938349664211273  \n",
      "Steps : 3497 , Loss : 0.7830936312675476, Weights 1.0574761629104614, bias : 0.05938015505671501  \n",
      "Steps : 3498 , Loss : 0.7830936312675476, Weights 1.057479977607727, bias : 0.05937682092189789  \n",
      "Steps : 3499 , Loss : 0.7830935716629028, Weights 1.0574837923049927, bias : 0.05937349423766136  \n",
      "Steps : 3500 , Loss : 0.7830935716629028, Weights 1.0574876070022583, bias : 0.059370171278715134  \n",
      "Steps : 3501 , Loss : 0.7830935120582581, Weights 1.057491421699524, bias : 0.0593668557703495  \n",
      "Steps : 3502 , Loss : 0.7830933928489685, Weights 1.0574952363967896, bias : 0.05936354398727417  \n",
      "Steps : 3503 , Loss : 0.7830935120582581, Weights 1.0574990510940552, bias : 0.059360239654779434  \n",
      "Steps : 3504 , Loss : 0.7830935120582581, Weights 1.0575028657913208, bias : 0.059356942772865295  \n",
      "Steps : 3505 , Loss : 0.7830933928489685, Weights 1.0575066804885864, bias : 0.059353649616241455  \n",
      "Steps : 3506 , Loss : 0.7830933928489685, Weights 1.057510495185852, bias : 0.05935036391019821  \n",
      "Steps : 3507 , Loss : 0.7830933928489685, Weights 1.0575143098831177, bias : 0.05934708192944527  \n",
      "Steps : 3508 , Loss : 0.7830933928489685, Weights 1.0575181245803833, bias : 0.05934380739927292  \n",
      "Steps : 3509 , Loss : 0.7830933332443237, Weights 1.057521939277649, bias : 0.05934054031968117  \n",
      "Steps : 3510 , Loss : 0.783093273639679, Weights 1.0575257539749146, bias : 0.059337276965379715  \n",
      "Steps : 3511 , Loss : 0.7830932140350342, Weights 1.0575295686721802, bias : 0.05933402106165886  \n",
      "Steps : 3512 , Loss : 0.783093273639679, Weights 1.0575333833694458, bias : 0.0593307688832283  \n",
      "Steps : 3513 , Loss : 0.7830932140350342, Weights 1.0575370788574219, bias : 0.05932752415537834  \n",
      "Steps : 3514 , Loss : 0.7830930948257446, Weights 1.057540774345398, bias : 0.05932428687810898  \n",
      "Steps : 3515 , Loss : 0.7830932140350342, Weights 1.057544469833374, bias : 0.05932105332612991  \n",
      "Steps : 3516 , Loss : 0.7830930948257446, Weights 1.05754816532135, bias : 0.059317827224731445  \n",
      "Steps : 3517 , Loss : 0.7830930948257446, Weights 1.0575518608093262, bias : 0.059314604848623276  \n",
      "Steps : 3518 , Loss : 0.7830930948257446, Weights 1.0575555562973022, bias : 0.0593113899230957  \n",
      "Steps : 3519 , Loss : 0.7830930352210999, Weights 1.0575592517852783, bias : 0.05930817872285843  \n",
      "Steps : 3520 , Loss : 0.7830930352210999, Weights 1.0575629472732544, bias : 0.05930497497320175  \n",
      "Steps : 3521 , Loss : 0.7830929756164551, Weights 1.0575666427612305, bias : 0.05930177867412567  \n",
      "Steps : 3522 , Loss : 0.7830930948257446, Weights 1.0575703382492065, bias : 0.05929858610033989  \n",
      "Steps : 3523 , Loss : 0.7830929756164551, Weights 1.0575740337371826, bias : 0.059295400977134705  \n",
      "Steps : 3524 , Loss : 0.7830929756164551, Weights 1.0575777292251587, bias : 0.05929221957921982  \n",
      "Steps : 3525 , Loss : 0.7830929756164551, Weights 1.0575814247131348, bias : 0.05928904563188553  \n",
      "Steps : 3526 , Loss : 0.7830929756164551, Weights 1.0575851202011108, bias : 0.05928587540984154  \n",
      "Steps : 3527 , Loss : 0.7830929756164551, Weights 1.057588815689087, bias : 0.05928271263837814  \n",
      "Steps : 3528 , Loss : 0.7830927968025208, Weights 1.057592511177063, bias : 0.05927955359220505  \n",
      "Steps : 3529 , Loss : 0.7830927968025208, Weights 1.057596206665039, bias : 0.05927640199661255  \n",
      "Steps : 3530 , Loss : 0.7830927968025208, Weights 1.0575999021530151, bias : 0.05927325412631035  \n",
      "Steps : 3531 , Loss : 0.7830927968025208, Weights 1.0576034784317017, bias : 0.059270113706588745  \n",
      "Steps : 3532 , Loss : 0.783092737197876, Weights 1.0576070547103882, bias : 0.05926698073744774  \n",
      "Steps : 3533 , Loss : 0.7830927968025208, Weights 1.0576106309890747, bias : 0.05926385149359703  \n",
      "Steps : 3534 , Loss : 0.7830926775932312, Weights 1.0576142072677612, bias : 0.05926072970032692  \n",
      "Steps : 3535 , Loss : 0.7830926775932312, Weights 1.0576177835464478, bias : 0.05925761163234711  \n",
      "Steps : 3536 , Loss : 0.7830926775932312, Weights 1.0576213598251343, bias : 0.05925450101494789  \n",
      "Steps : 3537 , Loss : 0.7830926775932312, Weights 1.0576249361038208, bias : 0.059251394122838974  \n",
      "Steps : 3538 , Loss : 0.7830926775932312, Weights 1.0576285123825073, bias : 0.059248294681310654  \n",
      "Steps : 3539 , Loss : 0.7830926775932312, Weights 1.0576320886611938, bias : 0.05924519896507263  \n",
      "Steps : 3540 , Loss : 0.7830924987792969, Weights 1.0576356649398804, bias : 0.05924211069941521  \n",
      "Steps : 3541 , Loss : 0.7830925583839417, Weights 1.057639241218567, bias : 0.05923902615904808  \n",
      "Steps : 3542 , Loss : 0.7830924987792969, Weights 1.0576428174972534, bias : 0.05923594906926155  \n",
      "Steps : 3543 , Loss : 0.7830924987792969, Weights 1.05764639377594, bias : 0.05923287570476532  \n",
      "Steps : 3544 , Loss : 0.7830924987792969, Weights 1.0576499700546265, bias : 0.059229809790849686  \n",
      "Steps : 3545 , Loss : 0.7830924987792969, Weights 1.057653546333313, bias : 0.05922674760222435  \n",
      "Steps : 3546 , Loss : 0.7830924987792969, Weights 1.0576571226119995, bias : 0.05922369286417961  \n",
      "Steps : 3547 , Loss : 0.7830924391746521, Weights 1.057660698890686, bias : 0.05922064185142517  \n",
      "Steps : 3548 , Loss : 0.7830924391746521, Weights 1.0576642751693726, bias : 0.05921759828925133  \n",
      "Steps : 3549 , Loss : 0.7830923199653625, Weights 1.057667851448059, bias : 0.05921455845236778  \n",
      "Steps : 3550 , Loss : 0.7830923199653625, Weights 1.057671308517456, bias : 0.059211526066064835  \n",
      "Steps : 3551 , Loss : 0.7830923199653625, Weights 1.057674765586853, bias : 0.059208497405052185  \n",
      "Steps : 3552 , Loss : 0.7830923199653625, Weights 1.05767822265625, bias : 0.05920547619462013  \n",
      "Steps : 3553 , Loss : 0.7830922603607178, Weights 1.057681679725647, bias : 0.05920245870947838  \n",
      "Steps : 3554 , Loss : 0.7830923199653625, Weights 1.057685136795044, bias : 0.05919944867491722  \n",
      "Steps : 3555 , Loss : 0.783092200756073, Weights 1.057688593864441, bias : 0.05919644236564636  \n",
      "Steps : 3556 , Loss : 0.7830922603607178, Weights 1.057692050933838, bias : 0.0591934397816658  \n",
      "Steps : 3557 , Loss : 0.7830922603607178, Weights 1.0576955080032349, bias : 0.05919044464826584  \n",
      "Steps : 3558 , Loss : 0.783092200756073, Weights 1.0576989650726318, bias : 0.059187453240156174  \n",
      "Steps : 3559 , Loss : 0.783092200756073, Weights 1.0577024221420288, bias : 0.059184469282627106  \n",
      "Steps : 3560 , Loss : 0.7830921411514282, Weights 1.0577058792114258, bias : 0.059181489050388336  \n",
      "Steps : 3561 , Loss : 0.7830921411514282, Weights 1.0577093362808228, bias : 0.059178516268730164  \n",
      "Steps : 3562 , Loss : 0.7830920219421387, Weights 1.0577127933502197, bias : 0.05917554721236229  \n",
      "Steps : 3563 , Loss : 0.7830920219421387, Weights 1.0577162504196167, bias : 0.05917258560657501  \n",
      "Steps : 3564 , Loss : 0.7830920219421387, Weights 1.0577197074890137, bias : 0.05916962772607803  \n",
      "Steps : 3565 , Loss : 0.7830921411514282, Weights 1.0577231645584106, bias : 0.05916667729616165  \n",
      "Steps : 3566 , Loss : 0.7830920219421387, Weights 1.0577266216278076, bias : 0.05916373059153557  \n",
      "Steps : 3567 , Loss : 0.7830920219421387, Weights 1.0577300786972046, bias : 0.05916078761219978  \n",
      "Steps : 3568 , Loss : 0.7830919623374939, Weights 1.0577335357666016, bias : 0.059157852083444595  \n",
      "Steps : 3569 , Loss : 0.7830920219421387, Weights 1.0577369928359985, bias : 0.059154920279979706  \n",
      "Steps : 3570 , Loss : 0.7830919623374939, Weights 1.057740330696106, bias : 0.05915199592709541  \n",
      "Steps : 3571 , Loss : 0.7830919623374939, Weights 1.0577436685562134, bias : 0.05914907529950142  \n",
      "Steps : 3572 , Loss : 0.7830919027328491, Weights 1.0577470064163208, bias : 0.05914616212248802  \n",
      "Steps : 3573 , Loss : 0.7830919027328491, Weights 1.0577503442764282, bias : 0.05914325267076492  \n",
      "Steps : 3574 , Loss : 0.7830919027328491, Weights 1.0577536821365356, bias : 0.05914034694433212  \n",
      "Steps : 3575 , Loss : 0.7830919027328491, Weights 1.057757019996643, bias : 0.05913744866847992  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3576 , Loss : 0.7830919027328491, Weights 1.0577603578567505, bias : 0.059134554117918015  \n",
      "Steps : 3577 , Loss : 0.7830917835235596, Weights 1.057763695716858, bias : 0.05913166701793671  \n",
      "Steps : 3578 , Loss : 0.7830917835235596, Weights 1.0577670335769653, bias : 0.0591287836432457  \n",
      "Steps : 3579 , Loss : 0.7830917239189148, Weights 1.0577703714370728, bias : 0.059125903993844986  \n",
      "Steps : 3580 , Loss : 0.7830917239189148, Weights 1.0577737092971802, bias : 0.05912303179502487  \n",
      "Steps : 3581 , Loss : 0.7830917239189148, Weights 1.0577770471572876, bias : 0.059120163321495056  \n",
      "Steps : 3582 , Loss : 0.7830917239189148, Weights 1.057780385017395, bias : 0.05911730229854584  \n",
      "Steps : 3583 , Loss : 0.78309166431427, Weights 1.0577837228775024, bias : 0.05911444500088692  \n",
      "Steps : 3584 , Loss : 0.7830916047096252, Weights 1.0577870607376099, bias : 0.059111591428518295  \n",
      "Steps : 3585 , Loss : 0.7830917239189148, Weights 1.0577903985977173, bias : 0.05910874530673027  \n",
      "Steps : 3586 , Loss : 0.7830916047096252, Weights 1.0577937364578247, bias : 0.059105902910232544  \n",
      "Steps : 3587 , Loss : 0.78309166431427, Weights 1.0577970743179321, bias : 0.059103067964315414  \n",
      "Steps : 3588 , Loss : 0.7830916047096252, Weights 1.0578004121780396, bias : 0.05910023674368858  \n",
      "Steps : 3589 , Loss : 0.7830916047096252, Weights 1.057803750038147, bias : 0.05909740924835205  \n",
      "Steps : 3590 , Loss : 0.7830916047096252, Weights 1.0578070878982544, bias : 0.059094589203596115  \n",
      "Steps : 3591 , Loss : 0.7830916047096252, Weights 1.0578103065490723, bias : 0.05909177288413048  \n",
      "Steps : 3592 , Loss : 0.7830916047096252, Weights 1.0578135251998901, bias : 0.05908896028995514  \n",
      "Steps : 3593 , Loss : 0.7830916047096252, Weights 1.057816743850708, bias : 0.0590861551463604  \n",
      "Steps : 3594 , Loss : 0.7830914855003357, Weights 1.0578199625015259, bias : 0.059083353728055954  \n",
      "Steps : 3595 , Loss : 0.7830914855003357, Weights 1.0578231811523438, bias : 0.05908055976033211  \n",
      "Steps : 3596 , Loss : 0.7830914258956909, Weights 1.0578263998031616, bias : 0.05907776951789856  \n",
      "Steps : 3597 , Loss : 0.7830914258956909, Weights 1.0578296184539795, bias : 0.05907498300075531  \n",
      "Steps : 3598 , Loss : 0.7830913662910461, Weights 1.0578328371047974, bias : 0.05907220393419266  \n",
      "Steps : 3599 , Loss : 0.7830914258956909, Weights 1.0578360557556152, bias : 0.0590694285929203  \n",
      "Steps : 3600 , Loss : 0.7830913662910461, Weights 1.057839274406433, bias : 0.05906665697693825  \n",
      "Steps : 3601 , Loss : 0.7830913662910461, Weights 1.057842493057251, bias : 0.05906389281153679  \n",
      "Steps : 3602 , Loss : 0.7830913662910461, Weights 1.0578457117080688, bias : 0.05906113237142563  \n",
      "Steps : 3603 , Loss : 0.7830914258956909, Weights 1.0578489303588867, bias : 0.05905837565660477  \n",
      "Steps : 3604 , Loss : 0.7830913066864014, Weights 1.0578521490097046, bias : 0.0590556263923645  \n",
      "Steps : 3605 , Loss : 0.7830913066864014, Weights 1.0578553676605225, bias : 0.059052880853414536  \n",
      "Steps : 3606 , Loss : 0.7830913066864014, Weights 1.0578585863113403, bias : 0.05905013903975487  \n",
      "Steps : 3607 , Loss : 0.7830913066864014, Weights 1.0578618049621582, bias : 0.059047404676675797  \n",
      "Steps : 3608 , Loss : 0.7830913066864014, Weights 1.057865023612976, bias : 0.059044674038887024  \n",
      "Steps : 3609 , Loss : 0.7830911874771118, Weights 1.057868242263794, bias : 0.05904194712638855  \n",
      "Steps : 3610 , Loss : 0.7830911874771118, Weights 1.0578714609146118, bias : 0.05903922766447067  \n",
      "Steps : 3611 , Loss : 0.7830911874771118, Weights 1.0578746795654297, bias : 0.059036511927843094  \n",
      "Steps : 3612 , Loss : 0.783091127872467, Weights 1.057877779006958, bias : 0.059033799916505814  \n",
      "Steps : 3613 , Loss : 0.783091127872467, Weights 1.0578808784484863, bias : 0.05903109535574913  \n",
      "Steps : 3614 , Loss : 0.783091127872467, Weights 1.0578839778900146, bias : 0.059028394520282745  \n",
      "Steps : 3615 , Loss : 0.783091127872467, Weights 1.057887077331543, bias : 0.05902569741010666  \n",
      "Steps : 3616 , Loss : 0.783091127872467, Weights 1.0578901767730713, bias : 0.05902300775051117  \n",
      "Steps : 3617 , Loss : 0.7830910682678223, Weights 1.0578932762145996, bias : 0.05902032181620598  \n",
      "Steps : 3618 , Loss : 0.7830910682678223, Weights 1.057896375656128, bias : 0.059017639607191086  \n",
      "Steps : 3619 , Loss : 0.7830910682678223, Weights 1.0578994750976562, bias : 0.05901496484875679  \n",
      "Steps : 3620 , Loss : 0.7830909490585327, Weights 1.0579025745391846, bias : 0.05901229381561279  \n",
      "Steps : 3621 , Loss : 0.7830909490585327, Weights 1.057905673980713, bias : 0.059009626507759094  \n",
      "Steps : 3622 , Loss : 0.7830909490585327, Weights 1.0579087734222412, bias : 0.05900696665048599  \n",
      "Steps : 3623 , Loss : 0.7830909490585327, Weights 1.0579118728637695, bias : 0.05900431051850319  \n",
      "Steps : 3624 , Loss : 0.7830909490585327, Weights 1.0579149723052979, bias : 0.059001658111810684  \n",
      "Steps : 3625 , Loss : 0.7830909490585327, Weights 1.0579180717468262, bias : 0.05899900943040848  \n",
      "Steps : 3626 , Loss : 0.7830909490585327, Weights 1.0579211711883545, bias : 0.05899636819958687  \n",
      "Steps : 3627 , Loss : 0.7830908894538879, Weights 1.0579242706298828, bias : 0.05899373069405556  \n",
      "Steps : 3628 , Loss : 0.7830908894538879, Weights 1.0579273700714111, bias : 0.058991096913814545  \n",
      "Steps : 3629 , Loss : 0.7830908298492432, Weights 1.0579304695129395, bias : 0.05898847058415413  \n",
      "Steps : 3630 , Loss : 0.7830908298492432, Weights 1.0579335689544678, bias : 0.05898584797978401  \n",
      "Steps : 3631 , Loss : 0.7830908298492432, Weights 1.057936668395996, bias : 0.05898322910070419  \n",
      "Steps : 3632 , Loss : 0.7830908298492432, Weights 1.0579397678375244, bias : 0.05898061394691467  \n",
      "Steps : 3633 , Loss : 0.7830908298492432, Weights 1.0579428672790527, bias : 0.05897800624370575  \n",
      "Steps : 3634 , Loss : 0.7830908298492432, Weights 1.0579458475112915, bias : 0.058975402265787125  \n",
      "Steps : 3635 , Loss : 0.7830908298492432, Weights 1.0579488277435303, bias : 0.0589728020131588  \n",
      "Steps : 3636 , Loss : 0.7830907702445984, Weights 1.057951807975769, bias : 0.05897020548582077  \n",
      "Steps : 3637 , Loss : 0.7830906510353088, Weights 1.0579547882080078, bias : 0.05896761640906334  \n",
      "Steps : 3638 , Loss : 0.7830908298492432, Weights 1.0579577684402466, bias : 0.05896503105759621  \n",
      "Steps : 3639 , Loss : 0.7830907702445984, Weights 1.0579607486724854, bias : 0.05896244943141937  \n",
      "Steps : 3640 , Loss : 0.7830906510353088, Weights 1.0579637289047241, bias : 0.05895987153053284  \n",
      "Steps : 3641 , Loss : 0.7830906510353088, Weights 1.057966709136963, bias : 0.0589573010802269  \n",
      "Steps : 3642 , Loss : 0.7830906510353088, Weights 1.0579696893692017, bias : 0.05895473435521126  \n",
      "Steps : 3643 , Loss : 0.7830905914306641, Weights 1.0579726696014404, bias : 0.058952171355485916  \n",
      "Steps : 3644 , Loss : 0.7830905914306641, Weights 1.0579756498336792, bias : 0.05894961208105087  \n",
      "Steps : 3645 , Loss : 0.7830906510353088, Weights 1.057978630065918, bias : 0.058947060257196426  \n",
      "Steps : 3646 , Loss : 0.7830906510353088, Weights 1.0579816102981567, bias : 0.05894451215863228  \n",
      "Steps : 3647 , Loss : 0.7830905914306641, Weights 1.0579845905303955, bias : 0.05894196778535843  \n",
      "Steps : 3648 , Loss : 0.7830905318260193, Weights 1.0579875707626343, bias : 0.05893942713737488  \n",
      "Steps : 3649 , Loss : 0.7830905914306641, Weights 1.057990550994873, bias : 0.058936893939971924  \n",
      "Steps : 3650 , Loss : 0.7830905914306641, Weights 1.0579935312271118, bias : 0.05893436446785927  \n",
      "Steps : 3651 , Loss : 0.7830905914306641, Weights 1.0579965114593506, bias : 0.05893183872103691  \n",
      "Steps : 3652 , Loss : 0.7830905318260193, Weights 1.0579994916915894, bias : 0.05892931669950485  \n",
      "Steps : 3653 , Loss : 0.7830905318260193, Weights 1.0580024719238281, bias : 0.05892680212855339  \n",
      "Steps : 3654 , Loss : 0.7830905318260193, Weights 1.058005452156067, bias : 0.05892429128289223  \n",
      "Steps : 3655 , Loss : 0.7830904126167297, Weights 1.0580084323883057, bias : 0.05892178416252136  \n",
      "Steps : 3656 , Loss : 0.7830905318260193, Weights 1.0580114126205444, bias : 0.058919280767440796  \n",
      "Steps : 3657 , Loss : 0.783090353012085, Weights 1.0580142736434937, bias : 0.058916784822940826  \n",
      "Steps : 3658 , Loss : 0.783090353012085, Weights 1.0580171346664429, bias : 0.058914292603731155  \n",
      "Steps : 3659 , Loss : 0.7830904126167297, Weights 1.058019995689392, bias : 0.05891180410981178  \n",
      "Steps : 3660 , Loss : 0.783090353012085, Weights 1.0580228567123413, bias : 0.05890931934118271  \n",
      "Steps : 3661 , Loss : 0.7830904126167297, Weights 1.0580257177352905, bias : 0.05890683829784393  \n",
      "Steps : 3662 , Loss : 0.783090353012085, Weights 1.0580285787582397, bias : 0.058904364705085754  \n",
      "Steps : 3663 , Loss : 0.783090353012085, Weights 1.058031439781189, bias : 0.058901894837617874  \n",
      "Steps : 3664 , Loss : 0.7830902934074402, Weights 1.0580343008041382, bias : 0.05889942869544029  \n",
      "Steps : 3665 , Loss : 0.7830902934074402, Weights 1.0580371618270874, bias : 0.05889696627855301  \n",
      "Steps : 3666 , Loss : 0.783090353012085, Weights 1.0580400228500366, bias : 0.058894507586956024  \n",
      "Steps : 3667 , Loss : 0.7830902934074402, Weights 1.0580428838729858, bias : 0.058892056345939636  \n",
      "Steps : 3668 , Loss : 0.7830902934074402, Weights 1.058045744895935, bias : 0.05888960883021355  \n",
      "Steps : 3669 , Loss : 0.7830902338027954, Weights 1.0580486059188843, bias : 0.058887165039777756  \n",
      "Steps : 3670 , Loss : 0.7830902934074402, Weights 1.0580514669418335, bias : 0.05888472497463226  \n",
      "Steps : 3671 , Loss : 0.7830902338027954, Weights 1.0580543279647827, bias : 0.05888228863477707  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3672 , Loss : 0.7830902934074402, Weights 1.058057188987732, bias : 0.05887985974550247  \n",
      "Steps : 3673 , Loss : 0.7830902338027954, Weights 1.0580600500106812, bias : 0.05887743458151817  \n",
      "Steps : 3674 , Loss : 0.7830902934074402, Weights 1.0580629110336304, bias : 0.05887501314282417  \n",
      "Steps : 3675 , Loss : 0.7830901145935059, Weights 1.0580657720565796, bias : 0.05887259542942047  \n",
      "Steps : 3676 , Loss : 0.7830902338027954, Weights 1.0580686330795288, bias : 0.05887018144130707  \n",
      "Steps : 3677 , Loss : 0.7830902338027954, Weights 1.058071494102478, bias : 0.05886777117848396  \n",
      "Steps : 3678 , Loss : 0.7830901145935059, Weights 1.0580743551254272, bias : 0.058865368366241455  \n",
      "Steps : 3679 , Loss : 0.7830901145935059, Weights 1.0580772161483765, bias : 0.058862969279289246  \n",
      "Steps : 3680 , Loss : 0.7830901145935059, Weights 1.0580799579620361, bias : 0.058860573917627335  \n",
      "Steps : 3681 , Loss : 0.7830901145935059, Weights 1.0580826997756958, bias : 0.05885818228125572  \n",
      "Steps : 3682 , Loss : 0.7830901145935059, Weights 1.0580854415893555, bias : 0.05885579437017441  \n",
      "Steps : 3683 , Loss : 0.7830900549888611, Weights 1.0580881834030151, bias : 0.05885341018438339  \n",
      "Steps : 3684 , Loss : 0.7830900549888611, Weights 1.0580909252166748, bias : 0.058851033449172974  \n",
      "Steps : 3685 , Loss : 0.7830900549888611, Weights 1.0580936670303345, bias : 0.05884866043925285  \n",
      "Steps : 3686 , Loss : 0.7830900549888611, Weights 1.0580964088439941, bias : 0.05884629115462303  \n",
      "Steps : 3687 , Loss : 0.7830900549888611, Weights 1.0580991506576538, bias : 0.05884392559528351  \n",
      "Steps : 3688 , Loss : 0.7830900549888611, Weights 1.0581018924713135, bias : 0.05884156376123428  \n",
      "Steps : 3689 , Loss : 0.7830900549888611, Weights 1.0581046342849731, bias : 0.05883920565247536  \n",
      "Steps : 3690 , Loss : 0.7830898761749268, Weights 1.0581073760986328, bias : 0.05883685499429703  \n",
      "Steps : 3691 , Loss : 0.7830898761749268, Weights 1.0581101179122925, bias : 0.058834508061409  \n",
      "Steps : 3692 , Loss : 0.7830898761749268, Weights 1.0581128597259521, bias : 0.058832164853811264  \n",
      "Steps : 3693 , Loss : 0.7830898761749268, Weights 1.0581156015396118, bias : 0.05882982537150383  \n",
      "Steps : 3694 , Loss : 0.7830900549888611, Weights 1.0581183433532715, bias : 0.058827489614486694  \n",
      "Steps : 3695 , Loss : 0.7830898761749268, Weights 1.0581210851669312, bias : 0.05882515758275986  \n",
      "Steps : 3696 , Loss : 0.7830898761749268, Weights 1.0581238269805908, bias : 0.05882282927632332  \n",
      "Steps : 3697 , Loss : 0.7830898761749268, Weights 1.0581265687942505, bias : 0.05882050469517708  \n",
      "Steps : 3698 , Loss : 0.7830898761749268, Weights 1.0581293106079102, bias : 0.058818187564611435  \n",
      "Steps : 3699 , Loss : 0.7830898761749268, Weights 1.0581320524215698, bias : 0.05881587415933609  \n",
      "Steps : 3700 , Loss : 0.783089816570282, Weights 1.0581347942352295, bias : 0.058813564479351044  \n",
      "Steps : 3701 , Loss : 0.7830898761749268, Weights 1.0581375360488892, bias : 0.058811258524656296  \n",
      "Steps : 3702 , Loss : 0.7830897569656372, Weights 1.0581402778625488, bias : 0.058808956295251846  \n",
      "Steps : 3703 , Loss : 0.7830897569656372, Weights 1.0581430196762085, bias : 0.058806657791137695  \n",
      "Steps : 3704 , Loss : 0.7830897569656372, Weights 1.0581457614898682, bias : 0.05880436301231384  \n",
      "Steps : 3705 , Loss : 0.783089816570282, Weights 1.0581483840942383, bias : 0.05880207195878029  \n",
      "Steps : 3706 , Loss : 0.7830897569656372, Weights 1.0581510066986084, bias : 0.05879978835582733  \n",
      "Steps : 3707 , Loss : 0.7830897569656372, Weights 1.0581536293029785, bias : 0.05879750847816467  \n",
      "Steps : 3708 , Loss : 0.783089816570282, Weights 1.0581562519073486, bias : 0.05879523232579231  \n",
      "Steps : 3709 , Loss : 0.7830897569656372, Weights 1.0581588745117188, bias : 0.05879295989871025  \n",
      "Steps : 3710 , Loss : 0.7830897569656372, Weights 1.0581614971160889, bias : 0.05879069119691849  \n",
      "Steps : 3711 , Loss : 0.7830897569656372, Weights 1.058164119720459, bias : 0.05878842622041702  \n",
      "Steps : 3712 , Loss : 0.7830897569656372, Weights 1.058166742324829, bias : 0.058786164969205856  \n",
      "Steps : 3713 , Loss : 0.7830897569656372, Weights 1.0581693649291992, bias : 0.05878390744328499  \n",
      "Steps : 3714 , Loss : 0.7830896973609924, Weights 1.0581719875335693, bias : 0.05878165364265442  \n",
      "Steps : 3715 , Loss : 0.7830895781517029, Weights 1.0581746101379395, bias : 0.058779407292604446  \n",
      "Steps : 3716 , Loss : 0.7830896973609924, Weights 1.0581772327423096, bias : 0.05877716466784477  \n",
      "Steps : 3717 , Loss : 0.7830895781517029, Weights 1.0581798553466797, bias : 0.0587749257683754  \n",
      "Steps : 3718 , Loss : 0.7830895781517029, Weights 1.0581824779510498, bias : 0.05877269059419632  \n",
      "Steps : 3719 , Loss : 0.7830896973609924, Weights 1.05818510055542, bias : 0.05877045914530754  \n",
      "Steps : 3720 , Loss : 0.7830895781517029, Weights 1.05818772315979, bias : 0.05876823142170906  \n",
      "Steps : 3721 , Loss : 0.7830895781517029, Weights 1.0581903457641602, bias : 0.05876600742340088  \n",
      "Steps : 3722 , Loss : 0.7830895781517029, Weights 1.0581929683685303, bias : 0.058763787150382996  \n",
      "Steps : 3723 , Loss : 0.7830895781517029, Weights 1.0581955909729004, bias : 0.05876157060265541  \n",
      "Steps : 3724 , Loss : 0.7830895781517029, Weights 1.0581982135772705, bias : 0.058759357780218124  \n",
      "Steps : 3725 , Loss : 0.7830895185470581, Weights 1.0582008361816406, bias : 0.058757148683071136  \n",
      "Steps : 3726 , Loss : 0.7830895781517029, Weights 1.0582034587860107, bias : 0.05875494331121445  \n",
      "Steps : 3727 , Loss : 0.7830895781517029, Weights 1.0582060813903809, bias : 0.058752745389938354  \n",
      "Steps : 3728 , Loss : 0.7830895185470581, Weights 1.058208703994751, bias : 0.05875055119395256  \n",
      "Steps : 3729 , Loss : 0.7830895781517029, Weights 1.058211326599121, bias : 0.058748360723257065  \n",
      "Steps : 3730 , Loss : 0.7830894589424133, Weights 1.0582139492034912, bias : 0.05874617397785187  \n",
      "Steps : 3731 , Loss : 0.7830894589424133, Weights 1.0582164525985718, bias : 0.05874399095773697  \n",
      "Steps : 3732 , Loss : 0.7830894589424133, Weights 1.0582189559936523, bias : 0.05874181166291237  \n",
      "Steps : 3733 , Loss : 0.7830894589424133, Weights 1.058221459388733, bias : 0.05873963609337807  \n",
      "Steps : 3734 , Loss : 0.7830894589424133, Weights 1.0582239627838135, bias : 0.058737464249134064  \n",
      "Steps : 3735 , Loss : 0.7830894589424133, Weights 1.058226466178894, bias : 0.05873529613018036  \n",
      "Steps : 3736 , Loss : 0.7830893993377686, Weights 1.0582289695739746, bias : 0.05873313173651695  \n",
      "Steps : 3737 , Loss : 0.7830894589424133, Weights 1.0582314729690552, bias : 0.058730971068143845  \n",
      "Steps : 3738 , Loss : 0.7830893993377686, Weights 1.0582339763641357, bias : 0.058728814125061035  \n",
      "Steps : 3739 , Loss : 0.783089280128479, Weights 1.0582364797592163, bias : 0.058726660907268524  \n",
      "Steps : 3740 , Loss : 0.7830894589424133, Weights 1.0582389831542969, bias : 0.05872451141476631  \n",
      "Steps : 3741 , Loss : 0.7830894589424133, Weights 1.0582414865493774, bias : 0.0587223656475544  \n",
      "Steps : 3742 , Loss : 0.783089280128479, Weights 1.058243989944458, bias : 0.05872022360563278  \n",
      "Steps : 3743 , Loss : 0.7830893993377686, Weights 1.0582464933395386, bias : 0.058718085289001465  \n",
      "Steps : 3744 , Loss : 0.783089280128479, Weights 1.0582489967346191, bias : 0.058715954422950745  \n",
      "Steps : 3745 , Loss : 0.7830893993377686, Weights 1.0582515001296997, bias : 0.05871382728219032  \n",
      "Steps : 3746 , Loss : 0.783089280128479, Weights 1.0582540035247803, bias : 0.0587117038667202  \n",
      "Steps : 3747 , Loss : 0.783089280128479, Weights 1.0582565069198608, bias : 0.058709584176540375  \n",
      "Steps : 3748 , Loss : 0.783089280128479, Weights 1.0582590103149414, bias : 0.05870746821165085  \n",
      "Steps : 3749 , Loss : 0.783089280128479, Weights 1.058261513710022, bias : 0.05870535597205162  \n",
      "Steps : 3750 , Loss : 0.783089280128479, Weights 1.0582640171051025, bias : 0.05870324745774269  \n",
      "Steps : 3751 , Loss : 0.783089280128479, Weights 1.058266520500183, bias : 0.05870114266872406  \n",
      "Steps : 3752 , Loss : 0.783089280128479, Weights 1.0582690238952637, bias : 0.05869904160499573  \n",
      "Steps : 3753 , Loss : 0.783089280128479, Weights 1.0582715272903442, bias : 0.058696944266557693  \n",
      "Steps : 3754 , Loss : 0.7830892205238342, Weights 1.0582740306854248, bias : 0.05869485065340996  \n",
      "Steps : 3755 , Loss : 0.783089280128479, Weights 1.0582765340805054, bias : 0.05869276076555252  \n",
      "Steps : 3756 , Loss : 0.783089280128479, Weights 1.058279037475586, bias : 0.05869067460298538  \n",
      "Steps : 3757 , Loss : 0.7830892205238342, Weights 1.0582815408706665, bias : 0.05868859216570854  \n",
      "Steps : 3758 , Loss : 0.7830891609191895, Weights 1.058284044265747, bias : 0.058686513453722  \n",
      "Steps : 3759 , Loss : 0.7830891609191895, Weights 1.058286428451538, bias : 0.05868443846702576  \n",
      "Steps : 3760 , Loss : 0.7830891609191895, Weights 1.058288812637329, bias : 0.05868236720561981  \n",
      "Steps : 3761 , Loss : 0.7830891609191895, Weights 1.0582911968231201, bias : 0.058680299669504166  \n",
      "Steps : 3762 , Loss : 0.7830891609191895, Weights 1.0582935810089111, bias : 0.05867823585867882  \n",
      "Steps : 3763 , Loss : 0.7830891609191895, Weights 1.0582959651947021, bias : 0.05867617577314377  \n",
      "Steps : 3764 , Loss : 0.7830891609191895, Weights 1.0582983493804932, bias : 0.05867411941289902  \n",
      "Steps : 3765 , Loss : 0.7830891609191895, Weights 1.0583007335662842, bias : 0.058672066777944565  \n",
      "Steps : 3766 , Loss : 0.7830891609191895, Weights 1.0583031177520752, bias : 0.05867001786828041  \n",
      "Steps : 3767 , Loss : 0.7830891609191895, Weights 1.0583055019378662, bias : 0.058667972683906555  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3768 , Loss : 0.7830890417098999, Weights 1.0583078861236572, bias : 0.058665931224823  \n",
      "Steps : 3769 , Loss : 0.7830891609191895, Weights 1.0583102703094482, bias : 0.05866389349102974  \n",
      "Steps : 3770 , Loss : 0.7830890417098999, Weights 1.0583126544952393, bias : 0.05866185948252678  \n",
      "Steps : 3771 , Loss : 0.7830890417098999, Weights 1.0583150386810303, bias : 0.05865982919931412  \n",
      "Steps : 3772 , Loss : 0.7830890417098999, Weights 1.0583174228668213, bias : 0.058657802641391754  \n",
      "Steps : 3773 , Loss : 0.7830889821052551, Weights 1.0583198070526123, bias : 0.05865577980875969  \n",
      "Steps : 3774 , Loss : 0.7830890417098999, Weights 1.0583221912384033, bias : 0.05865376070141792  \n",
      "Steps : 3775 , Loss : 0.7830890417098999, Weights 1.0583245754241943, bias : 0.058651745319366455  \n",
      "Steps : 3776 , Loss : 0.7830889821052551, Weights 1.0583269596099854, bias : 0.058649733662605286  \n",
      "Steps : 3777 , Loss : 0.7830890417098999, Weights 1.0583293437957764, bias : 0.058647725731134415  \n",
      "Steps : 3778 , Loss : 0.7830889821052551, Weights 1.0583317279815674, bias : 0.05864572152495384  \n",
      "Steps : 3779 , Loss : 0.7830889821052551, Weights 1.0583341121673584, bias : 0.05864372104406357  \n",
      "Steps : 3780 , Loss : 0.7830889821052551, Weights 1.0583364963531494, bias : 0.05864172428846359  \n",
      "Steps : 3781 , Loss : 0.7830889821052551, Weights 1.0583388805389404, bias : 0.058639731258153915  \n",
      "Steps : 3782 , Loss : 0.7830889821052551, Weights 1.0583412647247314, bias : 0.05863774195313454  \n",
      "Steps : 3783 , Loss : 0.7830889821052551, Weights 1.0583436489105225, bias : 0.05863575637340546  \n",
      "Steps : 3784 , Loss : 0.7830889821052551, Weights 1.0583460330963135, bias : 0.058633774518966675  \n",
      "Steps : 3785 , Loss : 0.7830889821052551, Weights 1.0583484172821045, bias : 0.05863179638981819  \n",
      "Steps : 3786 , Loss : 0.7830888628959656, Weights 1.0583508014678955, bias : 0.05862981826066971  \n",
      "Steps : 3787 , Loss : 0.7830889821052551, Weights 1.058353066444397, bias : 0.05862784385681152  \n",
      "Steps : 3788 , Loss : 0.7830889821052551, Weights 1.0583553314208984, bias : 0.05862587317824364  \n",
      "Steps : 3789 , Loss : 0.7830889225006104, Weights 1.0583575963974, bias : 0.05862390622496605  \n",
      "Steps : 3790 , Loss : 0.7830888628959656, Weights 1.0583598613739014, bias : 0.05862194299697876  \n",
      "Steps : 3791 , Loss : 0.7830889821052551, Weights 1.0583621263504028, bias : 0.05861998349428177  \n",
      "Steps : 3792 , Loss : 0.7830888628959656, Weights 1.0583643913269043, bias : 0.058618027716875076  \n",
      "Steps : 3793 , Loss : 0.7830888628959656, Weights 1.0583666563034058, bias : 0.05861607566475868  \n",
      "Steps : 3794 , Loss : 0.7830888628959656, Weights 1.0583689212799072, bias : 0.05861412733793259  \n",
      "Steps : 3795 , Loss : 0.7830888628959656, Weights 1.0583711862564087, bias : 0.05861218273639679  \n",
      "Steps : 3796 , Loss : 0.7830888628959656, Weights 1.0583734512329102, bias : 0.05861024186015129  \n",
      "Steps : 3797 , Loss : 0.7830888628959656, Weights 1.0583757162094116, bias : 0.05860830470919609  \n",
      "Steps : 3798 , Loss : 0.7830888628959656, Weights 1.058377981185913, bias : 0.05860637128353119  \n",
      "Steps : 3799 , Loss : 0.7830888628959656, Weights 1.0583802461624146, bias : 0.058604441583156586  \n",
      "Steps : 3800 , Loss : 0.7830886840820312, Weights 1.058382511138916, bias : 0.05860251560807228  \n",
      "Steps : 3801 , Loss : 0.7830886840820312, Weights 1.0583847761154175, bias : 0.058600593358278275  \n",
      "Steps : 3802 , Loss : 0.783088743686676, Weights 1.058387041091919, bias : 0.05859867483377457  \n",
      "Steps : 3803 , Loss : 0.7830888628959656, Weights 1.0583893060684204, bias : 0.05859676003456116  \n",
      "Steps : 3804 , Loss : 0.7830886840820312, Weights 1.0583915710449219, bias : 0.058594848960638046  \n",
      "Steps : 3805 , Loss : 0.783088743686676, Weights 1.0583938360214233, bias : 0.058592937886714935  \n",
      "Steps : 3806 , Loss : 0.7830886840820312, Weights 1.0583961009979248, bias : 0.05859103053808212  \n",
      "Steps : 3807 , Loss : 0.783088743686676, Weights 1.0583983659744263, bias : 0.05858912691473961  \n",
      "Steps : 3808 , Loss : 0.7830886840820312, Weights 1.0584006309509277, bias : 0.05858722701668739  \n",
      "Steps : 3809 , Loss : 0.7830886840820312, Weights 1.0584028959274292, bias : 0.058585330843925476  \n",
      "Steps : 3810 , Loss : 0.7830886840820312, Weights 1.0584051609039307, bias : 0.05858343839645386  \n",
      "Steps : 3811 , Loss : 0.7830886840820312, Weights 1.0584074258804321, bias : 0.05858154967427254  \n",
      "Steps : 3812 , Loss : 0.783088743686676, Weights 1.0584096908569336, bias : 0.058579664677381516  \n",
      "Steps : 3813 , Loss : 0.7830886840820312, Weights 1.058411955833435, bias : 0.05857778340578079  \n",
      "Steps : 3814 , Loss : 0.7830886840820312, Weights 1.0584142208099365, bias : 0.05857590585947037  \n",
      "Steps : 3815 , Loss : 0.7830886840820312, Weights 1.058416485786438, bias : 0.05857403203845024  \n",
      "Steps : 3816 , Loss : 0.7830886840820312, Weights 1.0584187507629395, bias : 0.05857216194272041  \n",
      "Steps : 3817 , Loss : 0.7830886840820312, Weights 1.0584208965301514, bias : 0.058570291846990585  \n",
      "Steps : 3818 , Loss : 0.7830886244773865, Weights 1.0584230422973633, bias : 0.058568425476551056  \n",
      "Steps : 3819 , Loss : 0.7830886840820312, Weights 1.0584251880645752, bias : 0.058566562831401825  \n",
      "Steps : 3820 , Loss : 0.7830886840820312, Weights 1.058427333831787, bias : 0.05856470391154289  \n",
      "Steps : 3821 , Loss : 0.7830886244773865, Weights 1.058429479598999, bias : 0.05856284871697426  \n",
      "Steps : 3822 , Loss : 0.7830886840820312, Weights 1.058431625366211, bias : 0.05856099724769592  \n",
      "Steps : 3823 , Loss : 0.7830885052680969, Weights 1.0584337711334229, bias : 0.058559149503707886  \n",
      "Steps : 3824 , Loss : 0.7830885052680969, Weights 1.0584359169006348, bias : 0.05855730548501015  \n",
      "Steps : 3825 , Loss : 0.7830886244773865, Weights 1.0584380626678467, bias : 0.05855546519160271  \n",
      "Steps : 3826 , Loss : 0.7830885052680969, Weights 1.0584402084350586, bias : 0.058553628623485565  \n",
      "Steps : 3827 , Loss : 0.7830885052680969, Weights 1.0584423542022705, bias : 0.05855179578065872  \n",
      "Steps : 3828 , Loss : 0.7830885052680969, Weights 1.0584444999694824, bias : 0.05854996293783188  \n",
      "Steps : 3829 , Loss : 0.7830884456634521, Weights 1.0584466457366943, bias : 0.058548133820295334  \n",
      "Steps : 3830 , Loss : 0.7830885052680969, Weights 1.0584487915039062, bias : 0.05854630842804909  \n",
      "Steps : 3831 , Loss : 0.7830885052680969, Weights 1.0584509372711182, bias : 0.05854448676109314  \n",
      "Steps : 3832 , Loss : 0.7830885052680969, Weights 1.05845308303833, bias : 0.05854266881942749  \n",
      "Steps : 3833 , Loss : 0.7830884456634521, Weights 1.058455228805542, bias : 0.05854085460305214  \n",
      "Steps : 3834 , Loss : 0.7830885052680969, Weights 1.058457374572754, bias : 0.05853904411196709  \n",
      "Steps : 3835 , Loss : 0.7830885052680969, Weights 1.0584595203399658, bias : 0.05853723734617233  \n",
      "Steps : 3836 , Loss : 0.7830885052680969, Weights 1.0584616661071777, bias : 0.05853543430566788  \n",
      "Steps : 3837 , Loss : 0.7830885052680969, Weights 1.0584638118743896, bias : 0.05853363126516342  \n",
      "Steps : 3838 , Loss : 0.7830883860588074, Weights 1.0584659576416016, bias : 0.058531831949949265  \n",
      "Steps : 3839 , Loss : 0.7830885052680969, Weights 1.0584681034088135, bias : 0.058530036360025406  \n",
      "Steps : 3840 , Loss : 0.7830884456634521, Weights 1.0584702491760254, bias : 0.058528244495391846  \n",
      "Steps : 3841 , Loss : 0.7830884456634521, Weights 1.0584723949432373, bias : 0.058526456356048584  \n",
      "Steps : 3842 , Loss : 0.7830884456634521, Weights 1.0584745407104492, bias : 0.05852467194199562  \n",
      "Steps : 3843 , Loss : 0.7830884456634521, Weights 1.0584766864776611, bias : 0.058522891253232956  \n",
      "Steps : 3844 , Loss : 0.7830884456634521, Weights 1.058478832244873, bias : 0.05852111056447029  \n",
      "Steps : 3845 , Loss : 0.7830883860588074, Weights 1.058480978012085, bias : 0.058519333600997925  \n",
      "Steps : 3846 , Loss : 0.7830883860588074, Weights 1.0584831237792969, bias : 0.05851756036281586  \n",
      "Steps : 3847 , Loss : 0.7830883860588074, Weights 1.0584852695465088, bias : 0.05851579084992409  \n",
      "Steps : 3848 , Loss : 0.7830883860588074, Weights 1.0584874153137207, bias : 0.05851402506232262  \n",
      "Steps : 3849 , Loss : 0.7830883860588074, Weights 1.058489441871643, bias : 0.058512263000011444  \n",
      "Steps : 3850 , Loss : 0.7830883860588074, Weights 1.0584914684295654, bias : 0.05851050466299057  \n",
      "Steps : 3851 , Loss : 0.7830883860588074, Weights 1.0584934949874878, bias : 0.058508746325969696  \n",
      "Steps : 3852 , Loss : 0.7830883860588074, Weights 1.0584955215454102, bias : 0.05850699171423912  \n",
      "Steps : 3853 , Loss : 0.7830883264541626, Weights 1.0584975481033325, bias : 0.05850524082779884  \n",
      "Steps : 3854 , Loss : 0.7830883860588074, Weights 1.0584995746612549, bias : 0.058503493666648865  \n",
      "Steps : 3855 , Loss : 0.7830883264541626, Weights 1.0585016012191772, bias : 0.058501750230789185  \n",
      "Steps : 3856 , Loss : 0.7830883860588074, Weights 1.0585036277770996, bias : 0.0585000105202198  \n",
      "Steps : 3857 , Loss : 0.7830883860588074, Weights 1.058505654335022, bias : 0.05849827453494072  \n",
      "Steps : 3858 , Loss : 0.7830883860588074, Weights 1.0585076808929443, bias : 0.058496538549661636  \n",
      "Steps : 3859 , Loss : 0.7830883264541626, Weights 1.0585097074508667, bias : 0.05849480628967285  \n",
      "Steps : 3860 , Loss : 0.7830883860588074, Weights 1.058511734008789, bias : 0.058493077754974365  \n",
      "Steps : 3861 , Loss : 0.783088207244873, Weights 1.0585137605667114, bias : 0.05849135294556618  \n",
      "Steps : 3862 , Loss : 0.7830883264541626, Weights 1.0585157871246338, bias : 0.05848963186144829  \n",
      "Steps : 3863 , Loss : 0.7830883264541626, Weights 1.0585178136825562, bias : 0.0584879145026207  \n",
      "Steps : 3864 , Loss : 0.7830883264541626, Weights 1.0585198402404785, bias : 0.058486200869083405  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3865 , Loss : 0.783088207244873, Weights 1.0585218667984009, bias : 0.05848448723554611  \n",
      "Steps : 3866 , Loss : 0.783088207244873, Weights 1.0585238933563232, bias : 0.05848277732729912  \n",
      "Steps : 3867 , Loss : 0.7830883264541626, Weights 1.0585259199142456, bias : 0.05848107114434242  \n",
      "Steps : 3868 , Loss : 0.783088207244873, Weights 1.058527946472168, bias : 0.058479368686676025  \n",
      "Steps : 3869 , Loss : 0.783088207244873, Weights 1.0585299730300903, bias : 0.05847766995429993  \n",
      "Steps : 3870 , Loss : 0.783088207244873, Weights 1.0585319995880127, bias : 0.05847597122192383  \n",
      "Steps : 3871 , Loss : 0.783088207244873, Weights 1.058534026145935, bias : 0.05847427621483803  \n",
      "Steps : 3872 , Loss : 0.7830883264541626, Weights 1.0585360527038574, bias : 0.058472584933042526  \n",
      "Steps : 3873 , Loss : 0.783088207244873, Weights 1.0585380792617798, bias : 0.05847089737653732  \n",
      "Steps : 3874 , Loss : 0.783088207244873, Weights 1.0585401058197021, bias : 0.05846921354532242  \n",
      "Steps : 3875 , Loss : 0.783088207244873, Weights 1.0585421323776245, bias : 0.05846753343939781  \n",
      "Steps : 3876 , Loss : 0.783088207244873, Weights 1.0585441589355469, bias : 0.058465853333473206  \n",
      "Steps : 3877 , Loss : 0.7830881476402283, Weights 1.0585461854934692, bias : 0.0584641769528389  \n",
      "Steps : 3878 , Loss : 0.783088207244873, Weights 1.0585482120513916, bias : 0.05846250429749489  \n",
      "Steps : 3879 , Loss : 0.783088207244873, Weights 1.058550238609314, bias : 0.05846083536744118  \n",
      "Steps : 3880 , Loss : 0.7830880880355835, Weights 1.0585522651672363, bias : 0.058459170162677765  \n",
      "Steps : 3881 , Loss : 0.783088207244873, Weights 1.0585542917251587, bias : 0.05845750495791435  \n",
      "Steps : 3882 , Loss : 0.783088207244873, Weights 1.058556318283081, bias : 0.05845584347844124  \n",
      "Steps : 3883 , Loss : 0.783088207244873, Weights 1.0585582256317139, bias : 0.05845418572425842  \n",
      "Steps : 3884 , Loss : 0.783088207244873, Weights 1.0585601329803467, bias : 0.058452531695365906  \n",
      "Steps : 3885 , Loss : 0.7830880880355835, Weights 1.0585620403289795, bias : 0.05845088139176369  \n",
      "Steps : 3886 , Loss : 0.7830880880355835, Weights 1.0585639476776123, bias : 0.05844923108816147  \n",
      "Steps : 3887 , Loss : 0.783088207244873, Weights 1.0585658550262451, bias : 0.05844758450984955  \n",
      "Steps : 3888 , Loss : 0.7830881476402283, Weights 1.058567762374878, bias : 0.05844594165682793  \n",
      "Steps : 3889 , Loss : 0.7830880880355835, Weights 1.0585696697235107, bias : 0.0584443025290966  \n",
      "Steps : 3890 , Loss : 0.7830880880355835, Weights 1.0585715770721436, bias : 0.05844266712665558  \n",
      "Steps : 3891 , Loss : 0.7830881476402283, Weights 1.0585734844207764, bias : 0.05844103544950485  \n",
      "Steps : 3892 , Loss : 0.7830880880355835, Weights 1.0585753917694092, bias : 0.058439403772354126  \n",
      "Steps : 3893 , Loss : 0.7830880880355835, Weights 1.058577299118042, bias : 0.0584377758204937  \n",
      "Steps : 3894 , Loss : 0.7830880880355835, Weights 1.0585792064666748, bias : 0.05843615159392357  \n",
      "Steps : 3895 , Loss : 0.7830880880355835, Weights 1.0585811138153076, bias : 0.05843453109264374  \n",
      "Steps : 3896 , Loss : 0.7830880880355835, Weights 1.0585830211639404, bias : 0.05843291059136391  \n",
      "Steps : 3897 , Loss : 0.7830880880355835, Weights 1.0585849285125732, bias : 0.058431293815374374  \n",
      "Steps : 3898 , Loss : 0.7830880880355835, Weights 1.058586835861206, bias : 0.05842968076467514  \n",
      "Steps : 3899 , Loss : 0.7830881476402283, Weights 1.0585887432098389, bias : 0.058428071439266205  \n",
      "Steps : 3900 , Loss : 0.7830880880355835, Weights 1.0585906505584717, bias : 0.05842646583914757  \n",
      "Steps : 3901 , Loss : 0.7830880880355835, Weights 1.0585925579071045, bias : 0.05842486023902893  \n",
      "Steps : 3902 , Loss : 0.7830880880355835, Weights 1.0585944652557373, bias : 0.05842325836420059  \n",
      "Steps : 3903 , Loss : 0.783087968826294, Weights 1.0585963726043701, bias : 0.05842166021466255  \n",
      "Steps : 3904 , Loss : 0.7830880880355835, Weights 1.058598279953003, bias : 0.05842006579041481  \n",
      "Steps : 3905 , Loss : 0.7830880880355835, Weights 1.0586001873016357, bias : 0.05841847136616707  \n",
      "Steps : 3906 , Loss : 0.783087968826294, Weights 1.0586020946502686, bias : 0.058416880667209625  \n",
      "Steps : 3907 , Loss : 0.783087968826294, Weights 1.0586040019989014, bias : 0.05841529369354248  \n",
      "Steps : 3908 , Loss : 0.783087968826294, Weights 1.0586059093475342, bias : 0.058413710445165634  \n",
      "Steps : 3909 , Loss : 0.783087968826294, Weights 1.058607816696167, bias : 0.058412130922079086  \n",
      "Steps : 3910 , Loss : 0.783087968826294, Weights 1.0586097240447998, bias : 0.05841055139899254  \n",
      "Steps : 3911 , Loss : 0.7830880880355835, Weights 1.0586116313934326, bias : 0.05840897560119629  \n",
      "Steps : 3912 , Loss : 0.7830879092216492, Weights 1.0586135387420654, bias : 0.05840740352869034  \n",
      "Steps : 3913 , Loss : 0.783087968826294, Weights 1.0586154460906982, bias : 0.058405835181474686  \n",
      "Steps : 3914 , Loss : 0.7830879092216492, Weights 1.058617353439331, bias : 0.05840426683425903  \n",
      "Steps : 3915 , Loss : 0.783087968826294, Weights 1.0586192607879639, bias : 0.05840270221233368  \n",
      "Steps : 3916 , Loss : 0.7830879092216492, Weights 1.0586211681365967, bias : 0.058401141315698624  \n",
      "Steps : 3917 , Loss : 0.783087968826294, Weights 1.0586230754852295, bias : 0.05839958414435387  \n",
      "Steps : 3918 , Loss : 0.7830879092216492, Weights 1.0586248636245728, bias : 0.05839802697300911  \n",
      "Steps : 3919 , Loss : 0.7830879092216492, Weights 1.058626651763916, bias : 0.05839647352695465  \n",
      "Steps : 3920 , Loss : 0.7830879092216492, Weights 1.0586284399032593, bias : 0.05839492380619049  \n",
      "Steps : 3921 , Loss : 0.7830879092216492, Weights 1.0586302280426025, bias : 0.05839337781071663  \n",
      "Steps : 3922 , Loss : 0.7830879092216492, Weights 1.0586320161819458, bias : 0.05839183181524277  \n",
      "Steps : 3923 , Loss : 0.7830879092216492, Weights 1.058633804321289, bias : 0.058390289545059204  \n",
      "Steps : 3924 , Loss : 0.7830878496170044, Weights 1.0586355924606323, bias : 0.05838875100016594  \n",
      "Steps : 3925 , Loss : 0.7830877900123596, Weights 1.0586373805999756, bias : 0.05838721618056297  \n",
      "Steps : 3926 , Loss : 0.7830879092216492, Weights 1.0586391687393188, bias : 0.05838568136096001  \n",
      "Steps : 3927 , Loss : 0.7830879092216492, Weights 1.058640956878662, bias : 0.05838415026664734  \n",
      "Steps : 3928 , Loss : 0.7830878496170044, Weights 1.0586427450180054, bias : 0.05838262289762497  \n",
      "Steps : 3929 , Loss : 0.7830879092216492, Weights 1.0586445331573486, bias : 0.0583810992538929  \n",
      "Steps : 3930 , Loss : 0.7830879092216492, Weights 1.058646321296692, bias : 0.05837957561016083  \n",
      "Steps : 3931 , Loss : 0.7830879092216492, Weights 1.0586481094360352, bias : 0.058378055691719055  \n",
      "Steps : 3932 , Loss : 0.7830879092216492, Weights 1.0586498975753784, bias : 0.05837653949856758  \n",
      "Steps : 3933 , Loss : 0.7830879092216492, Weights 1.0586516857147217, bias : 0.058375027030706406  \n",
      "Steps : 3934 , Loss : 0.7830878496170044, Weights 1.058653473854065, bias : 0.05837351456284523  \n",
      "Steps : 3935 , Loss : 0.7830877900123596, Weights 1.0586552619934082, bias : 0.05837200582027435  \n",
      "Steps : 3936 , Loss : 0.7830877900123596, Weights 1.0586570501327515, bias : 0.058370500802993774  \n",
      "Steps : 3937 , Loss : 0.7830877900123596, Weights 1.0586588382720947, bias : 0.058368999511003494  \n",
      "Steps : 3938 , Loss : 0.7830878496170044, Weights 1.058660626411438, bias : 0.058367498219013214  \n",
      "Steps : 3939 , Loss : 0.7830878496170044, Weights 1.0586624145507812, bias : 0.05836600065231323  \n",
      "Steps : 3940 , Loss : 0.7830878496170044, Weights 1.0586642026901245, bias : 0.05836450681090355  \n",
      "Steps : 3941 , Loss : 0.7830879092216492, Weights 1.0586659908294678, bias : 0.058363016694784164  \n",
      "Steps : 3942 , Loss : 0.7830878496170044, Weights 1.058667778968811, bias : 0.05836152657866478  \n",
      "Steps : 3943 , Loss : 0.7830877900123596, Weights 1.0586695671081543, bias : 0.05836004018783569  \n",
      "Steps : 3944 , Loss : 0.7830877900123596, Weights 1.0586713552474976, bias : 0.058358557522296906  \n",
      "Steps : 3945 , Loss : 0.7830877900123596, Weights 1.0586731433868408, bias : 0.05835707485675812  \n",
      "Steps : 3946 , Loss : 0.7830877900123596, Weights 1.058674931526184, bias : 0.05835559591650963  \n",
      "Steps : 3947 , Loss : 0.7830877900123596, Weights 1.0586767196655273, bias : 0.05835412070155144  \n",
      "Steps : 3948 , Loss : 0.7830877900123596, Weights 1.0586785078048706, bias : 0.058352649211883545  \n",
      "Steps : 3949 , Loss : 0.7830877900123596, Weights 1.0586802959442139, bias : 0.05835117772221565  \n",
      "Steps : 3950 , Loss : 0.7830877900123596, Weights 1.0586820840835571, bias : 0.05834970995783806  \n",
      "Steps : 3951 , Loss : 0.7830877900123596, Weights 1.0586838722229004, bias : 0.05834824591875076  \n",
      "Steps : 3952 , Loss : 0.7830877900123596, Weights 1.0586856603622437, bias : 0.05834678187966347  \n",
      "Steps : 3953 , Loss : 0.7830877900123596, Weights 1.058687448501587, bias : 0.05834532156586647  \n",
      "Steps : 3954 , Loss : 0.7830876708030701, Weights 1.0586892366409302, bias : 0.05834386497735977  \n",
      "Steps : 3955 , Loss : 0.7830877900123596, Weights 1.0586910247802734, bias : 0.05834241211414337  \n",
      "Steps : 3956 , Loss : 0.7830877900123596, Weights 1.0586926937103271, bias : 0.05834095925092697  \n",
      "Steps : 3957 , Loss : 0.7830876708030701, Weights 1.0586943626403809, bias : 0.05833951011300087  \n",
      "Steps : 3958 , Loss : 0.7830877900123596, Weights 1.0586960315704346, bias : 0.058338064700365067  \n",
      "Steps : 3959 , Loss : 0.7830877900123596, Weights 1.0586977005004883, bias : 0.05833661928772926  \n",
      "Steps : 3960 , Loss : 0.7830876708030701, Weights 1.058699369430542, bias : 0.05833517760038376  \n",
      "Steps : 3961 , Loss : 0.7830876708030701, Weights 1.0587010383605957, bias : 0.05833373963832855  \n",
      "Steps : 3962 , Loss : 0.7830877900123596, Weights 1.0587027072906494, bias : 0.058332301676273346  \n",
      "Steps : 3963 , Loss : 0.7830877900123596, Weights 1.0587043762207031, bias : 0.05833086743950844  \n",
      "Steps : 3964 , Loss : 0.7830877900123596, Weights 1.0587060451507568, bias : 0.05832943692803383  \n",
      "Steps : 3965 , Loss : 0.7830876111984253, Weights 1.0587077140808105, bias : 0.05832801014184952  \n",
      "Steps : 3966 , Loss : 0.7830876111984253, Weights 1.0587093830108643, bias : 0.05832658335566521  \n",
      "Steps : 3967 , Loss : 0.7830876111984253, Weights 1.058711051940918, bias : 0.058325160294771194  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 3968 , Loss : 0.7830876111984253, Weights 1.0587127208709717, bias : 0.05832374095916748  \n",
      "Steps : 3969 , Loss : 0.7830876708030701, Weights 1.0587143898010254, bias : 0.058322321623563766  \n",
      "Steps : 3970 , Loss : 0.7830876111984253, Weights 1.058716058731079, bias : 0.05832090601325035  \n",
      "Steps : 3971 , Loss : 0.7830876111984253, Weights 1.0587177276611328, bias : 0.058319494128227234  \n",
      "Steps : 3972 , Loss : 0.7830876111984253, Weights 1.0587193965911865, bias : 0.05831808224320412  \n",
      "Steps : 3973 , Loss : 0.7830876708030701, Weights 1.0587210655212402, bias : 0.0583166740834713  \n",
      "Steps : 3974 , Loss : 0.7830876111984253, Weights 1.058722734451294, bias : 0.05831526964902878  \n",
      "Steps : 3975 , Loss : 0.7830876111984253, Weights 1.0587244033813477, bias : 0.05831386521458626  \n",
      "Steps : 3976 , Loss : 0.7830876111984253, Weights 1.0587260723114014, bias : 0.058312464505434036  \n",
      "Steps : 3977 , Loss : 0.7830876111984253, Weights 1.058727741241455, bias : 0.05831106752157211  \n",
      "Steps : 3978 , Loss : 0.7830876111984253, Weights 1.0587294101715088, bias : 0.05830967426300049  \n",
      "Steps : 3979 , Loss : 0.7830876111984253, Weights 1.0587310791015625, bias : 0.058308281004428864  \n",
      "Steps : 3980 , Loss : 0.7830876111984253, Weights 1.0587327480316162, bias : 0.05830689147114754  \n",
      "Steps : 3981 , Loss : 0.7830876111984253, Weights 1.05873441696167, bias : 0.05830550566315651  \n",
      "Steps : 3982 , Loss : 0.7830876111984253, Weights 1.0587360858917236, bias : 0.05830411985516548  \n",
      "Steps : 3983 , Loss : 0.7830876111984253, Weights 1.0587377548217773, bias : 0.05830273777246475  \n",
      "Steps : 3984 , Loss : 0.7830876111984253, Weights 1.058739423751831, bias : 0.05830135941505432  \n",
      "Steps : 3985 , Loss : 0.7830876111984253, Weights 1.0587410926818848, bias : 0.05829998105764389  \n",
      "Steps : 3986 , Loss : 0.7830876111984253, Weights 1.0587427616119385, bias : 0.05829860642552376  \n",
      "Steps : 3987 , Loss : 0.7830876111984253, Weights 1.0587444305419922, bias : 0.058297235518693924  \n",
      "Steps : 3988 , Loss : 0.7830876111984253, Weights 1.058746099472046, bias : 0.05829586461186409  \n",
      "Steps : 3989 , Loss : 0.7830876111984253, Weights 1.0587477684020996, bias : 0.058294497430324554  \n",
      "Steps : 3990 , Loss : 0.7830876111984253, Weights 1.0587494373321533, bias : 0.05829313397407532  \n",
      "Steps : 3991 , Loss : 0.7830876111984253, Weights 1.058751106262207, bias : 0.05829177051782608  \n",
      "Steps : 3992 , Loss : 0.7830876111984253, Weights 1.0587527751922607, bias : 0.05829041078686714  \n",
      "Steps : 3993 , Loss : 0.7830876111984253, Weights 1.0587544441223145, bias : 0.0582890547811985  \n",
      "Steps : 3994 , Loss : 0.7830875515937805, Weights 1.0587561130523682, bias : 0.05828769877552986  \n",
      "Steps : 3995 , Loss : 0.7830876111984253, Weights 1.0587577819824219, bias : 0.05828634649515152  \n",
      "Steps : 3996 , Loss : 0.7830876111984253, Weights 1.0587594509124756, bias : 0.05828499421477318  \n",
      "Steps : 3997 , Loss : 0.7830875515937805, Weights 1.0587610006332397, bias : 0.058283645659685135  \n",
      "Steps : 3998 , Loss : 0.7830875515937805, Weights 1.058762550354004, bias : 0.05828230082988739  \n",
      "Steps : 3999 , Loss : 0.7830876111984253, Weights 1.058764100074768, bias : 0.058280956000089645  \n",
      "Steps : 4000 , Loss : 0.7830875515937805, Weights 1.0587656497955322, bias : 0.0582796148955822  \n",
      "Steps : 4001 , Loss : 0.7830875515937805, Weights 1.0587671995162964, bias : 0.05827827751636505  \n",
      "Steps : 4002 , Loss : 0.7830874919891357, Weights 1.0587687492370605, bias : 0.0582769401371479  \n",
      "Steps : 4003 , Loss : 0.7830876111984253, Weights 1.0587702989578247, bias : 0.058275606483221054  \n",
      "Steps : 4004 , Loss : 0.7830874919891357, Weights 1.0587718486785889, bias : 0.0582742765545845  \n",
      "Steps : 4005 , Loss : 0.7830874919891357, Weights 1.058773398399353, bias : 0.05827294662594795  \n",
      "Steps : 4006 , Loss : 0.7830874919891357, Weights 1.0587749481201172, bias : 0.0582716204226017  \n",
      "Steps : 4007 , Loss : 0.7830874919891357, Weights 1.0587764978408813, bias : 0.058270297944545746  \n",
      "Steps : 4008 , Loss : 0.7830874919891357, Weights 1.0587780475616455, bias : 0.05826897546648979  \n",
      "Steps : 4009 , Loss : 0.7830874919891357, Weights 1.0587795972824097, bias : 0.058267656713724136  \n",
      "Steps : 4010 , Loss : 0.7830875515937805, Weights 1.0587811470031738, bias : 0.05826634168624878  \n",
      "Steps : 4011 , Loss : 0.7830875515937805, Weights 1.058782696723938, bias : 0.05826502665877342  \n",
      "Steps : 4012 , Loss : 0.7830875515937805, Weights 1.0587842464447021, bias : 0.058263715356588364  \n",
      "Steps : 4013 , Loss : 0.7830874919891357, Weights 1.0587857961654663, bias : 0.058262407779693604  \n",
      "Steps : 4014 , Loss : 0.7830874919891357, Weights 1.0587873458862305, bias : 0.05826110020279884  \n",
      "Steps : 4015 , Loss : 0.7830874919891357, Weights 1.0587888956069946, bias : 0.05825979635119438  \n",
      "Steps : 4016 , Loss : 0.7830874919891357, Weights 1.0587904453277588, bias : 0.05825849249958992  \n",
      "Steps : 4017 , Loss : 0.7830874919891357, Weights 1.058791995048523, bias : 0.05825719237327576  \n",
      "Steps : 4018 , Loss : 0.7830874919891357, Weights 1.058793544769287, bias : 0.05825589597225189  \n",
      "Steps : 4019 , Loss : 0.7830874919891357, Weights 1.0587950944900513, bias : 0.05825459957122803  \n",
      "Steps : 4020 , Loss : 0.7830874919891357, Weights 1.0587966442108154, bias : 0.05825330689549446  \n",
      "Steps : 4021 , Loss : 0.7830874919891357, Weights 1.0587981939315796, bias : 0.05825201794505119  \n",
      "Steps : 4022 , Loss : 0.7830874919891357, Weights 1.0587997436523438, bias : 0.058250728994607925  \n",
      "Steps : 4023 , Loss : 0.7830874919891357, Weights 1.058801293373108, bias : 0.058249443769454956  \n",
      "Steps : 4024 , Loss : 0.7830874919891357, Weights 1.058802843093872, bias : 0.05824815854430199  \n",
      "Steps : 4025 , Loss : 0.7830874919891357, Weights 1.0588043928146362, bias : 0.058246877044439316  \n",
      "Steps : 4026 , Loss : 0.7830874919891357, Weights 1.0588059425354004, bias : 0.05824559926986694  \n",
      "Steps : 4027 , Loss : 0.7830873727798462, Weights 1.0588074922561646, bias : 0.05824432149529457  \n",
      "Steps : 4028 , Loss : 0.7830873727798462, Weights 1.0588090419769287, bias : 0.0582430474460125  \n",
      "Steps : 4029 , Loss : 0.7830874919891357, Weights 1.0588105916976929, bias : 0.05824177712202072  \n",
      "Steps : 4030 , Loss : 0.7830874919891357, Weights 1.058812141418457, bias : 0.058240506798028946  \n",
      "Steps : 4031 , Loss : 0.7830874919891357, Weights 1.0588136911392212, bias : 0.05823924019932747  \n",
      "Steps : 4032 , Loss : 0.7830874919891357, Weights 1.0588152408599854, bias : 0.05823797360062599  \n",
      "Steps : 4033 , Loss : 0.7830873131752014, Weights 1.0588167905807495, bias : 0.05823671072721481  \n",
      "Steps : 4034 , Loss : 0.7830873131752014, Weights 1.0588183403015137, bias : 0.05823545157909393  \n",
      "Steps : 4035 , Loss : 0.7830873727798462, Weights 1.0588198900222778, bias : 0.05823419243097305  \n",
      "Steps : 4036 , Loss : 0.7830873131752014, Weights 1.058821439743042, bias : 0.05823293700814247  \n",
      "Steps : 4037 , Loss : 0.7830873727798462, Weights 1.0588229894638062, bias : 0.05823168158531189  \n",
      "Steps : 4038 , Loss : 0.7830873727798462, Weights 1.0588245391845703, bias : 0.058230429887771606  \n",
      "Steps : 4039 , Loss : 0.7830873131752014, Weights 1.0588260889053345, bias : 0.05822918191552162  \n",
      "Steps : 4040 , Loss : 0.7830874919891357, Weights 1.0588276386260986, bias : 0.05822793394327164  \n",
      "Steps : 4041 , Loss : 0.7830873131752014, Weights 1.0588290691375732, bias : 0.05822668969631195  \n",
      "Steps : 4042 , Loss : 0.7830873131752014, Weights 1.0588304996490479, bias : 0.058225445449352264  \n",
      "Steps : 4043 , Loss : 0.7830874919891357, Weights 1.0588319301605225, bias : 0.05822420492768288  \n",
      "Steps : 4044 , Loss : 0.7830873131752014, Weights 1.058833360671997, bias : 0.05822296813130379  \n",
      "Steps : 4045 , Loss : 0.7830873727798462, Weights 1.0588347911834717, bias : 0.0582217313349247  \n",
      "Steps : 4046 , Loss : 0.7830873131752014, Weights 1.0588362216949463, bias : 0.05822049826383591  \n",
      "Steps : 4047 , Loss : 0.7830873131752014, Weights 1.058837652206421, bias : 0.058219265192747116  \n",
      "Steps : 4048 , Loss : 0.7830873727798462, Weights 1.0588390827178955, bias : 0.058218035846948624  \n",
      "Steps : 4049 , Loss : 0.7830873131752014, Weights 1.0588405132293701, bias : 0.05821681022644043  \n",
      "Steps : 4050 , Loss : 0.7830873131752014, Weights 1.0588419437408447, bias : 0.058215584605932236  \n",
      "Steps : 4051 , Loss : 0.7830872535705566, Weights 1.0588433742523193, bias : 0.05821436271071434  \n",
      "Steps : 4052 , Loss : 0.7830873131752014, Weights 1.058844804763794, bias : 0.058213140815496445  \n",
      "Steps : 4053 , Loss : 0.7830873131752014, Weights 1.0588462352752686, bias : 0.05821192264556885  \n",
      "Steps : 4054 , Loss : 0.7830873727798462, Weights 1.0588476657867432, bias : 0.05821070820093155  \n",
      "Steps : 4055 , Loss : 0.7830873131752014, Weights 1.0588490962982178, bias : 0.05820949375629425  \n",
      "Steps : 4056 , Loss : 0.7830873131752014, Weights 1.0588505268096924, bias : 0.05820828303694725  \n",
      "Steps : 4057 , Loss : 0.7830873131752014, Weights 1.058851957321167, bias : 0.05820707231760025  \n",
      "Steps : 4058 , Loss : 0.7830873131752014, Weights 1.0588533878326416, bias : 0.05820586532354355  \n",
      "Steps : 4059 , Loss : 0.7830873131752014, Weights 1.0588548183441162, bias : 0.058204662054777145  \n",
      "Steps : 4060 , Loss : 0.7830873131752014, Weights 1.0588562488555908, bias : 0.05820345878601074  \n",
      "Steps : 4061 , Loss : 0.7830873131752014, Weights 1.0588576793670654, bias : 0.05820225924253464  \n",
      "Steps : 4062 , Loss : 0.7830873131752014, Weights 1.05885910987854, bias : 0.05820105969905853  \n",
      "Steps : 4063 , Loss : 0.7830873131752014, Weights 1.0588605403900146, bias : 0.058199863880872726  \n",
      "Steps : 4064 , Loss : 0.7830872535705566, Weights 1.0588619709014893, bias : 0.05819867178797722  \n",
      "Steps : 4065 , Loss : 0.7830873131752014, Weights 1.0588634014129639, bias : 0.05819747969508171  \n",
      "Steps : 4066 , Loss : 0.7830873131752014, Weights 1.0588648319244385, bias : 0.0581962913274765  \n",
      "Steps : 4067 , Loss : 0.7830873131752014, Weights 1.058866262435913, bias : 0.05819510295987129  \n",
      "Steps : 4068 , Loss : 0.7830873131752014, Weights 1.0588676929473877, bias : 0.05819391831755638  \n",
      "Steps : 4069 , Loss : 0.7830872535705566, Weights 1.0588691234588623, bias : 0.05819273367524147  \n",
      "Steps : 4070 , Loss : 0.7830872535705566, Weights 1.058870553970337, bias : 0.05819155275821686  \n",
      "Steps : 4071 , Loss : 0.7830872535705566, Weights 1.0588719844818115, bias : 0.058190375566482544  \n",
      "Steps : 4072 , Loss : 0.7830872535705566, Weights 1.0588734149932861, bias : 0.05818919837474823  \n",
      "Steps : 4073 , Loss : 0.7830873727798462, Weights 1.0588748455047607, bias : 0.058188024908304214  \n",
      "Steps : 4074 , Loss : 0.7830873131752014, Weights 1.0588762760162354, bias : 0.0581868514418602  \n",
      "Steps : 4075 , Loss : 0.7830872535705566, Weights 1.05887770652771, bias : 0.05818568170070648  \n",
      "Steps : 4076 , Loss : 0.7830872535705566, Weights 1.0588791370391846, bias : 0.058184511959552765  \n",
      "Steps : 4077 , Loss : 0.7830872535705566, Weights 1.0588805675506592, bias : 0.058183345943689346  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4078 , Loss : 0.7830872535705566, Weights 1.0588819980621338, bias : 0.058182183653116226  \n",
      "Steps : 4079 , Loss : 0.7830872535705566, Weights 1.0588834285736084, bias : 0.058181021362543106  \n",
      "Steps : 4080 , Loss : 0.7830872535705566, Weights 1.058884859085083, bias : 0.058179862797260284  \n",
      "Steps : 4081 , Loss : 0.7830871343612671, Weights 1.0588862895965576, bias : 0.05817870423197746  \n",
      "Steps : 4082 , Loss : 0.7830872535705566, Weights 1.0588877201080322, bias : 0.05817754939198494  \n",
      "Steps : 4083 , Loss : 0.7830871343612671, Weights 1.0588891506195068, bias : 0.058176394551992416  \n",
      "Steps : 4084 , Loss : 0.7830872535705566, Weights 1.0588905811309814, bias : 0.05817524343729019  \n",
      "Steps : 4085 , Loss : 0.7830872535705566, Weights 1.058892011642456, bias : 0.05817409232258797  \n",
      "Steps : 4086 , Loss : 0.7830872535705566, Weights 1.0588934421539307, bias : 0.05817294493317604  \n",
      "Steps : 4087 , Loss : 0.7830871343612671, Weights 1.0588948726654053, bias : 0.05817180126905441  \n",
      "Steps : 4088 , Loss : 0.7830872535705566, Weights 1.0588963031768799, bias : 0.058170657604932785  \n",
      "Steps : 4089 , Loss : 0.7830872535705566, Weights 1.058897614479065, bias : 0.058169517666101456  \n",
      "Steps : 4090 , Loss : 0.7830871343612671, Weights 1.05889892578125, bias : 0.058168377727270126  \n",
      "Steps : 4091 , Loss : 0.7830872535705566, Weights 1.058900237083435, bias : 0.058167241513729095  \n",
      "Steps : 4092 , Loss : 0.7830872535705566, Weights 1.0589015483856201, bias : 0.058166105300188065  \n",
      "Steps : 4093 , Loss : 0.7830871343612671, Weights 1.0589028596878052, bias : 0.05816497281193733  \n",
      "Steps : 4094 , Loss : 0.7830871343612671, Weights 1.0589041709899902, bias : 0.0581638403236866  \n",
      "Steps : 4095 , Loss : 0.7830871343612671, Weights 1.0589054822921753, bias : 0.058162711560726166  \n",
      "Steps : 4096 , Loss : 0.7830873131752014, Weights 1.0589067935943604, bias : 0.05816158652305603  \n",
      "Steps : 4097 , Loss : 0.7830871343612671, Weights 1.0589081048965454, bias : 0.058160461485385895  \n",
      "Steps : 4098 , Loss : 0.7830871343612671, Weights 1.0589094161987305, bias : 0.05815934017300606  \n",
      "Steps : 4099 , Loss : 0.7830872535705566, Weights 1.0589107275009155, bias : 0.05815821886062622  \n",
      "Steps : 4100 , Loss : 0.7830871343612671, Weights 1.0589120388031006, bias : 0.05815710127353668  \n",
      "Steps : 4101 , Loss : 0.7830871343612671, Weights 1.0589133501052856, bias : 0.058155983686447144  \n",
      "Steps : 4102 , Loss : 0.7830871343612671, Weights 1.0589146614074707, bias : 0.0581548698246479  \n",
      "Steps : 4103 , Loss : 0.7830871343612671, Weights 1.0589159727096558, bias : 0.05815375596284866  \n",
      "Steps : 4104 , Loss : 0.7830871343612671, Weights 1.0589172840118408, bias : 0.05815264582633972  \n",
      "Steps : 4105 , Loss : 0.7830871343612671, Weights 1.0589185953140259, bias : 0.05815153568983078  \n",
      "Steps : 4106 , Loss : 0.7830872535705566, Weights 1.058919906616211, bias : 0.05815042927861214  \n",
      "Steps : 4107 , Loss : 0.7830871343612671, Weights 1.058921217918396, bias : 0.05814932659268379  \n",
      "Steps : 4108 , Loss : 0.7830871343612671, Weights 1.058922529220581, bias : 0.05814822390675545  \n",
      "Steps : 4109 , Loss : 0.7830871343612671, Weights 1.0589238405227661, bias : 0.0581471249461174  \n",
      "Steps : 4110 , Loss : 0.7830870151519775, Weights 1.0589251518249512, bias : 0.058146025985479355  \n",
      "Steps : 4111 , Loss : 0.7830871343612671, Weights 1.0589264631271362, bias : 0.05814493075013161  \n",
      "Steps : 4112 , Loss : 0.7830871343612671, Weights 1.0589277744293213, bias : 0.05814383551478386  \n",
      "Steps : 4113 , Loss : 0.7830871343612671, Weights 1.0589290857315063, bias : 0.05814274400472641  \n",
      "Steps : 4114 , Loss : 0.7830871343612671, Weights 1.0589303970336914, bias : 0.05814165249466896  \n",
      "Steps : 4115 , Loss : 0.7830871343612671, Weights 1.0589317083358765, bias : 0.05814056470990181  \n",
      "Steps : 4116 , Loss : 0.7830870747566223, Weights 1.0589330196380615, bias : 0.05813947692513466  \n",
      "Steps : 4117 , Loss : 0.7830870747566223, Weights 1.0589343309402466, bias : 0.058138392865657806  \n",
      "Steps : 4118 , Loss : 0.7830871343612671, Weights 1.0589356422424316, bias : 0.058137308806180954  \n",
      "Steps : 4119 , Loss : 0.7830871343612671, Weights 1.0589369535446167, bias : 0.0581362284719944  \n",
      "Steps : 4120 , Loss : 0.7830871343612671, Weights 1.0589382648468018, bias : 0.058135148137807846  \n",
      "Steps : 4121 , Loss : 0.7830870747566223, Weights 1.0589395761489868, bias : 0.05813407152891159  \n",
      "Steps : 4122 , Loss : 0.7830871343612671, Weights 1.0589408874511719, bias : 0.058132994920015335  \n",
      "Steps : 4123 , Loss : 0.7830870151519775, Weights 1.058942198753357, bias : 0.05813192203640938  \n",
      "Steps : 4124 , Loss : 0.7830871343612671, Weights 1.058943510055542, bias : 0.05813085287809372  \n",
      "Steps : 4125 , Loss : 0.7830871343612671, Weights 1.058944821357727, bias : 0.05812978371977806  \n",
      "Steps : 4126 , Loss : 0.7830870747566223, Weights 1.058946132659912, bias : 0.0581287182867527  \n",
      "Steps : 4127 , Loss : 0.7830870747566223, Weights 1.0589474439620972, bias : 0.05812765285372734  \n",
      "Steps : 4128 , Loss : 0.7830870747566223, Weights 1.0589487552642822, bias : 0.05812659114599228  \n",
      "Steps : 4129 , Loss : 0.7830870151519775, Weights 1.0589500665664673, bias : 0.05812552943825722  \n",
      "Steps : 4130 , Loss : 0.7830870747566223, Weights 1.0589513778686523, bias : 0.058124471455812454  \n",
      "Steps : 4131 , Loss : 0.7830870747566223, Weights 1.0589526891708374, bias : 0.05812341347336769  \n",
      "Steps : 4132 , Loss : 0.7830871343612671, Weights 1.0589540004730225, bias : 0.058122359216213226  \n",
      "Steps : 4133 , Loss : 0.7830871343612671, Weights 1.0589553117752075, bias : 0.05812130495905876  \n",
      "Steps : 4134 , Loss : 0.7830870151519775, Weights 1.0589566230773926, bias : 0.058120254427194595  \n",
      "Steps : 4135 , Loss : 0.7830870747566223, Weights 1.0589579343795776, bias : 0.05811920389533043  \n",
      "Steps : 4136 , Loss : 0.7830870747566223, Weights 1.0589592456817627, bias : 0.05811815708875656  \n",
      "Steps : 4137 , Loss : 0.7830870151519775, Weights 1.0589605569839478, bias : 0.058117110282182693  \n",
      "Steps : 4138 , Loss : 0.7830870151519775, Weights 1.0589618682861328, bias : 0.058116067200899124  \n",
      "Steps : 4139 , Loss : 0.7830871343612671, Weights 1.0589631795883179, bias : 0.058115024119615555  \n",
      "Steps : 4140 , Loss : 0.7830870747566223, Weights 1.0589643716812134, bias : 0.058113984763622284  \n",
      "Steps : 4141 , Loss : 0.7830870151519775, Weights 1.0589655637741089, bias : 0.05811294540762901  \n",
      "Steps : 4142 , Loss : 0.7830870151519775, Weights 1.0589667558670044, bias : 0.05811190977692604  \n",
      "Steps : 4143 , Loss : 0.7830870747566223, Weights 1.0589679479599, bias : 0.05811087414622307  \n",
      "Steps : 4144 , Loss : 0.7830870151519775, Weights 1.0589691400527954, bias : 0.058109842240810394  \n",
      "Steps : 4145 , Loss : 0.7830870151519775, Weights 1.058970332145691, bias : 0.05810881033539772  \n",
      "Steps : 4146 , Loss : 0.7830870151519775, Weights 1.0589715242385864, bias : 0.058107782155275345  \n",
      "Steps : 4147 , Loss : 0.7830870151519775, Weights 1.058972716331482, bias : 0.05810675397515297  \n",
      "Steps : 4148 , Loss : 0.7830870151519775, Weights 1.0589739084243774, bias : 0.05810572952032089  \n",
      "Steps : 4149 , Loss : 0.7830870151519775, Weights 1.058975100517273, bias : 0.058104705065488815  \n",
      "Steps : 4150 , Loss : 0.7830870151519775, Weights 1.0589762926101685, bias : 0.05810368433594704  \n",
      "Steps : 4151 , Loss : 0.7830870151519775, Weights 1.058977484703064, bias : 0.05810266360640526  \n",
      "Steps : 4152 , Loss : 0.7830870151519775, Weights 1.0589786767959595, bias : 0.05810164660215378  \n",
      "Steps : 4153 , Loss : 0.7830870151519775, Weights 1.058979868888855, bias : 0.0581006295979023  \n",
      "Steps : 4154 , Loss : 0.7830870151519775, Weights 1.0589810609817505, bias : 0.058099616318941116  \n",
      "Steps : 4155 , Loss : 0.7830870151519775, Weights 1.058982253074646, bias : 0.058098603039979935  \n",
      "Steps : 4156 , Loss : 0.7830870747566223, Weights 1.0589834451675415, bias : 0.05809759348630905  \n",
      "Steps : 4157 , Loss : 0.7830870151519775, Weights 1.058984637260437, bias : 0.05809658393263817  \n",
      "Steps : 4158 , Loss : 0.7830870151519775, Weights 1.0589858293533325, bias : 0.058095578104257584  \n",
      "Steps : 4159 , Loss : 0.7830870151519775, Weights 1.058987021446228, bias : 0.058094572275877  \n",
      "Steps : 4160 , Loss : 0.7830870151519775, Weights 1.0589882135391235, bias : 0.05809357017278671  \n",
      "Steps : 4161 , Loss : 0.7830870151519775, Weights 1.058989405632019, bias : 0.058092568069696426  \n",
      "Steps : 4162 , Loss : 0.7830870151519775, Weights 1.0589905977249146, bias : 0.05809156969189644  \n",
      "Steps : 4163 , Loss : 0.7830870151519775, Weights 1.05899178981781, bias : 0.05809057131409645  \n",
      "Steps : 4164 , Loss : 0.7830870151519775, Weights 1.0589929819107056, bias : 0.05808957666158676  \n",
      "Steps : 4165 , Loss : 0.7830870151519775, Weights 1.058994174003601, bias : 0.05808858200907707  \n",
      "Steps : 4166 , Loss : 0.7830870151519775, Weights 1.0589953660964966, bias : 0.05808759108185768  \n",
      "Steps : 4167 , Loss : 0.7830870151519775, Weights 1.058996558189392, bias : 0.05808660015463829  \n",
      "Steps : 4168 , Loss : 0.7830870151519775, Weights 1.0589977502822876, bias : 0.0580856129527092  \n",
      "Steps : 4169 , Loss : 0.7830870151519775, Weights 1.058998942375183, bias : 0.058084625750780106  \n",
      "Steps : 4170 , Loss : 0.7830870151519775, Weights 1.0590001344680786, bias : 0.05808363854885101  \n",
      "Steps : 4171 , Loss : 0.7830870151519775, Weights 1.0590013265609741, bias : 0.05808265507221222  \n",
      "Steps : 4172 , Loss : 0.7830870151519775, Weights 1.0590025186538696, bias : 0.058081671595573425  \n",
      "Steps : 4173 , Loss : 0.7830869555473328, Weights 1.0590037107467651, bias : 0.05808069184422493  \n",
      "Steps : 4174 , Loss : 0.7830869555473328, Weights 1.0590049028396606, bias : 0.058079712092876434  \n",
      "Steps : 4175 , Loss : 0.7830870151519775, Weights 1.0590060949325562, bias : 0.05807873606681824  \n",
      "Steps : 4176 , Loss : 0.7830868363380432, Weights 1.0590072870254517, bias : 0.05807776004076004  \n",
      "Steps : 4177 , Loss : 0.7830870151519775, Weights 1.0590084791183472, bias : 0.05807678773999214  \n",
      "Steps : 4178 , Loss : 0.7830870151519775, Weights 1.0590096712112427, bias : 0.05807581543922424  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4179 , Loss : 0.7830870151519775, Weights 1.0590108633041382, bias : 0.05807484686374664  \n",
      "Steps : 4180 , Loss : 0.7830870151519775, Weights 1.0590120553970337, bias : 0.05807387828826904  \n",
      "Steps : 4181 , Loss : 0.7830869555473328, Weights 1.0590132474899292, bias : 0.05807291343808174  \n",
      "Steps : 4182 , Loss : 0.7830870151519775, Weights 1.0590144395828247, bias : 0.05807194858789444  \n",
      "Steps : 4183 , Loss : 0.7830868363380432, Weights 1.0590156316757202, bias : 0.058070987462997437  \n",
      "Steps : 4184 , Loss : 0.7830869555473328, Weights 1.0590168237686157, bias : 0.05807002633810043  \n",
      "Steps : 4185 , Loss : 0.7830869555473328, Weights 1.0590180158615112, bias : 0.05806906893849373  \n",
      "Steps : 4186 , Loss : 0.7830869555473328, Weights 1.0590192079544067, bias : 0.058068111538887024  \n",
      "Steps : 4187 , Loss : 0.7830869555473328, Weights 1.0590204000473022, bias : 0.05806715786457062  \n",
      "Steps : 4188 , Loss : 0.7830869555473328, Weights 1.0590215921401978, bias : 0.05806620419025421  \n",
      "Steps : 4189 , Loss : 0.7830870151519775, Weights 1.0590227842330933, bias : 0.058065250515937805  \n",
      "Steps : 4190 , Loss : 0.7830869555473328, Weights 1.0590239763259888, bias : 0.0580643005669117  \n",
      "Steps : 4191 , Loss : 0.7830869555473328, Weights 1.0590251684188843, bias : 0.05806335061788559  \n",
      "Steps : 4192 , Loss : 0.7830870151519775, Weights 1.0590263605117798, bias : 0.05806240439414978  \n",
      "Steps : 4193 , Loss : 0.7830870151519775, Weights 1.0590275526046753, bias : 0.05806145817041397  \n",
      "Steps : 4194 , Loss : 0.7830870151519775, Weights 1.0590287446975708, bias : 0.05806051567196846  \n",
      "Steps : 4195 , Loss : 0.7830868363380432, Weights 1.0590299367904663, bias : 0.05805957317352295  \n",
      "Steps : 4196 , Loss : 0.7830869555473328, Weights 1.0590311288833618, bias : 0.05805863440036774  \n",
      "Steps : 4197 , Loss : 0.7830868363380432, Weights 1.0590323209762573, bias : 0.058057695627212524  \n",
      "Steps : 4198 , Loss : 0.7830868363380432, Weights 1.0590333938598633, bias : 0.05805676057934761  \n",
      "Steps : 4199 , Loss : 0.7830868363380432, Weights 1.0590344667434692, bias : 0.0580558255314827  \n",
      "Steps : 4200 , Loss : 0.7830869555473328, Weights 1.0590355396270752, bias : 0.05805489048361778  \n",
      "Steps : 4201 , Loss : 0.7830869555473328, Weights 1.0590366125106812, bias : 0.05805395916104317  \n",
      "Steps : 4202 , Loss : 0.7830868363380432, Weights 1.059037685394287, bias : 0.05805302783846855  \n",
      "Steps : 4203 , Loss : 0.7830868363380432, Weights 1.059038758277893, bias : 0.058052100241184235  \n",
      "Steps : 4204 , Loss : 0.7830868363380432, Weights 1.059039831161499, bias : 0.05805117264389992  \n",
      "Steps : 4205 , Loss : 0.7830870151519775, Weights 1.059040904045105, bias : 0.0580502487719059  \n",
      "Steps : 4206 , Loss : 0.7830868363380432, Weights 1.059041976928711, bias : 0.05804932489991188  \n",
      "Steps : 4207 , Loss : 0.7830869555473328, Weights 1.059043049812317, bias : 0.05804840475320816  \n",
      "Steps : 4208 , Loss : 0.7830868363380432, Weights 1.0590441226959229, bias : 0.05804748460650444  \n",
      "Steps : 4209 , Loss : 0.7830870151519775, Weights 1.0590451955795288, bias : 0.05804656818509102  \n",
      "Steps : 4210 , Loss : 0.7830868363380432, Weights 1.0590462684631348, bias : 0.0580456517636776  \n",
      "Steps : 4211 , Loss : 0.7830869555473328, Weights 1.0590473413467407, bias : 0.058044735342264175  \n",
      "Steps : 4212 , Loss : 0.7830869555473328, Weights 1.0590484142303467, bias : 0.05804382264614105  \n",
      "Steps : 4213 , Loss : 0.7830868363380432, Weights 1.0590494871139526, bias : 0.05804290995001793  \n",
      "Steps : 4214 , Loss : 0.7830868363380432, Weights 1.0590505599975586, bias : 0.058042000979185104  \n",
      "Steps : 4215 , Loss : 0.7830868363380432, Weights 1.0590516328811646, bias : 0.05804109200835228  \n",
      "Steps : 4216 , Loss : 0.7830868363380432, Weights 1.0590527057647705, bias : 0.05804018676280975  \n",
      "Steps : 4217 , Loss : 0.7830868363380432, Weights 1.0590537786483765, bias : 0.05803928151726723  \n",
      "Steps : 4218 , Loss : 0.7830869555473328, Weights 1.0590548515319824, bias : 0.058038379997015  \n",
      "Steps : 4219 , Loss : 0.7830868363380432, Weights 1.0590559244155884, bias : 0.05803747847676277  \n",
      "Steps : 4220 , Loss : 0.7830867767333984, Weights 1.0590569972991943, bias : 0.058036576956510544  \n",
      "Steps : 4221 , Loss : 0.7830868363380432, Weights 1.0590580701828003, bias : 0.058035679161548615  \n",
      "Steps : 4222 , Loss : 0.7830868363380432, Weights 1.0590591430664062, bias : 0.058034781366586685  \n",
      "Steps : 4223 , Loss : 0.7830868363380432, Weights 1.0590602159500122, bias : 0.058033887296915054  \n",
      "Steps : 4224 , Loss : 0.7830867767333984, Weights 1.0590612888336182, bias : 0.05803299322724342  \n",
      "Steps : 4225 , Loss : 0.7830868363380432, Weights 1.0590623617172241, bias : 0.05803210288286209  \n",
      "Steps : 4226 , Loss : 0.7830868363380432, Weights 1.05906343460083, bias : 0.05803121253848076  \n",
      "Steps : 4227 , Loss : 0.7830868363380432, Weights 1.059064507484436, bias : 0.058030322194099426  \n",
      "Steps : 4228 , Loss : 0.7830868363380432, Weights 1.059065580368042, bias : 0.05802943557500839  \n",
      "Steps : 4229 , Loss : 0.7830868363380432, Weights 1.059066653251648, bias : 0.05802854895591736  \n",
      "Steps : 4230 , Loss : 0.7830868363380432, Weights 1.059067726135254, bias : 0.05802766606211662  \n",
      "Steps : 4231 , Loss : 0.7830868363380432, Weights 1.0590687990188599, bias : 0.05802678316831589  \n",
      "Steps : 4232 , Loss : 0.7830868363380432, Weights 1.0590698719024658, bias : 0.05802590399980545  \n",
      "Steps : 4233 , Loss : 0.7830867767333984, Weights 1.0590709447860718, bias : 0.05802502483129501  \n",
      "Steps : 4234 , Loss : 0.7830868363380432, Weights 1.0590720176696777, bias : 0.058024145662784576  \n",
      "Steps : 4235 , Loss : 0.7830868363380432, Weights 1.0590730905532837, bias : 0.05802327021956444  \n",
      "Steps : 4236 , Loss : 0.7830868363380432, Weights 1.0590741634368896, bias : 0.0580223947763443  \n",
      "Steps : 4237 , Loss : 0.7830867767333984, Weights 1.0590752363204956, bias : 0.05802152305841446  \n",
      "Steps : 4238 , Loss : 0.7830868363380432, Weights 1.0590763092041016, bias : 0.05802065134048462  \n",
      "Steps : 4239 , Loss : 0.7830868363380432, Weights 1.0590773820877075, bias : 0.05801978334784508  \n",
      "Steps : 4240 , Loss : 0.7830868363380432, Weights 1.0590784549713135, bias : 0.058018915355205536  \n",
      "Steps : 4241 , Loss : 0.7830867767333984, Weights 1.0590795278549194, bias : 0.058018047362565994  \n",
      "Steps : 4242 , Loss : 0.7830868363380432, Weights 1.0590806007385254, bias : 0.05801718309521675  \n",
      "Steps : 4243 , Loss : 0.7830868363380432, Weights 1.0590816736221313, bias : 0.05801631882786751  \n",
      "Steps : 4244 , Loss : 0.7830867767333984, Weights 1.0590827465057373, bias : 0.05801545828580856  \n",
      "Steps : 4245 , Loss : 0.7830868363380432, Weights 1.0590838193893433, bias : 0.05801459774374962  \n",
      "Steps : 4246 , Loss : 0.7830868363380432, Weights 1.0590848922729492, bias : 0.058013737201690674  \n",
      "Steps : 4247 , Loss : 0.7830868363380432, Weights 1.0590859651565552, bias : 0.05801288038492203  \n",
      "Steps : 4248 , Loss : 0.7830868363380432, Weights 1.0590870380401611, bias : 0.05801202356815338  \n",
      "Steps : 4249 , Loss : 0.7830868363380432, Weights 1.059088110923767, bias : 0.058011170476675034  \n",
      "Steps : 4250 , Loss : 0.7830868363380432, Weights 1.059089183807373, bias : 0.058010317385196686  \n",
      "Steps : 4251 , Loss : 0.7830867767333984, Weights 1.059090256690979, bias : 0.058009468019008636  \n",
      "Steps : 4252 , Loss : 0.7830868363380432, Weights 1.059091329574585, bias : 0.05800861865282059  \n",
      "Steps : 4253 , Loss : 0.7830867767333984, Weights 1.059092402458191, bias : 0.05800776928663254  \n",
      "Steps : 4254 , Loss : 0.7830868363380432, Weights 1.0590934753417969, bias : 0.05800692364573479  \n",
      "Steps : 4255 , Loss : 0.7830868363380432, Weights 1.0590945482254028, bias : 0.058006078004837036  \n",
      "Steps : 4256 , Loss : 0.7830867767333984, Weights 1.0590956211090088, bias : 0.058005236089229584  \n",
      "Steps : 4257 , Loss : 0.7830867171287537, Weights 1.0590966939926147, bias : 0.05800439417362213  \n",
      "Steps : 4258 , Loss : 0.7830868363380432, Weights 1.0590977668762207, bias : 0.05800355225801468  \n",
      "Steps : 4259 , Loss : 0.7830868363380432, Weights 1.0590988397598267, bias : 0.058002714067697525  \n",
      "Steps : 4260 , Loss : 0.7830868363380432, Weights 1.0590999126434326, bias : 0.05800187587738037  \n",
      "Steps : 4261 , Loss : 0.7830868363380432, Weights 1.059100866317749, bias : 0.058001041412353516  \n",
      "Steps : 4262 , Loss : 0.7830868363380432, Weights 1.0591018199920654, bias : 0.05800020694732666  \n",
      "Steps : 4263 , Loss : 0.7830867767333984, Weights 1.0591027736663818, bias : 0.057999372482299805  \n",
      "Steps : 4264 , Loss : 0.7830867767333984, Weights 1.0591037273406982, bias : 0.05799854174256325  \n",
      "Steps : 4265 , Loss : 0.7830867171287537, Weights 1.0591046810150146, bias : 0.05799771100282669  \n",
      "Steps : 4266 , Loss : 0.7830867767333984, Weights 1.059105634689331, bias : 0.05799688398838043  \n",
      "Steps : 4267 , Loss : 0.7830867767333984, Weights 1.0591065883636475, bias : 0.057996056973934174  \n",
      "Steps : 4268 , Loss : 0.7830867171287537, Weights 1.0591075420379639, bias : 0.057995229959487915  \n",
      "Steps : 4269 , Loss : 0.7830867171287537, Weights 1.0591084957122803, bias : 0.057994406670331955  \n",
      "Steps : 4270 , Loss : 0.7830867171287537, Weights 1.0591094493865967, bias : 0.057993583381175995  \n",
      "Steps : 4271 , Loss : 0.7830868363380432, Weights 1.059110403060913, bias : 0.05799276381731033  \n",
      "Steps : 4272 , Loss : 0.7830867767333984, Weights 1.0591113567352295, bias : 0.05799194425344467  \n",
      "Steps : 4273 , Loss : 0.7830867767333984, Weights 1.059112310409546, bias : 0.05799112468957901  \n",
      "Steps : 4274 , Loss : 0.7830867171287537, Weights 1.0591132640838623, bias : 0.05799030885100365  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4275 , Loss : 0.7830867171287537, Weights 1.0591142177581787, bias : 0.057989493012428284  \n",
      "Steps : 4276 , Loss : 0.7830867767333984, Weights 1.0591151714324951, bias : 0.05798868089914322  \n",
      "Steps : 4277 , Loss : 0.7830867767333984, Weights 1.0591161251068115, bias : 0.057987868785858154  \n",
      "Steps : 4278 , Loss : 0.7830868363380432, Weights 1.059117078781128, bias : 0.05798705667257309  \n",
      "Steps : 4279 , Loss : 0.7830867171287537, Weights 1.0591180324554443, bias : 0.05798624828457832  \n",
      "Steps : 4280 , Loss : 0.7830867767333984, Weights 1.0591189861297607, bias : 0.05798543989658356  \n",
      "Steps : 4281 , Loss : 0.7830867171287537, Weights 1.0591199398040771, bias : 0.05798463523387909  \n",
      "Steps : 4282 , Loss : 0.7830867767333984, Weights 1.0591208934783936, bias : 0.05798383057117462  \n",
      "Steps : 4283 , Loss : 0.7830867767333984, Weights 1.05912184715271, bias : 0.057983025908470154  \n",
      "Steps : 4284 , Loss : 0.7830867171287537, Weights 1.0591228008270264, bias : 0.057982224971055984  \n",
      "Steps : 4285 , Loss : 0.7830867171287537, Weights 1.0591237545013428, bias : 0.057981424033641815  \n",
      "Steps : 4286 , Loss : 0.7830867171287537, Weights 1.0591247081756592, bias : 0.057980626821517944  \n",
      "Steps : 4287 , Loss : 0.7830868363380432, Weights 1.0591256618499756, bias : 0.057979829609394073  \n",
      "Steps : 4288 , Loss : 0.7830867767333984, Weights 1.059126615524292, bias : 0.0579790323972702  \n",
      "Steps : 4289 , Loss : 0.7830867767333984, Weights 1.0591275691986084, bias : 0.05797823891043663  \n",
      "Steps : 4290 , Loss : 0.7830867767333984, Weights 1.0591285228729248, bias : 0.05797744542360306  \n",
      "Steps : 4291 , Loss : 0.7830867767333984, Weights 1.0591294765472412, bias : 0.057976655662059784  \n",
      "Steps : 4292 , Loss : 0.7830867171287537, Weights 1.0591304302215576, bias : 0.05797586590051651  \n",
      "Steps : 4293 , Loss : 0.7830867171287537, Weights 1.059131383895874, bias : 0.057975076138973236  \n",
      "Steps : 4294 , Loss : 0.7830867767333984, Weights 1.0591323375701904, bias : 0.05797429010272026  \n",
      "Steps : 4295 , Loss : 0.7830867171287537, Weights 1.0591332912445068, bias : 0.057973504066467285  \n",
      "Steps : 4296 , Loss : 0.7830867171287537, Weights 1.0591342449188232, bias : 0.05797271803021431  \n",
      "Steps : 4297 , Loss : 0.7830867767333984, Weights 1.0591351985931396, bias : 0.05797193571925163  \n",
      "Steps : 4298 , Loss : 0.7830867767333984, Weights 1.059136152267456, bias : 0.057971153408288956  \n",
      "Steps : 4299 , Loss : 0.7830867171287537, Weights 1.0591371059417725, bias : 0.05797037482261658  \n",
      "Steps : 4300 , Loss : 0.7830867767333984, Weights 1.0591380596160889, bias : 0.0579695962369442  \n",
      "Steps : 4301 , Loss : 0.7830867171287537, Weights 1.0591390132904053, bias : 0.05796881765127182  \n",
      "Steps : 4302 , Loss : 0.7830867767333984, Weights 1.0591399669647217, bias : 0.05796804279088974  \n",
      "Steps : 4303 , Loss : 0.7830867171287537, Weights 1.059140920639038, bias : 0.05796726793050766  \n",
      "Steps : 4304 , Loss : 0.7830867171287537, Weights 1.0591418743133545, bias : 0.05796649307012558  \n",
      "Steps : 4305 , Loss : 0.7830867171287537, Weights 1.059142827987671, bias : 0.0579657219350338  \n",
      "Steps : 4306 , Loss : 0.7830867171287537, Weights 1.0591437816619873, bias : 0.05796495079994202  \n",
      "Steps : 4307 , Loss : 0.7830867171287537, Weights 1.0591447353363037, bias : 0.05796418339014053  \n",
      "Steps : 4308 , Loss : 0.7830867171287537, Weights 1.0591456890106201, bias : 0.05796341598033905  \n",
      "Steps : 4309 , Loss : 0.7830867171287537, Weights 1.0591466426849365, bias : 0.05796264857053757  \n",
      "Steps : 4310 , Loss : 0.7830867171287537, Weights 1.059147596359253, bias : 0.05796188488602638  \n",
      "Steps : 4311 , Loss : 0.7830867171287537, Weights 1.0591485500335693, bias : 0.0579611212015152  \n",
      "Steps : 4312 , Loss : 0.7830867171287537, Weights 1.0591495037078857, bias : 0.05796035751700401  \n",
      "Steps : 4313 , Loss : 0.7830867171287537, Weights 1.0591504573822021, bias : 0.05795959755778313  \n",
      "Steps : 4314 , Loss : 0.7830867171287537, Weights 1.0591514110565186, bias : 0.05795883759856224  \n",
      "Steps : 4315 , Loss : 0.7830867171287537, Weights 1.059152364730835, bias : 0.05795808136463165  \n",
      "Steps : 4316 , Loss : 0.7830867171287537, Weights 1.0591533184051514, bias : 0.057957325130701065  \n",
      "Steps : 4317 , Loss : 0.7830867171287537, Weights 1.0591542720794678, bias : 0.05795656889677048  \n",
      "Steps : 4318 , Loss : 0.7830867171287537, Weights 1.0591552257537842, bias : 0.05795581638813019  \n",
      "Steps : 4319 , Loss : 0.7830867171287537, Weights 1.0591561794281006, bias : 0.0579550638794899  \n",
      "Steps : 4320 , Loss : 0.7830867171287537, Weights 1.059157133102417, bias : 0.05795431137084961  \n",
      "Steps : 4321 , Loss : 0.7830867171287537, Weights 1.0591580867767334, bias : 0.05795356258749962  \n",
      "Steps : 4322 , Loss : 0.7830867171287537, Weights 1.0591590404510498, bias : 0.05795281380414963  \n",
      "Steps : 4323 , Loss : 0.7830867171287537, Weights 1.0591599941253662, bias : 0.05795206502079964  \n",
      "Steps : 4324 , Loss : 0.7830867171287537, Weights 1.0591609477996826, bias : 0.057951319962739944  \n",
      "Steps : 4325 , Loss : 0.7830867171287537, Weights 1.059161901473999, bias : 0.05795057490468025  \n",
      "Steps : 4326 , Loss : 0.7830865979194641, Weights 1.0591628551483154, bias : 0.05794983357191086  \n",
      "Steps : 4327 , Loss : 0.7830867767333984, Weights 1.0591638088226318, bias : 0.057949092239141464  \n",
      "Steps : 4328 , Loss : 0.7830867171287537, Weights 1.0591647624969482, bias : 0.05794835090637207  \n",
      "Steps : 4329 , Loss : 0.7830867171287537, Weights 1.0591657161712646, bias : 0.057947613298892975  \n",
      "Steps : 4330 , Loss : 0.7830867171287537, Weights 1.059166669845581, bias : 0.05794687569141388  \n",
      "Steps : 4331 , Loss : 0.7830867171287537, Weights 1.0591676235198975, bias : 0.057946138083934784  \n",
      "Steps : 4332 , Loss : 0.7830867171287537, Weights 1.0591684579849243, bias : 0.05794540420174599  \n",
      "Steps : 4333 , Loss : 0.7830867171287537, Weights 1.0591692924499512, bias : 0.05794467031955719  \n",
      "Steps : 4334 , Loss : 0.7830867171287537, Weights 1.059170126914978, bias : 0.05794393643736839  \n",
      "Steps : 4335 , Loss : 0.7830865979194641, Weights 1.0591709613800049, bias : 0.057943206280469894  \n",
      "Steps : 4336 , Loss : 0.7830867171287537, Weights 1.0591717958450317, bias : 0.057942476123571396  \n",
      "Steps : 4337 , Loss : 0.7830867171287537, Weights 1.0591726303100586, bias : 0.0579417459666729  \n",
      "Steps : 4338 , Loss : 0.7830865979194641, Weights 1.0591734647750854, bias : 0.0579410195350647  \n",
      "Steps : 4339 , Loss : 0.7830867171287537, Weights 1.0591742992401123, bias : 0.0579402931034565  \n",
      "Steps : 4340 , Loss : 0.7830867171287537, Weights 1.0591751337051392, bias : 0.057939570397138596  \n",
      "Steps : 4341 , Loss : 0.7830865979194641, Weights 1.059175968170166, bias : 0.057938847690820694  \n",
      "Steps : 4342 , Loss : 0.7830867171287537, Weights 1.0591768026351929, bias : 0.05793812498450279  \n",
      "Steps : 4343 , Loss : 0.7830867171287537, Weights 1.0591776371002197, bias : 0.05793740600347519  \n",
      "Steps : 4344 , Loss : 0.7830867171287537, Weights 1.0591784715652466, bias : 0.057936687022447586  \n",
      "Steps : 4345 , Loss : 0.7830867171287537, Weights 1.0591793060302734, bias : 0.05793596804141998  \n",
      "Steps : 4346 , Loss : 0.7830867171287537, Weights 1.0591801404953003, bias : 0.05793525278568268  \n",
      "Steps : 4347 , Loss : 0.7830867171287537, Weights 1.0591809749603271, bias : 0.057934537529945374  \n",
      "Steps : 4348 , Loss : 0.7830865979194641, Weights 1.059181809425354, bias : 0.05793382227420807  \n",
      "Steps : 4349 , Loss : 0.7830867171287537, Weights 1.0591826438903809, bias : 0.05793311074376106  \n",
      "Steps : 4350 , Loss : 0.7830867171287537, Weights 1.0591834783554077, bias : 0.057932399213314056  \n",
      "Steps : 4351 , Loss : 0.7830867171287537, Weights 1.0591843128204346, bias : 0.05793168768286705  \n",
      "Steps : 4352 , Loss : 0.7830867171287537, Weights 1.0591851472854614, bias : 0.05793097987771034  \n",
      "Steps : 4353 , Loss : 0.7830867171287537, Weights 1.0591859817504883, bias : 0.057930272072553635  \n",
      "Steps : 4354 , Loss : 0.7830865979194641, Weights 1.0591868162155151, bias : 0.05792956426739693  \n",
      "Steps : 4355 , Loss : 0.7830865979194641, Weights 1.059187650680542, bias : 0.05792886018753052  \n",
      "Steps : 4356 , Loss : 0.7830867171287537, Weights 1.0591884851455688, bias : 0.05792815610766411  \n",
      "Steps : 4357 , Loss : 0.7830867171287537, Weights 1.0591893196105957, bias : 0.0579274520277977  \n",
      "Steps : 4358 , Loss : 0.7830865979194641, Weights 1.0591901540756226, bias : 0.05792675167322159  \n",
      "Steps : 4359 , Loss : 0.7830865979194641, Weights 1.0591909885406494, bias : 0.05792605131864548  \n",
      "Steps : 4360 , Loss : 0.7830867171287537, Weights 1.0591918230056763, bias : 0.057925350964069366  \n",
      "Steps : 4361 , Loss : 0.7830865383148193, Weights 1.0591926574707031, bias : 0.057924654334783554  \n",
      "Steps : 4362 , Loss : 0.7830867171287537, Weights 1.05919349193573, bias : 0.05792395770549774  \n",
      "Steps : 4363 , Loss : 0.7830867171287537, Weights 1.0591943264007568, bias : 0.05792326107621193  \n",
      "Steps : 4364 , Loss : 0.7830865979194641, Weights 1.0591951608657837, bias : 0.057922568172216415  \n",
      "Steps : 4365 , Loss : 0.7830865979194641, Weights 1.0591959953308105, bias : 0.0579218752682209  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4366 , Loss : 0.7830865979194641, Weights 1.0591968297958374, bias : 0.05792118236422539  \n",
      "Steps : 4367 , Loss : 0.7830867171287537, Weights 1.0591976642608643, bias : 0.05792049318552017  \n",
      "Steps : 4368 , Loss : 0.7830865383148193, Weights 1.0591984987258911, bias : 0.05791980400681496  \n",
      "Steps : 4369 , Loss : 0.7830867171287537, Weights 1.059199333190918, bias : 0.05791911482810974  \n",
      "Steps : 4370 , Loss : 0.7830867171287537, Weights 1.0592001676559448, bias : 0.057918429374694824  \n",
      "Steps : 4371 , Loss : 0.7830867171287537, Weights 1.0592010021209717, bias : 0.05791774392127991  \n",
      "Steps : 4372 , Loss : 0.7830865979194641, Weights 1.0592018365859985, bias : 0.05791705846786499  \n",
      "Steps : 4373 , Loss : 0.7830867171287537, Weights 1.0592026710510254, bias : 0.05791637673974037  \n",
      "Steps : 4374 , Loss : 0.7830865383148193, Weights 1.0592035055160522, bias : 0.05791569501161575  \n",
      "Steps : 4375 , Loss : 0.7830865979194641, Weights 1.059204339981079, bias : 0.057915013283491135  \n",
      "Steps : 4376 , Loss : 0.7830865979194641, Weights 1.059205174446106, bias : 0.057914335280656815  \n",
      "Steps : 4377 , Loss : 0.7830867171287537, Weights 1.0592060089111328, bias : 0.057913657277822495  \n",
      "Steps : 4378 , Loss : 0.7830865383148193, Weights 1.0592068433761597, bias : 0.057912979274988174  \n",
      "Steps : 4379 , Loss : 0.7830867171287537, Weights 1.0592076778411865, bias : 0.05791230499744415  \n",
      "Steps : 4380 , Loss : 0.7830867171287537, Weights 1.0592085123062134, bias : 0.05791163071990013  \n",
      "Steps : 4381 , Loss : 0.7830867171287537, Weights 1.0592093467712402, bias : 0.05791095644235611  \n",
      "Steps : 4382 , Loss : 0.7830865979194641, Weights 1.059210181236267, bias : 0.057910285890102386  \n",
      "Steps : 4383 , Loss : 0.7830867171287537, Weights 1.059211015701294, bias : 0.05790961533784866  \n",
      "Steps : 4384 , Loss : 0.7830865979194641, Weights 1.0592118501663208, bias : 0.05790894478559494  \n",
      "Steps : 4385 , Loss : 0.7830865383148193, Weights 1.0592126846313477, bias : 0.057908277958631516  \n",
      "Steps : 4386 , Loss : 0.7830867171287537, Weights 1.0592135190963745, bias : 0.05790761113166809  \n",
      "Steps : 4387 , Loss : 0.7830865979194641, Weights 1.0592143535614014, bias : 0.057906944304704666  \n",
      "Steps : 4388 , Loss : 0.7830865383148193, Weights 1.0592151880264282, bias : 0.05790628120303154  \n",
      "Steps : 4389 , Loss : 0.7830865383148193, Weights 1.059216022491455, bias : 0.057905618101358414  \n",
      "Steps : 4390 , Loss : 0.7830865979194641, Weights 1.059216856956482, bias : 0.05790495499968529  \n",
      "Steps : 4391 , Loss : 0.7830865383148193, Weights 1.0592176914215088, bias : 0.05790429562330246  \n",
      "Steps : 4392 , Loss : 0.7830865383148193, Weights 1.0592185258865356, bias : 0.05790363624691963  \n",
      "Steps : 4393 , Loss : 0.7830865383148193, Weights 1.0592193603515625, bias : 0.057902976870536804  \n",
      "Steps : 4394 , Loss : 0.7830865979194641, Weights 1.0592201948165894, bias : 0.057902321219444275  \n",
      "Steps : 4395 , Loss : 0.7830865979194641, Weights 1.0592210292816162, bias : 0.057901665568351746  \n",
      "Steps : 4396 , Loss : 0.7830865979194641, Weights 1.059221863746643, bias : 0.057901009917259216  \n",
      "Steps : 4397 , Loss : 0.7830865383148193, Weights 1.05922269821167, bias : 0.057900357991456985  \n",
      "Steps : 4398 , Loss : 0.7830865979194641, Weights 1.0592235326766968, bias : 0.057899706065654755  \n",
      "Steps : 4399 , Loss : 0.7830865979194641, Weights 1.0592243671417236, bias : 0.057899054139852524  \n",
      "Steps : 4400 , Loss : 0.7830865979194641, Weights 1.0592252016067505, bias : 0.05789840593934059  \n",
      "Steps : 4401 , Loss : 0.7830865979194641, Weights 1.0592260360717773, bias : 0.05789775773882866  \n",
      "Steps : 4402 , Loss : 0.7830865383148193, Weights 1.0592268705368042, bias : 0.05789710953831673  \n",
      "Steps : 4403 , Loss : 0.7830865383148193, Weights 1.059227705001831, bias : 0.057896461337804794  \n",
      "Steps : 4404 , Loss : 0.7830865979194641, Weights 1.059228539466858, bias : 0.05789581686258316  \n",
      "Steps : 4405 , Loss : 0.7830865979194641, Weights 1.0592293739318848, bias : 0.057895172387361526  \n",
      "Steps : 4406 , Loss : 0.7830865979194641, Weights 1.0592302083969116, bias : 0.05789452791213989  \n",
      "Steps : 4407 , Loss : 0.7830865979194641, Weights 1.0592310428619385, bias : 0.05789388716220856  \n",
      "Steps : 4408 , Loss : 0.7830867171287537, Weights 1.0592318773269653, bias : 0.05789324641227722  \n",
      "Steps : 4409 , Loss : 0.7830865383148193, Weights 1.0592327117919922, bias : 0.057892605662345886  \n",
      "Steps : 4410 , Loss : 0.7830865383148193, Weights 1.059233546257019, bias : 0.05789196863770485  \n",
      "Steps : 4411 , Loss : 0.7830865383148193, Weights 1.059234380722046, bias : 0.05789133161306381  \n",
      "Steps : 4412 , Loss : 0.7830865383148193, Weights 1.0592352151870728, bias : 0.057890694588422775  \n",
      "Steps : 4413 , Loss : 0.7830865383148193, Weights 1.0592360496520996, bias : 0.05789006128907204  \n",
      "Steps : 4414 , Loss : 0.7830865979194641, Weights 1.059236764907837, bias : 0.0578894279897213  \n",
      "Steps : 4415 , Loss : 0.7830865383148193, Weights 1.0592374801635742, bias : 0.05788879469037056  \n",
      "Steps : 4416 , Loss : 0.7830865383148193, Weights 1.0592381954193115, bias : 0.05788816139101982  \n",
      "Steps : 4417 , Loss : 0.7830865979194641, Weights 1.0592389106750488, bias : 0.05788753181695938  \n",
      "Steps : 4418 , Loss : 0.7830865383148193, Weights 1.0592396259307861, bias : 0.05788690224289894  \n",
      "Steps : 4419 , Loss : 0.7830865979194641, Weights 1.0592403411865234, bias : 0.0578862726688385  \n",
      "Steps : 4420 , Loss : 0.7830865383148193, Weights 1.0592410564422607, bias : 0.05788564682006836  \n",
      "Steps : 4421 , Loss : 0.7830865383148193, Weights 1.059241771697998, bias : 0.05788502097129822  \n",
      "Steps : 4422 , Loss : 0.7830865383148193, Weights 1.0592424869537354, bias : 0.057884395122528076  \n",
      "Steps : 4423 , Loss : 0.7830865979194641, Weights 1.0592432022094727, bias : 0.05788377299904823  \n",
      "Steps : 4424 , Loss : 0.7830865383148193, Weights 1.05924391746521, bias : 0.05788315087556839  \n",
      "Steps : 4425 , Loss : 0.7830865383148193, Weights 1.0592446327209473, bias : 0.05788252875208855  \n",
      "Steps : 4426 , Loss : 0.7830864787101746, Weights 1.0592453479766846, bias : 0.057881910353899  \n",
      "Steps : 4427 , Loss : 0.7830865383148193, Weights 1.0592460632324219, bias : 0.05788129195570946  \n",
      "Steps : 4428 , Loss : 0.7830865383148193, Weights 1.0592467784881592, bias : 0.05788067355751991  \n",
      "Steps : 4429 , Loss : 0.7830865383148193, Weights 1.0592474937438965, bias : 0.05788005515933037  \n",
      "Steps : 4430 , Loss : 0.7830865383148193, Weights 1.0592482089996338, bias : 0.05787944048643112  \n",
      "Steps : 4431 , Loss : 0.7830865383148193, Weights 1.059248924255371, bias : 0.057878825813531876  \n",
      "Steps : 4432 , Loss : 0.7830865383148193, Weights 1.0592496395111084, bias : 0.05787821114063263  \n",
      "Steps : 4433 , Loss : 0.7830865979194641, Weights 1.0592503547668457, bias : 0.05787760019302368  \n",
      "Steps : 4434 , Loss : 0.7830865383148193, Weights 1.059251070022583, bias : 0.057876989245414734  \n",
      "Steps : 4435 , Loss : 0.7830865383148193, Weights 1.0592517852783203, bias : 0.057876378297805786  \n",
      "Steps : 4436 , Loss : 0.7830865383148193, Weights 1.0592525005340576, bias : 0.05787577107548714  \n",
      "Steps : 4437 , Loss : 0.7830865979194641, Weights 1.059253215789795, bias : 0.05787516385316849  \n",
      "Steps : 4438 , Loss : 0.7830865383148193, Weights 1.0592539310455322, bias : 0.05787455663084984  \n",
      "Steps : 4439 , Loss : 0.7830865383148193, Weights 1.0592546463012695, bias : 0.05787394940853119  \n",
      "Steps : 4440 , Loss : 0.7830865979194641, Weights 1.0592553615570068, bias : 0.05787334591150284  \n",
      "Steps : 4441 , Loss : 0.7830865383148193, Weights 1.0592560768127441, bias : 0.05787274241447449  \n",
      "Steps : 4442 , Loss : 0.7830865383148193, Weights 1.0592567920684814, bias : 0.057872138917446136  \n",
      "Steps : 4443 , Loss : 0.7830865383148193, Weights 1.0592575073242188, bias : 0.057871539145708084  \n",
      "Steps : 4444 , Loss : 0.7830865383148193, Weights 1.059258222579956, bias : 0.05787093937397003  \n",
      "Steps : 4445 , Loss : 0.7830865383148193, Weights 1.0592589378356934, bias : 0.05787033960223198  \n",
      "Steps : 4446 , Loss : 0.7830865383148193, Weights 1.0592596530914307, bias : 0.05786973983049393  \n",
      "Steps : 4447 , Loss : 0.7830865383148193, Weights 1.059260368347168, bias : 0.05786914378404617  \n",
      "Steps : 4448 , Loss : 0.7830865383148193, Weights 1.0592610836029053, bias : 0.05786854773759842  \n",
      "Steps : 4449 , Loss : 0.7830865383148193, Weights 1.0592617988586426, bias : 0.057867951691150665  \n",
      "Steps : 4450 , Loss : 0.7830865383148193, Weights 1.0592625141143799, bias : 0.05786735936999321  \n",
      "Steps : 4451 , Loss : 0.7830865383148193, Weights 1.0592632293701172, bias : 0.057866767048835754  \n",
      "Steps : 4452 , Loss : 0.7830865383148193, Weights 1.0592639446258545, bias : 0.0578661747276783  \n",
      "Steps : 4453 , Loss : 0.7830865979194641, Weights 1.0592646598815918, bias : 0.057865582406520844  \n",
      "Steps : 4454 , Loss : 0.7830865383148193, Weights 1.059265375137329, bias : 0.057864993810653687  \n",
      "Steps : 4455 , Loss : 0.7830865383148193, Weights 1.0592660903930664, bias : 0.05786440521478653  \n",
      "Steps : 4456 , Loss : 0.7830865979194641, Weights 1.0592668056488037, bias : 0.05786381661891937  \n",
      "Steps : 4457 , Loss : 0.7830865383148193, Weights 1.059267520904541, bias : 0.057863231748342514  \n",
      "Steps : 4458 , Loss : 0.7830865383148193, Weights 1.0592682361602783, bias : 0.057862646877765656  \n",
      "Steps : 4459 , Loss : 0.7830865383148193, Weights 1.0592689514160156, bias : 0.0578620620071888  \n",
      "Steps : 4460 , Loss : 0.7830865979194641, Weights 1.059269666671753, bias : 0.05786147713661194  \n",
      "Steps : 4461 , Loss : 0.7830865383148193, Weights 1.0592703819274902, bias : 0.05786089599132538  \n",
      "Steps : 4462 , Loss : 0.7830865383148193, Weights 1.0592710971832275, bias : 0.05786031484603882  \n",
      "Steps : 4463 , Loss : 0.7830865383148193, Weights 1.0592718124389648, bias : 0.05785973370075226  \n",
      "Steps : 4464 , Loss : 0.7830865383148193, Weights 1.0592725276947021, bias : 0.057859156280756  \n",
      "Steps : 4465 , Loss : 0.7830865383148193, Weights 1.0592732429504395, bias : 0.057858578860759735  \n",
      "Steps : 4466 , Loss : 0.7830865979194641, Weights 1.0592739582061768, bias : 0.057858001440763474  \n",
      "Steps : 4467 , Loss : 0.7830865383148193, Weights 1.059274673461914, bias : 0.05785742402076721  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4468 , Loss : 0.7830865383148193, Weights 1.0592753887176514, bias : 0.05785685032606125  \n",
      "Steps : 4469 , Loss : 0.7830865383148193, Weights 1.0592761039733887, bias : 0.057856276631355286  \n",
      "Steps : 4470 , Loss : 0.7830865383148193, Weights 1.059276819229126, bias : 0.05785570293664932  \n",
      "Steps : 4471 , Loss : 0.7830865383148193, Weights 1.0592775344848633, bias : 0.05785513296723366  \n",
      "Steps : 4472 , Loss : 0.7830865383148193, Weights 1.0592782497406006, bias : 0.05785456299781799  \n",
      "Steps : 4473 , Loss : 0.7830865383148193, Weights 1.059278964996338, bias : 0.05785399302840233  \n",
      "Steps : 4474 , Loss : 0.7830865383148193, Weights 1.0592796802520752, bias : 0.057853423058986664  \n",
      "Steps : 4475 , Loss : 0.7830864787101746, Weights 1.0592803955078125, bias : 0.0578528568148613  \n",
      "Steps : 4476 , Loss : 0.7830865383148193, Weights 1.0592811107635498, bias : 0.05785229057073593  \n",
      "Steps : 4477 , Loss : 0.7830865383148193, Weights 1.059281826019287, bias : 0.057851724326610565  \n",
      "Steps : 4478 , Loss : 0.7830865383148193, Weights 1.0592825412750244, bias : 0.0578511580824852  \n",
      "Steps : 4479 , Loss : 0.7830865383148193, Weights 1.0592832565307617, bias : 0.05785059556365013  \n",
      "Steps : 4480 , Loss : 0.7830865383148193, Weights 1.059283971786499, bias : 0.057850033044815063  \n",
      "Steps : 4481 , Loss : 0.7830865383148193, Weights 1.0592846870422363, bias : 0.057849470525979996  \n",
      "Steps : 4482 , Loss : 0.7830865383148193, Weights 1.0592854022979736, bias : 0.057848911732435226  \n",
      "Steps : 4483 , Loss : 0.7830865383148193, Weights 1.059286117553711, bias : 0.05784835293889046  \n",
      "Steps : 4484 , Loss : 0.7830865383148193, Weights 1.0592868328094482, bias : 0.05784779414534569  \n",
      "Steps : 4485 , Loss : 0.7830865383148193, Weights 1.0592875480651855, bias : 0.05784723535180092  \n",
      "Steps : 4486 , Loss : 0.7830865383148193, Weights 1.0592882633209229, bias : 0.05784668028354645  \n",
      "Steps : 4487 , Loss : 0.7830865383148193, Weights 1.0592889785766602, bias : 0.05784612521529198  \n",
      "Steps : 4488 , Loss : 0.7830865383148193, Weights 1.0592896938323975, bias : 0.057845570147037506  \n",
      "Steps : 4489 , Loss : 0.7830865383148193, Weights 1.0592904090881348, bias : 0.057845015078783035  \n",
      "Steps : 4490 , Loss : 0.7830864787101746, Weights 1.059291124343872, bias : 0.05784446373581886  \n",
      "Steps : 4491 , Loss : 0.7830864787101746, Weights 1.0592918395996094, bias : 0.05784391239285469  \n",
      "Steps : 4492 , Loss : 0.7830865383148193, Weights 1.0592925548553467, bias : 0.05784336104989052  \n",
      "Steps : 4493 , Loss : 0.7830864787101746, Weights 1.059293270111084, bias : 0.057842813432216644  \n",
      "Steps : 4494 , Loss : 0.7830865383148193, Weights 1.0592939853668213, bias : 0.05784226581454277  \n",
      "Steps : 4495 , Loss : 0.7830865383148193, Weights 1.0592947006225586, bias : 0.057841718196868896  \n",
      "Steps : 4496 , Loss : 0.7830864191055298, Weights 1.059295415878296, bias : 0.05784117057919502  \n",
      "Steps : 4497 , Loss : 0.7830865383148193, Weights 1.0592961311340332, bias : 0.05784062668681145  \n",
      "Steps : 4498 , Loss : 0.7830865383148193, Weights 1.0592968463897705, bias : 0.05784008279442787  \n",
      "Steps : 4499 , Loss : 0.7830864191055298, Weights 1.0592975616455078, bias : 0.057839538902044296  \n",
      "Steps : 4500 , Loss : 0.7830865383148193, Weights 1.0592982769012451, bias : 0.05783899500966072  \n",
      "Steps : 4501 , Loss : 0.7830865383148193, Weights 1.0592989921569824, bias : 0.057838454842567444  \n",
      "Steps : 4502 , Loss : 0.7830865383148193, Weights 1.0592997074127197, bias : 0.05783791467547417  \n",
      "Steps : 4503 , Loss : 0.7830865383148193, Weights 1.059300422668457, bias : 0.05783737450838089  \n",
      "Steps : 4504 , Loss : 0.7830865383148193, Weights 1.0593011379241943, bias : 0.05783683434128761  \n",
      "Steps : 4505 , Loss : 0.7830865383148193, Weights 1.0593018531799316, bias : 0.057836297899484634  \n",
      "Steps : 4506 , Loss : 0.7830865383148193, Weights 1.059302568435669, bias : 0.057835761457681656  \n",
      "Steps : 4507 , Loss : 0.7830864787101746, Weights 1.0593032836914062, bias : 0.05783522501587868  \n",
      "Steps : 4508 , Loss : 0.7830865383148193, Weights 1.059303879737854, bias : 0.0578346885740757  \n",
      "Steps : 4509 , Loss : 0.7830864787101746, Weights 1.0593044757843018, bias : 0.05783415585756302  \n",
      "Steps : 4510 , Loss : 0.7830864787101746, Weights 1.0593050718307495, bias : 0.05783362314105034  \n",
      "Steps : 4511 , Loss : 0.7830865383148193, Weights 1.0593056678771973, bias : 0.05783309042453766  \n",
      "Steps : 4512 , Loss : 0.7830865383148193, Weights 1.059306263923645, bias : 0.05783255770802498  \n",
      "Steps : 4513 , Loss : 0.7830865383148193, Weights 1.0593068599700928, bias : 0.0578320287168026  \n",
      "Steps : 4514 , Loss : 0.7830864787101746, Weights 1.0593074560165405, bias : 0.057831499725580215  \n",
      "Steps : 4515 , Loss : 0.7830865383148193, Weights 1.0593080520629883, bias : 0.057830970734357834  \n",
      "Steps : 4516 , Loss : 0.7830864787101746, Weights 1.059308648109436, bias : 0.05783044546842575  \n",
      "Steps : 4517 , Loss : 0.7830865383148193, Weights 1.0593092441558838, bias : 0.05782992020249367  \n",
      "Steps : 4518 , Loss : 0.7830865383148193, Weights 1.0593098402023315, bias : 0.057829394936561584  \n",
      "Steps : 4519 , Loss : 0.7830864787101746, Weights 1.0593104362487793, bias : 0.0578288696706295  \n",
      "Steps : 4520 , Loss : 0.7830865383148193, Weights 1.059311032295227, bias : 0.05782834812998772  \n",
      "Steps : 4521 , Loss : 0.7830864787101746, Weights 1.0593116283416748, bias : 0.05782782658934593  \n",
      "Steps : 4522 , Loss : 0.7830865383148193, Weights 1.0593122243881226, bias : 0.05782730504870415  \n",
      "Steps : 4523 , Loss : 0.7830865383148193, Weights 1.0593128204345703, bias : 0.05782678350806236  \n",
      "Steps : 4524 , Loss : 0.7830864787101746, Weights 1.059313416481018, bias : 0.057826265692710876  \n",
      "Steps : 4525 , Loss : 0.7830865383148193, Weights 1.0593140125274658, bias : 0.05782574787735939  \n",
      "Steps : 4526 , Loss : 0.7830865383148193, Weights 1.0593146085739136, bias : 0.057825230062007904  \n",
      "Steps : 4527 , Loss : 0.7830865383148193, Weights 1.0593152046203613, bias : 0.05782471224665642  \n",
      "Steps : 4528 , Loss : 0.7830865383148193, Weights 1.059315800666809, bias : 0.05782419815659523  \n",
      "Steps : 4529 , Loss : 0.7830865383148193, Weights 1.0593163967132568, bias : 0.05782368406653404  \n",
      "Steps : 4530 , Loss : 0.7830864787101746, Weights 1.0593169927597046, bias : 0.057823169976472855  \n",
      "Steps : 4531 , Loss : 0.7830865383148193, Weights 1.0593175888061523, bias : 0.05782265588641167  \n",
      "Steps : 4532 , Loss : 0.7830865383148193, Weights 1.0593181848526, bias : 0.05782214552164078  \n",
      "Steps : 4533 , Loss : 0.7830865383148193, Weights 1.0593187808990479, bias : 0.05782163515686989  \n",
      "Steps : 4534 , Loss : 0.7830865383148193, Weights 1.0593193769454956, bias : 0.057821124792099  \n",
      "Steps : 4535 , Loss : 0.7830865383148193, Weights 1.0593199729919434, bias : 0.05782061442732811  \n",
      "Steps : 4536 , Loss : 0.7830865383148193, Weights 1.0593205690383911, bias : 0.05782010778784752  \n",
      "Steps : 4537 , Loss : 0.7830865383148193, Weights 1.0593211650848389, bias : 0.05781960114836693  \n",
      "Steps : 4538 , Loss : 0.7830864787101746, Weights 1.0593217611312866, bias : 0.05781909450888634  \n",
      "Steps : 4539 , Loss : 0.7830864787101746, Weights 1.0593223571777344, bias : 0.057818587869405746  \n",
      "Steps : 4540 , Loss : 0.7830864787101746, Weights 1.0593229532241821, bias : 0.057818084955215454  \n",
      "Steps : 4541 , Loss : 0.7830865383148193, Weights 1.0593235492706299, bias : 0.05781758204102516  \n",
      "Steps : 4542 , Loss : 0.7830864787101746, Weights 1.0593241453170776, bias : 0.05781707912683487  \n",
      "Steps : 4543 , Loss : 0.7830864787101746, Weights 1.0593247413635254, bias : 0.05781657621264458  \n",
      "Steps : 4544 , Loss : 0.7830865383148193, Weights 1.0593253374099731, bias : 0.05781607702374458  \n",
      "Steps : 4545 , Loss : 0.7830864191055298, Weights 1.059325933456421, bias : 0.05781557783484459  \n",
      "Steps : 4546 , Loss : 0.7830865383148193, Weights 1.0593265295028687, bias : 0.057815078645944595  \n",
      "Steps : 4547 , Loss : 0.7830864787101746, Weights 1.0593271255493164, bias : 0.0578145794570446  \n",
      "Steps : 4548 , Loss : 0.7830864787101746, Weights 1.0593277215957642, bias : 0.05781408026814461  \n",
      "Steps : 4549 , Loss : 0.7830864787101746, Weights 1.059328317642212, bias : 0.05781358480453491  \n",
      "Steps : 4550 , Loss : 0.7830864787101746, Weights 1.0593289136886597, bias : 0.05781308934092522  \n",
      "Steps : 4551 , Loss : 0.7830864191055298, Weights 1.0593295097351074, bias : 0.05781259387731552  \n",
      "Steps : 4552 , Loss : 0.7830865383148193, Weights 1.0593301057815552, bias : 0.057812098413705826  \n",
      "Steps : 4553 , Loss : 0.7830864787101746, Weights 1.059330701828003, bias : 0.05781160667538643  \n",
      "Steps : 4554 , Loss : 0.7830865383148193, Weights 1.0593312978744507, bias : 0.05781111493706703  \n",
      "Steps : 4555 , Loss : 0.7830864787101746, Weights 1.0593318939208984, bias : 0.057810623198747635  \n",
      "Steps : 4556 , Loss : 0.7830865383148193, Weights 1.0593324899673462, bias : 0.05781013146042824  \n",
      "Steps : 4557 , Loss : 0.7830865383148193, Weights 1.059333086013794, bias : 0.05780964344739914  \n",
      "Steps : 4558 , Loss : 0.7830864787101746, Weights 1.0593336820602417, bias : 0.05780915543437004  \n",
      "Steps : 4559 , Loss : 0.7830864787101746, Weights 1.0593342781066895, bias : 0.05780866742134094  \n",
      "Steps : 4560 , Loss : 0.7830864787101746, Weights 1.0593348741531372, bias : 0.057808179408311844  \n",
      "Steps : 4561 , Loss : 0.7830865383148193, Weights 1.059335470199585, bias : 0.057807695120573044  \n",
      "Steps : 4562 , Loss : 0.7830864191055298, Weights 1.0593360662460327, bias : 0.057807210832834244  \n",
      "Steps : 4563 , Loss : 0.7830864787101746, Weights 1.0593366622924805, bias : 0.057806726545095444  \n",
      "Steps : 4564 , Loss : 0.7830864787101746, Weights 1.0593372583389282, bias : 0.057806242257356644  \n",
      "Steps : 4565 , Loss : 0.7830864787101746, Weights 1.059337854385376, bias : 0.05780576169490814  \n",
      "Steps : 4566 , Loss : 0.7830865383148193, Weights 1.0593384504318237, bias : 0.05780528113245964  \n",
      "Steps : 4567 , Loss : 0.7830865383148193, Weights 1.0593390464782715, bias : 0.05780480057001114  \n",
      "Steps : 4568 , Loss : 0.7830864787101746, Weights 1.0593396425247192, bias : 0.05780432000756264  \n",
      "Steps : 4569 , Loss : 0.7830864787101746, Weights 1.059340238571167, bias : 0.057803839445114136  \n",
      "Steps : 4570 , Loss : 0.7830864787101746, Weights 1.0593408346176147, bias : 0.05780336260795593  \n",
      "Steps : 4571 , Loss : 0.7830864191055298, Weights 1.0593414306640625, bias : 0.05780288577079773  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4572 , Loss : 0.7830864191055298, Weights 1.0593420267105103, bias : 0.057802408933639526  \n",
      "Steps : 4573 , Loss : 0.7830864191055298, Weights 1.059342622756958, bias : 0.05780193209648132  \n",
      "Steps : 4574 , Loss : 0.7830864787101746, Weights 1.0593432188034058, bias : 0.05780145898461342  \n",
      "Steps : 4575 , Loss : 0.7830865383148193, Weights 1.0593438148498535, bias : 0.057800985872745514  \n",
      "Steps : 4576 , Loss : 0.7830864191055298, Weights 1.0593444108963013, bias : 0.05780051276087761  \n",
      "Steps : 4577 , Loss : 0.7830864191055298, Weights 1.059345006942749, bias : 0.057800039649009705  \n",
      "Steps : 4578 , Loss : 0.7830864191055298, Weights 1.0593456029891968, bias : 0.0577995702624321  \n",
      "Steps : 4579 , Loss : 0.7830864191055298, Weights 1.0593461990356445, bias : 0.05779910087585449  \n",
      "Steps : 4580 , Loss : 0.7830865383148193, Weights 1.0593467950820923, bias : 0.057798631489276886  \n",
      "Steps : 4581 , Loss : 0.7830865383148193, Weights 1.05934739112854, bias : 0.05779816210269928  \n",
      "Steps : 4582 , Loss : 0.7830865383148193, Weights 1.0593479871749878, bias : 0.057797692716121674  \n",
      "Steps : 4583 , Loss : 0.7830864787101746, Weights 1.0593485832214355, bias : 0.057797227054834366  \n",
      "Steps : 4584 , Loss : 0.7830864191055298, Weights 1.0593491792678833, bias : 0.05779676139354706  \n",
      "Steps : 4585 , Loss : 0.7830864191055298, Weights 1.059349775314331, bias : 0.05779629573225975  \n",
      "Steps : 4586 , Loss : 0.7830864787101746, Weights 1.0593503713607788, bias : 0.05779583007097244  \n",
      "Steps : 4587 , Loss : 0.7830865383148193, Weights 1.0593509674072266, bias : 0.05779536813497543  \n",
      "Steps : 4588 , Loss : 0.7830864787101746, Weights 1.0593515634536743, bias : 0.057794906198978424  \n",
      "Steps : 4589 , Loss : 0.7830864787101746, Weights 1.059352159500122, bias : 0.057794444262981415  \n",
      "Steps : 4590 , Loss : 0.7830865383148193, Weights 1.0593527555465698, bias : 0.057793982326984406  \n",
      "Steps : 4591 , Loss : 0.7830864191055298, Weights 1.0593533515930176, bias : 0.057793520390987396  \n",
      "Steps : 4592 , Loss : 0.7830864787101746, Weights 1.0593539476394653, bias : 0.057793062180280685  \n",
      "Steps : 4593 , Loss : 0.7830864191055298, Weights 1.059354543685913, bias : 0.057792603969573975  \n",
      "Steps : 4594 , Loss : 0.7830864787101746, Weights 1.0593551397323608, bias : 0.057792145758867264  \n",
      "Steps : 4595 , Loss : 0.7830864191055298, Weights 1.0593557357788086, bias : 0.05779168754816055  \n",
      "Steps : 4596 , Loss : 0.7830864191055298, Weights 1.0593563318252563, bias : 0.05779123306274414  \n",
      "Steps : 4597 , Loss : 0.7830864787101746, Weights 1.059356927871704, bias : 0.05779077857732773  \n",
      "Steps : 4598 , Loss : 0.7830865383148193, Weights 1.0593575239181519, bias : 0.057790324091911316  \n",
      "Steps : 4599 , Loss : 0.7830865383148193, Weights 1.0593581199645996, bias : 0.057789869606494904  \n",
      "Steps : 4600 , Loss : 0.7830864787101746, Weights 1.0593587160110474, bias : 0.05778941512107849  \n",
      "Steps : 4601 , Loss : 0.7830865383148193, Weights 1.0593593120574951, bias : 0.05778896436095238  \n",
      "Steps : 4602 , Loss : 0.7830865383148193, Weights 1.0593599081039429, bias : 0.05778851360082626  \n",
      "Steps : 4603 , Loss : 0.7830864787101746, Weights 1.0593605041503906, bias : 0.05778806284070015  \n",
      "Steps : 4604 , Loss : 0.7830864191055298, Weights 1.0593611001968384, bias : 0.057787612080574036  \n",
      "Steps : 4605 , Loss : 0.7830864191055298, Weights 1.0593616962432861, bias : 0.05778716504573822  \n",
      "Steps : 4606 , Loss : 0.7830864191055298, Weights 1.0593622922897339, bias : 0.057786718010902405  \n",
      "Steps : 4607 , Loss : 0.7830864787101746, Weights 1.0593628883361816, bias : 0.05778627097606659  \n",
      "Steps : 4608 , Loss : 0.7830865383148193, Weights 1.0593634843826294, bias : 0.057785823941230774  \n",
      "Steps : 4609 , Loss : 0.7830865383148193, Weights 1.0593640804290771, bias : 0.05778537690639496  \n",
      "Steps : 4610 , Loss : 0.7830864191055298, Weights 1.059364676475525, bias : 0.05778493359684944  \n",
      "Steps : 4611 , Loss : 0.7830864191055298, Weights 1.0593652725219727, bias : 0.057784490287303925  \n",
      "Steps : 4612 , Loss : 0.7830864787101746, Weights 1.0593658685684204, bias : 0.05778404697775841  \n",
      "Steps : 4613 , Loss : 0.7830864191055298, Weights 1.0593664646148682, bias : 0.05778360366821289  \n",
      "Steps : 4614 , Loss : 0.7830864787101746, Weights 1.059367060661316, bias : 0.057783160358667374  \n",
      "Steps : 4615 , Loss : 0.7830864787101746, Weights 1.0593676567077637, bias : 0.057782720774412155  \n",
      "Steps : 4616 , Loss : 0.7830864191055298, Weights 1.0593682527542114, bias : 0.05778228119015694  \n",
      "Steps : 4617 , Loss : 0.7830864787101746, Weights 1.0593688488006592, bias : 0.05778184160590172  \n",
      "Steps : 4618 , Loss : 0.7830864191055298, Weights 1.059369444847107, bias : 0.0577814020216465  \n",
      "Steps : 4619 , Loss : 0.7830864787101746, Weights 1.0593700408935547, bias : 0.05778096243739128  \n",
      "Steps : 4620 , Loss : 0.7830864787101746, Weights 1.0593706369400024, bias : 0.05778052657842636  \n",
      "Steps : 4621 , Loss : 0.7830864787101746, Weights 1.0593712329864502, bias : 0.05778009071946144  \n",
      "Steps : 4622 , Loss : 0.7830865383148193, Weights 1.0593717098236084, bias : 0.05777965486049652  \n",
      "Steps : 4623 , Loss : 0.7830864787101746, Weights 1.0593721866607666, bias : 0.0577792190015316  \n",
      "Steps : 4624 , Loss : 0.7830865383148193, Weights 1.0593726634979248, bias : 0.05777878686785698  \n",
      "Steps : 4625 , Loss : 0.7830864787101746, Weights 1.059373140335083, bias : 0.05777835473418236  \n",
      "Steps : 4626 , Loss : 0.7830864191055298, Weights 1.0593736171722412, bias : 0.057777922600507736  \n",
      "Steps : 4627 , Loss : 0.7830864191055298, Weights 1.0593740940093994, bias : 0.057777490466833115  \n",
      "Steps : 4628 , Loss : 0.7830864191055298, Weights 1.0593745708465576, bias : 0.05777705833315849  \n",
      "Steps : 4629 , Loss : 0.7830864191055298, Weights 1.0593750476837158, bias : 0.05777662992477417  \n",
      "Steps : 4630 , Loss : 0.7830864787101746, Weights 1.059375524520874, bias : 0.05777620151638985  \n",
      "Steps : 4631 , Loss : 0.7830864191055298, Weights 1.0593760013580322, bias : 0.057775773108005524  \n",
      "Steps : 4632 , Loss : 0.7830864191055298, Weights 1.0593764781951904, bias : 0.0577753446996212  \n",
      "Steps : 4633 , Loss : 0.7830864191055298, Weights 1.0593769550323486, bias : 0.05777491629123688  \n",
      "Steps : 4634 , Loss : 0.7830864787101746, Weights 1.0593774318695068, bias : 0.05777449160814285  \n",
      "Steps : 4635 , Loss : 0.7830864191055298, Weights 1.059377908706665, bias : 0.05777406692504883  \n",
      "Steps : 4636 , Loss : 0.7830864787101746, Weights 1.0593783855438232, bias : 0.0577736422419548  \n",
      "Steps : 4637 , Loss : 0.7830864191055298, Weights 1.0593788623809814, bias : 0.05777321755886078  \n",
      "Steps : 4638 , Loss : 0.7830864191055298, Weights 1.0593793392181396, bias : 0.057772792875766754  \n",
      "Steps : 4639 , Loss : 0.7830864787101746, Weights 1.0593798160552979, bias : 0.05777237191796303  \n",
      "Steps : 4640 , Loss : 0.7830864191055298, Weights 1.059380292892456, bias : 0.0577719509601593  \n",
      "Steps : 4641 , Loss : 0.7830864787101746, Weights 1.0593807697296143, bias : 0.057771530002355576  \n",
      "Steps : 4642 , Loss : 0.7830864191055298, Weights 1.0593812465667725, bias : 0.05777110904455185  \n",
      "Steps : 4643 , Loss : 0.7830864787101746, Weights 1.0593817234039307, bias : 0.05777069181203842  \n",
      "Steps : 4644 , Loss : 0.7830864191055298, Weights 1.0593822002410889, bias : 0.057770274579524994  \n",
      "Steps : 4645 , Loss : 0.7830864787101746, Weights 1.059382677078247, bias : 0.057769857347011566  \n",
      "Steps : 4646 , Loss : 0.7830864191055298, Weights 1.0593831539154053, bias : 0.05776944011449814  \n",
      "Steps : 4647 , Loss : 0.7830864191055298, Weights 1.0593836307525635, bias : 0.05776902288198471  \n",
      "Steps : 4648 , Loss : 0.7830864191055298, Weights 1.0593841075897217, bias : 0.05776860937476158  \n",
      "Steps : 4649 , Loss : 0.7830865383148193, Weights 1.0593845844268799, bias : 0.05776819586753845  \n",
      "Steps : 4650 , Loss : 0.7830864787101746, Weights 1.059385061264038, bias : 0.05776778236031532  \n",
      "Steps : 4651 , Loss : 0.7830864191055298, Weights 1.0593855381011963, bias : 0.057767368853092194  \n",
      "Steps : 4652 , Loss : 0.7830864787101746, Weights 1.0593860149383545, bias : 0.057766955345869064  \n",
      "Steps : 4653 , Loss : 0.7830864191055298, Weights 1.0593864917755127, bias : 0.057766545563936234  \n",
      "Steps : 4654 , Loss : 0.7830864191055298, Weights 1.059386968612671, bias : 0.0577661357820034  \n",
      "Steps : 4655 , Loss : 0.7830864191055298, Weights 1.059387445449829, bias : 0.05776572600007057  \n",
      "Steps : 4656 , Loss : 0.7830864191055298, Weights 1.0593879222869873, bias : 0.05776531621813774  \n",
      "Steps : 4657 , Loss : 0.7830864191055298, Weights 1.0593883991241455, bias : 0.05776490643620491  \n",
      "Steps : 4658 , Loss : 0.7830864787101746, Weights 1.0593888759613037, bias : 0.05776450037956238  \n",
      "Steps : 4659 , Loss : 0.7830864191055298, Weights 1.059389352798462, bias : 0.057764094322919846  \n",
      "Steps : 4660 , Loss : 0.7830865383148193, Weights 1.0593898296356201, bias : 0.05776368826627731  \n",
      "Steps : 4661 , Loss : 0.7830864191055298, Weights 1.0593903064727783, bias : 0.05776328220963478  \n",
      "Steps : 4662 , Loss : 0.7830864787101746, Weights 1.0593907833099365, bias : 0.05776287615299225  \n",
      "Steps : 4663 , Loss : 0.7830864191055298, Weights 1.0593912601470947, bias : 0.057762473821640015  \n",
      "Steps : 4664 , Loss : 0.7830864191055298, Weights 1.059391736984253, bias : 0.05776207149028778  \n",
      "Steps : 4665 , Loss : 0.7830864191055298, Weights 1.0593922138214111, bias : 0.05776166915893555  \n",
      "Steps : 4666 , Loss : 0.7830864191055298, Weights 1.0593926906585693, bias : 0.05776126682758331  \n",
      "Steps : 4667 , Loss : 0.7830864191055298, Weights 1.0593931674957275, bias : 0.05776086449623108  \n",
      "Steps : 4668 , Loss : 0.7830864191055298, Weights 1.0593936443328857, bias : 0.057760465890169144  \n",
      "Steps : 4669 , Loss : 0.7830864191055298, Weights 1.059394121170044, bias : 0.05776006728410721  \n",
      "Steps : 4670 , Loss : 0.7830864191055298, Weights 1.0593945980072021, bias : 0.05775966867804527  \n",
      "Steps : 4671 , Loss : 0.7830864191055298, Weights 1.0593950748443604, bias : 0.05775927007198334  \n",
      "Steps : 4672 , Loss : 0.7830864191055298, Weights 1.0593955516815186, bias : 0.0577588714659214  \n",
      "Steps : 4673 , Loss : 0.7830864191055298, Weights 1.0593960285186768, bias : 0.05775847285985947  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4674 , Loss : 0.7830864191055298, Weights 1.059396505355835, bias : 0.05775807797908783  \n",
      "Steps : 4675 , Loss : 0.7830864191055298, Weights 1.0593969821929932, bias : 0.05775768309831619  \n",
      "Steps : 4676 , Loss : 0.7830864191055298, Weights 1.0593974590301514, bias : 0.057757288217544556  \n",
      "Steps : 4677 , Loss : 0.7830864191055298, Weights 1.0593979358673096, bias : 0.05775689333677292  \n",
      "Steps : 4678 , Loss : 0.7830864191055298, Weights 1.0593984127044678, bias : 0.05775649845600128  \n",
      "Steps : 4679 , Loss : 0.7830864787101746, Weights 1.059398889541626, bias : 0.05775610730051994  \n",
      "Steps : 4680 , Loss : 0.7830864191055298, Weights 1.0593993663787842, bias : 0.057755716145038605  \n",
      "Steps : 4681 , Loss : 0.7830864191055298, Weights 1.0593998432159424, bias : 0.057755324989557266  \n",
      "Steps : 4682 , Loss : 0.7830864191055298, Weights 1.0594003200531006, bias : 0.05775493383407593  \n",
      "Steps : 4683 , Loss : 0.7830864191055298, Weights 1.0594007968902588, bias : 0.05775454267859459  \n",
      "Steps : 4684 , Loss : 0.7830864787101746, Weights 1.059401273727417, bias : 0.05775415524840355  \n",
      "Steps : 4685 , Loss : 0.7830864191055298, Weights 1.0594017505645752, bias : 0.05775376781821251  \n",
      "Steps : 4686 , Loss : 0.7830864191055298, Weights 1.0594022274017334, bias : 0.05775338038802147  \n",
      "Steps : 4687 , Loss : 0.7830864191055298, Weights 1.0594027042388916, bias : 0.05775299295783043  \n",
      "Steps : 4688 , Loss : 0.7830864191055298, Weights 1.0594031810760498, bias : 0.05775260552763939  \n",
      "Steps : 4689 , Loss : 0.7830864191055298, Weights 1.059403657913208, bias : 0.05775222182273865  \n",
      "Steps : 4690 , Loss : 0.7830864191055298, Weights 1.0594041347503662, bias : 0.057751838117837906  \n",
      "Steps : 4691 , Loss : 0.7830864787101746, Weights 1.0594046115875244, bias : 0.057751454412937164  \n",
      "Steps : 4692 , Loss : 0.7830864191055298, Weights 1.0594050884246826, bias : 0.05775107070803642  \n",
      "Steps : 4693 , Loss : 0.7830864787101746, Weights 1.0594055652618408, bias : 0.05775068700313568  \n",
      "Steps : 4694 , Loss : 0.7830864191055298, Weights 1.059406042098999, bias : 0.05775030329823494  \n",
      "Steps : 4695 , Loss : 0.7830864787101746, Weights 1.0594065189361572, bias : 0.057749923318624496  \n",
      "Steps : 4696 , Loss : 0.7830864191055298, Weights 1.0594069957733154, bias : 0.05774954333901405  \n",
      "Steps : 4697 , Loss : 0.7830864191055298, Weights 1.0594074726104736, bias : 0.05774916335940361  \n",
      "Steps : 4698 , Loss : 0.7830864191055298, Weights 1.0594079494476318, bias : 0.05774878337979317  \n",
      "Steps : 4699 , Loss : 0.7830864191055298, Weights 1.05940842628479, bias : 0.057748403400182724  \n",
      "Steps : 4700 , Loss : 0.7830864191055298, Weights 1.0594089031219482, bias : 0.05774802714586258  \n",
      "Steps : 4701 , Loss : 0.7830864191055298, Weights 1.0594093799591064, bias : 0.057747650891542435  \n",
      "Steps : 4702 , Loss : 0.7830864191055298, Weights 1.0594098567962646, bias : 0.05774727463722229  \n",
      "Steps : 4703 , Loss : 0.7830864191055298, Weights 1.0594103336334229, bias : 0.057746898382902145  \n",
      "Steps : 4704 , Loss : 0.7830864191055298, Weights 1.059410810470581, bias : 0.057746522128582  \n",
      "Steps : 4705 , Loss : 0.7830864191055298, Weights 1.0594112873077393, bias : 0.057746145874261856  \n",
      "Steps : 4706 , Loss : 0.7830864191055298, Weights 1.0594117641448975, bias : 0.05774577334523201  \n",
      "Steps : 4707 , Loss : 0.7830862998962402, Weights 1.0594122409820557, bias : 0.057745400816202164  \n",
      "Steps : 4708 , Loss : 0.7830864191055298, Weights 1.0594127178192139, bias : 0.05774502828717232  \n",
      "Steps : 4709 , Loss : 0.7830864191055298, Weights 1.059413194656372, bias : 0.05774465575814247  \n",
      "Steps : 4710 , Loss : 0.7830864191055298, Weights 1.0594136714935303, bias : 0.057744283229112625  \n",
      "Steps : 4711 , Loss : 0.7830864191055298, Weights 1.0594141483306885, bias : 0.05774391442537308  \n",
      "Steps : 4712 , Loss : 0.7830864191055298, Weights 1.0594146251678467, bias : 0.05774354562163353  \n",
      "Steps : 4713 , Loss : 0.7830862998962402, Weights 1.0594151020050049, bias : 0.05774317681789398  \n",
      "Steps : 4714 , Loss : 0.7830864191055298, Weights 1.059415578842163, bias : 0.057742808014154434  \n",
      "Steps : 4715 , Loss : 0.7830864787101746, Weights 1.0594160556793213, bias : 0.057742439210414886  \n",
      "Steps : 4716 , Loss : 0.7830864787101746, Weights 1.0594165325164795, bias : 0.05774207040667534  \n",
      "Steps : 4717 , Loss : 0.7830864191055298, Weights 1.0594170093536377, bias : 0.05774170532822609  \n",
      "Steps : 4718 , Loss : 0.7830864191055298, Weights 1.059417486190796, bias : 0.05774134024977684  \n",
      "Steps : 4719 , Loss : 0.7830864191055298, Weights 1.059417963027954, bias : 0.05774097517132759  \n",
      "Steps : 4720 , Loss : 0.7830864191055298, Weights 1.0594184398651123, bias : 0.05774061009287834  \n",
      "Steps : 4721 , Loss : 0.7830864191055298, Weights 1.0594189167022705, bias : 0.05774024501442909  \n",
      "Steps : 4722 , Loss : 0.7830864191055298, Weights 1.0594193935394287, bias : 0.05773987993597984  \n",
      "Steps : 4723 , Loss : 0.7830864787101746, Weights 1.059419870376587, bias : 0.05773951858282089  \n",
      "Steps : 4724 , Loss : 0.7830864191055298, Weights 1.0594203472137451, bias : 0.05773915722966194  \n",
      "Steps : 4725 , Loss : 0.7830864191055298, Weights 1.0594208240509033, bias : 0.05773879587650299  \n",
      "Steps : 4726 , Loss : 0.7830864191055298, Weights 1.0594213008880615, bias : 0.05773843452334404  \n",
      "Steps : 4727 , Loss : 0.7830864787101746, Weights 1.0594217777252197, bias : 0.05773807317018509  \n",
      "Steps : 4728 , Loss : 0.7830864191055298, Weights 1.059422254562378, bias : 0.05773771554231644  \n",
      "Steps : 4729 , Loss : 0.7830864191055298, Weights 1.0594227313995361, bias : 0.057737357914447784  \n",
      "Steps : 4730 , Loss : 0.7830864787101746, Weights 1.0594232082366943, bias : 0.05773700028657913  \n",
      "Steps : 4731 , Loss : 0.7830864191055298, Weights 1.0594236850738525, bias : 0.05773664265871048  \n",
      "Steps : 4732 , Loss : 0.7830864191055298, Weights 1.0594241619110107, bias : 0.05773628503084183  \n",
      "Steps : 4733 , Loss : 0.7830864191055298, Weights 1.059424638748169, bias : 0.057735927402973175  \n",
      "Steps : 4734 , Loss : 0.7830864191055298, Weights 1.0594251155853271, bias : 0.05773557350039482  \n",
      "Steps : 4735 , Loss : 0.7830864191055298, Weights 1.0594255924224854, bias : 0.05773521959781647  \n",
      "Steps : 4736 , Loss : 0.7830864191055298, Weights 1.0594260692596436, bias : 0.05773486569523811  \n",
      "Steps : 4737 , Loss : 0.7830864191055298, Weights 1.0594265460968018, bias : 0.05773451179265976  \n",
      "Steps : 4738 , Loss : 0.7830864191055298, Weights 1.05942702293396, bias : 0.057734157890081406  \n",
      "Steps : 4739 , Loss : 0.7830864191055298, Weights 1.0594274997711182, bias : 0.05773380398750305  \n",
      "Steps : 4740 , Loss : 0.7830864191055298, Weights 1.0594279766082764, bias : 0.057733453810214996  \n",
      "Steps : 4741 , Loss : 0.7830864191055298, Weights 1.0594284534454346, bias : 0.05773310363292694  \n",
      "Steps : 4742 , Loss : 0.7830864191055298, Weights 1.0594289302825928, bias : 0.057732753455638885  \n",
      "Steps : 4743 , Loss : 0.7830864787101746, Weights 1.059429407119751, bias : 0.05773240327835083  \n",
      "Steps : 4744 , Loss : 0.7830864191055298, Weights 1.0594298839569092, bias : 0.057732053101062775  \n",
      "Steps : 4745 , Loss : 0.7830864191055298, Weights 1.0594303607940674, bias : 0.05773170292377472  \n",
      "Steps : 4746 , Loss : 0.7830864191055298, Weights 1.0594308376312256, bias : 0.05773135647177696  \n",
      "Steps : 4747 , Loss : 0.7830864191055298, Weights 1.0594313144683838, bias : 0.057731010019779205  \n",
      "Steps : 4748 , Loss : 0.7830864191055298, Weights 1.059431791305542, bias : 0.05773066356778145  \n",
      "Steps : 4749 , Loss : 0.7830864191055298, Weights 1.0594322681427002, bias : 0.05773031711578369  \n",
      "Steps : 4750 , Loss : 0.7830864191055298, Weights 1.0594327449798584, bias : 0.057729970663785934  \n",
      "Steps : 4751 , Loss : 0.7830864191055298, Weights 1.0594332218170166, bias : 0.05772962421178818  \n",
      "Steps : 4752 , Loss : 0.7830864191055298, Weights 1.0594336986541748, bias : 0.05772928148508072  \n",
      "Steps : 4753 , Loss : 0.7830864191055298, Weights 1.059434175491333, bias : 0.05772893875837326  \n",
      "Steps : 4754 , Loss : 0.7830864191055298, Weights 1.0594346523284912, bias : 0.0577285960316658  \n",
      "Steps : 4755 , Loss : 0.7830864191055298, Weights 1.0594351291656494, bias : 0.057728253304958344  \n",
      "Steps : 4756 , Loss : 0.7830864191055298, Weights 1.0594356060028076, bias : 0.057727910578250885  \n",
      "Steps : 4757 , Loss : 0.7830864787101746, Weights 1.0594360828399658, bias : 0.057727567851543427  \n",
      "Steps : 4758 , Loss : 0.7830864191055298, Weights 1.059436559677124, bias : 0.057727228850126266  \n",
      "Steps : 4759 , Loss : 0.7830864787101746, Weights 1.0594370365142822, bias : 0.057726889848709106  \n",
      "Steps : 4760 , Loss : 0.7830864191055298, Weights 1.0594375133514404, bias : 0.057726550847291946  \n",
      "Steps : 4761 , Loss : 0.7830864191055298, Weights 1.0594379901885986, bias : 0.057726211845874786  \n",
      "Steps : 4762 , Loss : 0.7830864191055298, Weights 1.0594384670257568, bias : 0.057725872844457626  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4763 , Loss : 0.7830862998962402, Weights 1.059438943862915, bias : 0.057725533843040466  \n",
      "Steps : 4764 , Loss : 0.7830864191055298, Weights 1.0594394207000732, bias : 0.057725198566913605  \n",
      "Steps : 4765 , Loss : 0.7830864191055298, Weights 1.059439778327942, bias : 0.05772486329078674  \n",
      "Steps : 4766 , Loss : 0.7830864191055298, Weights 1.0594401359558105, bias : 0.05772452801465988  \n",
      "Steps : 4767 , Loss : 0.7830864191055298, Weights 1.0594404935836792, bias : 0.05772419273853302  \n",
      "Steps : 4768 , Loss : 0.7830864191055298, Weights 1.0594408512115479, bias : 0.05772385746240616  \n",
      "Steps : 4769 , Loss : 0.7830864191055298, Weights 1.0594412088394165, bias : 0.0577235221862793  \n",
      "Steps : 4770 , Loss : 0.7830864191055298, Weights 1.0594415664672852, bias : 0.057723190635442734  \n",
      "Steps : 4771 , Loss : 0.7830862998962402, Weights 1.0594419240951538, bias : 0.05772285908460617  \n",
      "Steps : 4772 , Loss : 0.7830862998962402, Weights 1.0594422817230225, bias : 0.05772252753376961  \n",
      "Steps : 4773 , Loss : 0.7830864191055298, Weights 1.0594426393508911, bias : 0.057722195982933044  \n",
      "Steps : 4774 , Loss : 0.7830864191055298, Weights 1.0594429969787598, bias : 0.05772186443209648  \n",
      "Steps : 4775 , Loss : 0.7830862998962402, Weights 1.0594433546066284, bias : 0.05772153288125992  \n",
      "Steps : 4776 , Loss : 0.7830864191055298, Weights 1.059443712234497, bias : 0.057721201330423355  \n",
      "Steps : 4777 , Loss : 0.7830864191055298, Weights 1.0594440698623657, bias : 0.05772087350487709  \n",
      "Steps : 4778 , Loss : 0.7830864191055298, Weights 1.0594444274902344, bias : 0.057720545679330826  \n",
      "Steps : 4779 , Loss : 0.7830864191055298, Weights 1.059444785118103, bias : 0.05772021785378456  \n",
      "Steps : 4780 , Loss : 0.7830864191055298, Weights 1.0594451427459717, bias : 0.057719890028238297  \n",
      "Steps : 4781 , Loss : 0.7830864191055298, Weights 1.0594455003738403, bias : 0.05771956220269203  \n",
      "Steps : 4782 , Loss : 0.7830864191055298, Weights 1.059445858001709, bias : 0.05771923437714577  \n",
      "Steps : 4783 , Loss : 0.7830864191055298, Weights 1.0594462156295776, bias : 0.0577189102768898  \n",
      "Steps : 4784 , Loss : 0.7830864191055298, Weights 1.0594465732574463, bias : 0.057718586176633835  \n",
      "Steps : 4785 , Loss : 0.7830864191055298, Weights 1.059446930885315, bias : 0.05771826207637787  \n",
      "Steps : 4786 , Loss : 0.7830864191055298, Weights 1.0594472885131836, bias : 0.0577179379761219  \n",
      "Steps : 4787 , Loss : 0.7830864191055298, Weights 1.0594476461410522, bias : 0.057717613875865936  \n",
      "Steps : 4788 , Loss : 0.7830864191055298, Weights 1.059448003768921, bias : 0.05771728977560997  \n",
      "Steps : 4789 , Loss : 0.7830864191055298, Weights 1.0594483613967896, bias : 0.0577169694006443  \n",
      "Steps : 4790 , Loss : 0.7830862998962402, Weights 1.0594487190246582, bias : 0.057716649025678635  \n",
      "Steps : 4791 , Loss : 0.7830864191055298, Weights 1.0594490766525269, bias : 0.05771632865071297  \n",
      "Steps : 4792 , Loss : 0.7830864191055298, Weights 1.0594494342803955, bias : 0.0577160082757473  \n",
      "Steps : 4793 , Loss : 0.7830864191055298, Weights 1.0594497919082642, bias : 0.05771568790078163  \n",
      "Steps : 4794 , Loss : 0.7830864191055298, Weights 1.0594501495361328, bias : 0.057715367525815964  \n",
      "Steps : 4795 , Loss : 0.7830864191055298, Weights 1.0594505071640015, bias : 0.057715050876140594  \n",
      "Steps : 4796 , Loss : 0.7830864191055298, Weights 1.0594508647918701, bias : 0.057714734226465225  \n",
      "Steps : 4797 , Loss : 0.7830864191055298, Weights 1.0594512224197388, bias : 0.057714417576789856  \n",
      "Steps : 4798 , Loss : 0.7830864191055298, Weights 1.0594515800476074, bias : 0.05771410092711449  \n",
      "Steps : 4799 , Loss : 0.7830864191055298, Weights 1.059451937675476, bias : 0.05771378427743912  \n",
      "Steps : 4800 , Loss : 0.7830862998962402, Weights 1.0594522953033447, bias : 0.05771346762776375  \n",
      "Steps : 4801 , Loss : 0.7830864191055298, Weights 1.0594526529312134, bias : 0.05771315097808838  \n",
      "Steps : 4802 , Loss : 0.7830862998962402, Weights 1.059453010559082, bias : 0.05771283805370331  \n",
      "Steps : 4803 , Loss : 0.7830864191055298, Weights 1.0594533681869507, bias : 0.05771252512931824  \n",
      "Steps : 4804 , Loss : 0.7830862402915955, Weights 1.0594537258148193, bias : 0.057712212204933167  \n",
      "Steps : 4805 , Loss : 0.7830864191055298, Weights 1.059454083442688, bias : 0.057711899280548096  \n",
      "Steps : 4806 , Loss : 0.7830864191055298, Weights 1.0594544410705566, bias : 0.057711586356163025  \n",
      "Steps : 4807 , Loss : 0.7830864191055298, Weights 1.0594547986984253, bias : 0.057711273431777954  \n",
      "Steps : 4808 , Loss : 0.7830864191055298, Weights 1.059455156326294, bias : 0.05771096423268318  \n",
      "Steps : 4809 , Loss : 0.7830864191055298, Weights 1.0594555139541626, bias : 0.05771065503358841  \n",
      "Steps : 4810 , Loss : 0.7830864191055298, Weights 1.0594558715820312, bias : 0.05771034583449364  \n",
      "Steps : 4811 , Loss : 0.7830864191055298, Weights 1.0594562292099, bias : 0.057710036635398865  \n",
      "Steps : 4812 , Loss : 0.7830864191055298, Weights 1.0594565868377686, bias : 0.05770972743630409  \n",
      "Steps : 4813 , Loss : 0.7830864191055298, Weights 1.0594569444656372, bias : 0.05770941823720932  \n",
      "Steps : 4814 , Loss : 0.7830864191055298, Weights 1.0594573020935059, bias : 0.05770910903811455  \n",
      "Steps : 4815 , Loss : 0.7830864787101746, Weights 1.0594576597213745, bias : 0.057708803564310074  \n",
      "Steps : 4816 , Loss : 0.7830864191055298, Weights 1.0594580173492432, bias : 0.0577084980905056  \n",
      "Steps : 4817 , Loss : 0.7830864191055298, Weights 1.0594583749771118, bias : 0.057708192616701126  \n",
      "Steps : 4818 , Loss : 0.7830862998962402, Weights 1.0594587326049805, bias : 0.05770788714289665  \n",
      "Steps : 4819 , Loss : 0.7830864191055298, Weights 1.0594590902328491, bias : 0.05770758166909218  \n",
      "Steps : 4820 , Loss : 0.7830864191055298, Weights 1.0594594478607178, bias : 0.057707276195287704  \n",
      "Steps : 4821 , Loss : 0.7830864191055298, Weights 1.0594598054885864, bias : 0.05770697072148323  \n",
      "Steps : 4822 , Loss : 0.7830864191055298, Weights 1.059460163116455, bias : 0.057706668972969055  \n",
      "Steps : 4823 , Loss : 0.7830864191055298, Weights 1.0594605207443237, bias : 0.05770636722445488  \n",
      "Steps : 4824 , Loss : 0.7830864191055298, Weights 1.0594608783721924, bias : 0.057706065475940704  \n",
      "Steps : 4825 , Loss : 0.7830864191055298, Weights 1.059461236000061, bias : 0.05770576372742653  \n",
      "Steps : 4826 , Loss : 0.7830864191055298, Weights 1.0594615936279297, bias : 0.057705461978912354  \n",
      "Steps : 4827 , Loss : 0.7830864191055298, Weights 1.0594619512557983, bias : 0.05770516023039818  \n",
      "Steps : 4828 , Loss : 0.7830864191055298, Weights 1.059462308883667, bias : 0.057704858481884  \n",
      "Steps : 4829 , Loss : 0.7830862998962402, Weights 1.0594626665115356, bias : 0.057704560458660126  \n",
      "Steps : 4830 , Loss : 0.7830864191055298, Weights 1.0594630241394043, bias : 0.05770426243543625  \n",
      "Steps : 4831 , Loss : 0.7830864191055298, Weights 1.059463381767273, bias : 0.05770396441221237  \n",
      "Steps : 4832 , Loss : 0.7830864191055298, Weights 1.0594637393951416, bias : 0.057703666388988495  \n",
      "Steps : 4833 , Loss : 0.7830864191055298, Weights 1.0594640970230103, bias : 0.05770336836576462  \n",
      "Steps : 4834 , Loss : 0.7830864191055298, Weights 1.059464454650879, bias : 0.05770307034254074  \n",
      "Steps : 4835 , Loss : 0.7830862998962402, Weights 1.0594648122787476, bias : 0.05770277604460716  \n",
      "Steps : 4836 , Loss : 0.7830862998962402, Weights 1.0594651699066162, bias : 0.057702481746673584  \n",
      "Steps : 4837 , Loss : 0.7830864191055298, Weights 1.0594655275344849, bias : 0.057702187448740005  \n",
      "Steps : 4838 , Loss : 0.7830864191055298, Weights 1.0594658851623535, bias : 0.05770189315080643  \n",
      "Steps : 4839 , Loss : 0.7830864191055298, Weights 1.0594662427902222, bias : 0.05770159885287285  \n",
      "Steps : 4840 , Loss : 0.7830864191055298, Weights 1.0594666004180908, bias : 0.05770130455493927  \n",
      "Steps : 4841 , Loss : 0.7830862998962402, Weights 1.0594669580459595, bias : 0.05770101025700569  \n",
      "Steps : 4842 , Loss : 0.7830862998962402, Weights 1.0594673156738281, bias : 0.05770071968436241  \n",
      "Steps : 4843 , Loss : 0.7830862998962402, Weights 1.0594676733016968, bias : 0.05770042911171913  \n",
      "Steps : 4844 , Loss : 0.7830864191055298, Weights 1.0594680309295654, bias : 0.05770013853907585  \n",
      "Steps : 4845 , Loss : 0.7830864191055298, Weights 1.059468388557434, bias : 0.05769984796643257  \n",
      "Steps : 4846 , Loss : 0.7830862998962402, Weights 1.0594687461853027, bias : 0.05769955739378929  \n",
      "Steps : 4847 , Loss : 0.7830864191055298, Weights 1.0594691038131714, bias : 0.05769926682114601  \n",
      "Steps : 4848 , Loss : 0.7830864191055298, Weights 1.05946946144104, bias : 0.05769897624850273  \n",
      "Steps : 4849 , Loss : 0.7830864191055298, Weights 1.0594698190689087, bias : 0.05769868567585945  \n",
      "Steps : 4850 , Loss : 0.7830862998962402, Weights 1.0594701766967773, bias : 0.05769839882850647  \n",
      "Steps : 4851 , Loss : 0.7830864191055298, Weights 1.059470534324646, bias : 0.05769811198115349  \n",
      "Steps : 4852 , Loss : 0.7830864191055298, Weights 1.0594708919525146, bias : 0.05769782513380051  \n",
      "Steps : 4853 , Loss : 0.7830864191055298, Weights 1.0594712495803833, bias : 0.057697538286447525  \n",
      "Steps : 4854 , Loss : 0.7830864191055298, Weights 1.059471607208252, bias : 0.05769725143909454  \n",
      "Steps : 4855 , Loss : 0.7830862998962402, Weights 1.0594719648361206, bias : 0.05769696459174156  \n",
      "Steps : 4856 , Loss : 0.7830864191055298, Weights 1.0594723224639893, bias : 0.05769667774438858  \n",
      "Steps : 4857 , Loss : 0.7830864191055298, Weights 1.059472680091858, bias : 0.0576963946223259  \n",
      "Steps : 4858 , Loss : 0.7830864191055298, Weights 1.0594730377197266, bias : 0.057696111500263214  \n",
      "Steps : 4859 , Loss : 0.7830862998962402, Weights 1.0594733953475952, bias : 0.05769582837820053  \n",
      "Steps : 4860 , Loss : 0.7830864191055298, Weights 1.0594737529754639, bias : 0.05769554525613785  \n",
      "Steps : 4861 , Loss : 0.7830862998962402, Weights 1.0594741106033325, bias : 0.057695262134075165  \n",
      "Steps : 4862 , Loss : 0.7830864191055298, Weights 1.0594744682312012, bias : 0.05769497901201248  \n",
      "Steps : 4863 , Loss : 0.7830862998962402, Weights 1.0594748258590698, bias : 0.0576946958899498  \n",
      "Steps : 4864 , Loss : 0.7830864191055298, Weights 1.0594751834869385, bias : 0.057694416493177414  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4865 , Loss : 0.7830864191055298, Weights 1.0594755411148071, bias : 0.05769413709640503  \n",
      "Steps : 4866 , Loss : 0.7830864191055298, Weights 1.0594758987426758, bias : 0.057693857699632645  \n",
      "Steps : 4867 , Loss : 0.7830864191055298, Weights 1.0594762563705444, bias : 0.05769357830286026  \n",
      "Steps : 4868 , Loss : 0.7830864191055298, Weights 1.059476613998413, bias : 0.057693298906087875  \n",
      "Steps : 4869 , Loss : 0.7830864191055298, Weights 1.0594769716262817, bias : 0.05769301950931549  \n",
      "Steps : 4870 , Loss : 0.7830864191055298, Weights 1.0594773292541504, bias : 0.057692740112543106  \n",
      "Steps : 4871 , Loss : 0.7830864191055298, Weights 1.059477686882019, bias : 0.05769246071577072  \n",
      "Steps : 4872 , Loss : 0.7830864191055298, Weights 1.0594780445098877, bias : 0.057692185044288635  \n",
      "Steps : 4873 , Loss : 0.7830864191055298, Weights 1.0594784021377563, bias : 0.05769190937280655  \n",
      "Steps : 4874 , Loss : 0.7830862998962402, Weights 1.059478759765625, bias : 0.05769163370132446  \n",
      "Steps : 4875 , Loss : 0.7830864191055298, Weights 1.0594791173934937, bias : 0.05769135802984238  \n",
      "Steps : 4876 , Loss : 0.7830862998962402, Weights 1.0594794750213623, bias : 0.05769108235836029  \n",
      "Steps : 4877 , Loss : 0.7830864191055298, Weights 1.059479832649231, bias : 0.057690806686878204  \n",
      "Steps : 4878 , Loss : 0.7830862998962402, Weights 1.0594801902770996, bias : 0.05769053101539612  \n",
      "Steps : 4879 , Loss : 0.7830862998962402, Weights 1.0594805479049683, bias : 0.05769025906920433  \n",
      "Steps : 4880 , Loss : 0.7830864191055298, Weights 1.059480905532837, bias : 0.05768998712301254  \n",
      "Steps : 4881 , Loss : 0.7830864191055298, Weights 1.0594812631607056, bias : 0.057689715176820755  \n",
      "Steps : 4882 , Loss : 0.7830864191055298, Weights 1.0594816207885742, bias : 0.05768944323062897  \n",
      "Steps : 4883 , Loss : 0.7830864191055298, Weights 1.0594819784164429, bias : 0.05768917128443718  \n",
      "Steps : 4884 , Loss : 0.7830864191055298, Weights 1.0594823360443115, bias : 0.05768889933824539  \n",
      "Steps : 4885 , Loss : 0.7830864191055298, Weights 1.0594826936721802, bias : 0.057688627392053604  \n",
      "Steps : 4886 , Loss : 0.7830862998962402, Weights 1.0594830513000488, bias : 0.057688355445861816  \n",
      "Steps : 4887 , Loss : 0.7830864191055298, Weights 1.0594834089279175, bias : 0.05768808722496033  \n",
      "Steps : 4888 , Loss : 0.7830862998962402, Weights 1.0594837665557861, bias : 0.05768781900405884  \n",
      "Steps : 4889 , Loss : 0.7830864191055298, Weights 1.0594841241836548, bias : 0.05768755078315735  \n",
      "Steps : 4890 , Loss : 0.7830864191055298, Weights 1.0594844818115234, bias : 0.05768728256225586  \n",
      "Steps : 4891 , Loss : 0.7830864191055298, Weights 1.059484839439392, bias : 0.05768701434135437  \n",
      "Steps : 4892 , Loss : 0.7830864191055298, Weights 1.0594851970672607, bias : 0.05768674612045288  \n",
      "Steps : 4893 , Loss : 0.7830864191055298, Weights 1.0594855546951294, bias : 0.05768647789955139  \n",
      "Steps : 4894 , Loss : 0.7830864191055298, Weights 1.059485912322998, bias : 0.0576862134039402  \n",
      "Steps : 4895 , Loss : 0.7830864191055298, Weights 1.0594862699508667, bias : 0.05768594890832901  \n",
      "Steps : 4896 , Loss : 0.7830862998962402, Weights 1.0594866275787354, bias : 0.05768568441271782  \n",
      "Steps : 4897 , Loss : 0.7830862998962402, Weights 1.059486985206604, bias : 0.05768541991710663  \n",
      "Steps : 4898 , Loss : 0.7830864191055298, Weights 1.0594873428344727, bias : 0.05768515542149544  \n",
      "Steps : 4899 , Loss : 0.7830862998962402, Weights 1.0594877004623413, bias : 0.05768489092588425  \n",
      "Steps : 4900 , Loss : 0.7830864191055298, Weights 1.05948805809021, bias : 0.057684626430273056  \n",
      "Steps : 4901 , Loss : 0.7830864191055298, Weights 1.0594884157180786, bias : 0.057684361934661865  \n",
      "Steps : 4902 , Loss : 0.7830864191055298, Weights 1.0594887733459473, bias : 0.05768410116434097  \n",
      "Steps : 4903 , Loss : 0.7830864191055298, Weights 1.059489130973816, bias : 0.05768384039402008  \n",
      "Steps : 4904 , Loss : 0.7830864191055298, Weights 1.0594894886016846, bias : 0.05768357962369919  \n",
      "Steps : 4905 , Loss : 0.7830864191055298, Weights 1.0594898462295532, bias : 0.057683318853378296  \n",
      "Steps : 4906 , Loss : 0.7830864191055298, Weights 1.0594902038574219, bias : 0.057683058083057404  \n",
      "Steps : 4907 , Loss : 0.7830862998962402, Weights 1.0594905614852905, bias : 0.05768279731273651  \n",
      "Steps : 4908 , Loss : 0.7830864191055298, Weights 1.0594909191131592, bias : 0.05768253654241562  \n",
      "Steps : 4909 , Loss : 0.7830864191055298, Weights 1.0594912767410278, bias : 0.05768227577209473  \n",
      "Steps : 4910 , Loss : 0.7830862998962402, Weights 1.0594916343688965, bias : 0.05768201872706413  \n",
      "Steps : 4911 , Loss : 0.7830864191055298, Weights 1.0594919919967651, bias : 0.05768176168203354  \n",
      "Steps : 4912 , Loss : 0.7830864191055298, Weights 1.0594923496246338, bias : 0.057681504637002945  \n",
      "Steps : 4913 , Loss : 0.7830862402915955, Weights 1.0594927072525024, bias : 0.05768124759197235  \n",
      "Steps : 4914 , Loss : 0.7830864191055298, Weights 1.059493064880371, bias : 0.05768099054694176  \n",
      "Steps : 4915 , Loss : 0.7830864191055298, Weights 1.0594934225082397, bias : 0.05768073350191116  \n",
      "Steps : 4916 , Loss : 0.7830862998962402, Weights 1.0594937801361084, bias : 0.05768047645688057  \n",
      "Steps : 4917 , Loss : 0.7830864191055298, Weights 1.059494137763977, bias : 0.057680219411849976  \n",
      "Steps : 4918 , Loss : 0.7830864191055298, Weights 1.0594944953918457, bias : 0.05767996609210968  \n",
      "Steps : 4919 , Loss : 0.7830864191055298, Weights 1.0594948530197144, bias : 0.057679712772369385  \n",
      "Steps : 4920 , Loss : 0.7830864191055298, Weights 1.059495210647583, bias : 0.05767945945262909  \n",
      "Steps : 4921 , Loss : 0.7830864191055298, Weights 1.0594955682754517, bias : 0.057679206132888794  \n",
      "Steps : 4922 , Loss : 0.7830864191055298, Weights 1.0594959259033203, bias : 0.0576789528131485  \n",
      "Steps : 4923 , Loss : 0.7830864191055298, Weights 1.059496283531189, bias : 0.0576786994934082  \n",
      "Steps : 4924 , Loss : 0.7830864191055298, Weights 1.0594966411590576, bias : 0.05767844617366791  \n",
      "Steps : 4925 , Loss : 0.7830864191055298, Weights 1.0594969987869263, bias : 0.05767819285392761  \n",
      "Steps : 4926 , Loss : 0.7830864191055298, Weights 1.059497356414795, bias : 0.05767793953418732  \n",
      "Steps : 4927 , Loss : 0.7830864191055298, Weights 1.0594977140426636, bias : 0.05767768993973732  \n",
      "Steps : 4928 , Loss : 0.7830864191055298, Weights 1.0594980716705322, bias : 0.05767744034528732  \n",
      "Steps : 4929 , Loss : 0.7830864191055298, Weights 1.0594984292984009, bias : 0.057677190750837326  \n",
      "Steps : 4930 , Loss : 0.7830864191055298, Weights 1.0594987869262695, bias : 0.05767694115638733  \n",
      "Steps : 4931 , Loss : 0.7830864191055298, Weights 1.0594991445541382, bias : 0.05767669156193733  \n",
      "Steps : 4932 , Loss : 0.7830864191055298, Weights 1.0594995021820068, bias : 0.057676441967487335  \n",
      "Steps : 4933 , Loss : 0.7830864191055298, Weights 1.0594998598098755, bias : 0.05767619237303734  \n",
      "Steps : 4934 , Loss : 0.7830864191055298, Weights 1.0595002174377441, bias : 0.05767594277858734  \n",
      "Steps : 4935 , Loss : 0.7830862402915955, Weights 1.0595005750656128, bias : 0.05767569690942764  \n",
      "Steps : 4936 , Loss : 0.7830864191055298, Weights 1.0595009326934814, bias : 0.057675451040267944  \n",
      "Steps : 4937 , Loss : 0.7830862998962402, Weights 1.05950129032135, bias : 0.057675205171108246  \n",
      "Steps : 4938 , Loss : 0.7830864191055298, Weights 1.0595016479492188, bias : 0.05767495930194855  \n",
      "Steps : 4939 , Loss : 0.7830864191055298, Weights 1.0595020055770874, bias : 0.05767471343278885  \n",
      "Steps : 4940 , Loss : 0.7830864191055298, Weights 1.059502363204956, bias : 0.05767446756362915  \n",
      "Steps : 4941 , Loss : 0.7830864191055298, Weights 1.0595027208328247, bias : 0.05767422169446945  \n",
      "Steps : 4942 , Loss : 0.7830864191055298, Weights 1.0595030784606934, bias : 0.05767397582530975  \n",
      "Steps : 4943 , Loss : 0.7830864191055298, Weights 1.059503436088562, bias : 0.057673729956150055  \n",
      "Steps : 4944 , Loss : 0.7830864191055298, Weights 1.0595037937164307, bias : 0.057673487812280655  \n",
      "Steps : 4945 , Loss : 0.7830862998962402, Weights 1.0595041513442993, bias : 0.057673245668411255  \n",
      "Steps : 4946 , Loss : 0.7830862402915955, Weights 1.059504508972168, bias : 0.057673003524541855  \n",
      "Steps : 4947 , Loss : 0.7830862998962402, Weights 1.0595048666000366, bias : 0.057672761380672455  \n",
      "Steps : 4948 , Loss : 0.7830862998962402, Weights 1.0595052242279053, bias : 0.057672519236803055  \n",
      "Steps : 4949 , Loss : 0.7830862998962402, Weights 1.059505581855774, bias : 0.057672277092933655  \n",
      "Steps : 4950 , Loss : 0.7830864191055298, Weights 1.0595059394836426, bias : 0.057672034949064255  \n",
      "Steps : 4951 , Loss : 0.7830862998962402, Weights 1.0595062971115112, bias : 0.057671792805194855  \n",
      "Steps : 4952 , Loss : 0.7830864191055298, Weights 1.0595066547393799, bias : 0.05767155438661575  \n",
      "Steps : 4953 , Loss : 0.7830864191055298, Weights 1.0595070123672485, bias : 0.05767131596803665  \n",
      "Steps : 4954 , Loss : 0.7830862998962402, Weights 1.0595073699951172, bias : 0.05767107754945755  \n",
      "Steps : 4955 , Loss : 0.7830864191055298, Weights 1.0595076084136963, bias : 0.05767083913087845  \n",
      "Steps : 4956 , Loss : 0.7830864191055298, Weights 1.0595078468322754, bias : 0.05767060071229935  \n",
      "Steps : 4957 , Loss : 0.7830862402915955, Weights 1.0595080852508545, bias : 0.057670362293720245  \n",
      "Steps : 4958 , Loss : 0.7830864191055298, Weights 1.0595083236694336, bias : 0.057670123875141144  \n",
      "Steps : 4959 , Loss : 0.7830864191055298, Weights 1.0595085620880127, bias : 0.05766988545656204  \n",
      "Steps : 4960 , Loss : 0.7830864191055298, Weights 1.0595088005065918, bias : 0.05766964703798294  \n",
      "Steps : 4961 , Loss : 0.7830864191055298, Weights 1.059509038925171, bias : 0.05766941234469414  \n",
      "Steps : 4962 , Loss : 0.7830864191055298, Weights 1.05950927734375, bias : 0.057669177651405334  \n",
      "Steps : 4963 , Loss : 0.7830862402915955, Weights 1.059509515762329, bias : 0.05766894295811653  \n",
      "Steps : 4964 , Loss : 0.7830864191055298, Weights 1.0595097541809082, bias : 0.05766870826482773  \n",
      "Steps : 4965 , Loss : 0.7830864191055298, Weights 1.0595099925994873, bias : 0.057668473571538925  \n",
      "Steps : 4966 , Loss : 0.7830864191055298, Weights 1.0595102310180664, bias : 0.05766823887825012  \n",
      "Steps : 4967 , Loss : 0.7830864191055298, Weights 1.0595104694366455, bias : 0.05766800418496132  \n",
      "Steps : 4968 , Loss : 0.7830864191055298, Weights 1.0595107078552246, bias : 0.057667769491672516  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 4969 , Loss : 0.7830864191055298, Weights 1.0595109462738037, bias : 0.05766753479838371  \n",
      "Steps : 4970 , Loss : 0.7830864191055298, Weights 1.0595111846923828, bias : 0.05766730383038521  \n",
      "Steps : 4971 , Loss : 0.7830864191055298, Weights 1.059511423110962, bias : 0.057667072862386703  \n",
      "Steps : 4972 , Loss : 0.7830862998962402, Weights 1.059511661529541, bias : 0.0576668418943882  \n",
      "Steps : 4973 , Loss : 0.7830862998962402, Weights 1.0595118999481201, bias : 0.057666610926389694  \n",
      "Steps : 4974 , Loss : 0.7830864191055298, Weights 1.0595121383666992, bias : 0.05766637995839119  \n",
      "Steps : 4975 , Loss : 0.7830864191055298, Weights 1.0595123767852783, bias : 0.057666148990392685  \n",
      "Steps : 4976 , Loss : 0.7830862402915955, Weights 1.0595126152038574, bias : 0.05766591802239418  \n",
      "Steps : 4977 , Loss : 0.7830862998962402, Weights 1.0595128536224365, bias : 0.057665687054395676  \n",
      "Steps : 4978 , Loss : 0.7830864191055298, Weights 1.0595130920410156, bias : 0.05766545981168747  \n",
      "Steps : 4979 , Loss : 0.7830864191055298, Weights 1.0595133304595947, bias : 0.05766523256897926  \n",
      "Steps : 4980 , Loss : 0.7830864191055298, Weights 1.0595135688781738, bias : 0.05766500532627106  \n",
      "Steps : 4981 , Loss : 0.7830864191055298, Weights 1.059513807296753, bias : 0.05766477808356285  \n",
      "Steps : 4982 , Loss : 0.7830864191055298, Weights 1.059514045715332, bias : 0.057664550840854645  \n",
      "Steps : 4983 , Loss : 0.7830864191055298, Weights 1.0595142841339111, bias : 0.05766432359814644  \n",
      "Steps : 4984 , Loss : 0.7830864191055298, Weights 1.0595145225524902, bias : 0.05766409635543823  \n",
      "Steps : 4985 , Loss : 0.7830862998962402, Weights 1.0595147609710693, bias : 0.057663869112730026  \n",
      "Steps : 4986 , Loss : 0.7830864191055298, Weights 1.0595149993896484, bias : 0.05766364187002182  \n",
      "Steps : 4987 , Loss : 0.7830864191055298, Weights 1.0595152378082275, bias : 0.05766341835260391  \n",
      "Steps : 4988 , Loss : 0.7830864191055298, Weights 1.0595154762268066, bias : 0.057663194835186005  \n",
      "Steps : 4989 , Loss : 0.7830864191055298, Weights 1.0595157146453857, bias : 0.0576629713177681  \n",
      "Steps : 4990 , Loss : 0.7830862998962402, Weights 1.0595159530639648, bias : 0.05766274780035019  \n",
      "Steps : 4991 , Loss : 0.7830864191055298, Weights 1.059516191482544, bias : 0.05766252428293228  \n",
      "Steps : 4992 , Loss : 0.7830864191055298, Weights 1.059516429901123, bias : 0.057662300765514374  \n",
      "Steps : 4993 , Loss : 0.7830862998962402, Weights 1.0595166683197021, bias : 0.057662077248096466  \n",
      "Steps : 4994 , Loss : 0.7830864191055298, Weights 1.0595169067382812, bias : 0.05766185373067856  \n",
      "Steps : 4995 , Loss : 0.7830864191055298, Weights 1.0595171451568604, bias : 0.05766163021326065  \n",
      "Steps : 4996 , Loss : 0.7830864191055298, Weights 1.0595173835754395, bias : 0.05766141042113304  \n",
      "Steps : 4997 , Loss : 0.7830864191055298, Weights 1.0595176219940186, bias : 0.05766119062900543  \n",
      "Steps : 4998 , Loss : 0.7830862998962402, Weights 1.0595178604125977, bias : 0.05766097083687782  \n",
      "Steps : 4999 , Loss : 0.7830862402915955, Weights 1.0595180988311768, bias : 0.057660751044750214  \n",
      "Steps : 5000 , Loss : 0.7830864191055298, Weights 1.0595183372497559, bias : 0.057660531252622604  \n",
      "Steps : 5001 , Loss : 0.7830864191055298, Weights 1.059518575668335, bias : 0.057660311460494995  \n",
      "Steps : 5002 , Loss : 0.7830862998962402, Weights 1.059518814086914, bias : 0.057660091668367386  \n",
      "Steps : 5003 , Loss : 0.7830862998962402, Weights 1.0595190525054932, bias : 0.05765987187623978  \n",
      "Steps : 5004 , Loss : 0.7830862998962402, Weights 1.0595192909240723, bias : 0.05765965208411217  \n",
      "Steps : 5005 , Loss : 0.7830862402915955, Weights 1.0595195293426514, bias : 0.05765943229198456  \n",
      "Steps : 5006 , Loss : 0.7830862998962402, Weights 1.0595197677612305, bias : 0.05765921622514725  \n",
      "Steps : 5007 , Loss : 0.7830864191055298, Weights 1.0595200061798096, bias : 0.057659000158309937  \n",
      "Steps : 5008 , Loss : 0.7830862998962402, Weights 1.0595202445983887, bias : 0.057658784091472626  \n",
      "Steps : 5009 , Loss : 0.7830862998962402, Weights 1.0595204830169678, bias : 0.057658568024635315  \n",
      "Steps : 5010 , Loss : 0.7830862998962402, Weights 1.0595207214355469, bias : 0.057658351957798004  \n",
      "Steps : 5011 , Loss : 0.7830864191055298, Weights 1.059520959854126, bias : 0.05765813589096069  \n",
      "Steps : 5012 , Loss : 0.7830864191055298, Weights 1.059521198272705, bias : 0.05765791982412338  \n",
      "Steps : 5013 , Loss : 0.7830864191055298, Weights 1.0595214366912842, bias : 0.05765770375728607  \n",
      "Steps : 5014 , Loss : 0.7830864191055298, Weights 1.0595216751098633, bias : 0.05765748769044876  \n",
      "Steps : 5015 , Loss : 0.7830862998962402, Weights 1.0595219135284424, bias : 0.05765727534890175  \n",
      "Steps : 5016 , Loss : 0.7830862402915955, Weights 1.0595221519470215, bias : 0.057657063007354736  \n",
      "Steps : 5017 , Loss : 0.7830864191055298, Weights 1.0595223903656006, bias : 0.057656850665807724  \n",
      "Steps : 5018 , Loss : 0.7830862998962402, Weights 1.0595226287841797, bias : 0.05765663832426071  \n",
      "Steps : 5019 , Loss : 0.7830864191055298, Weights 1.0595228672027588, bias : 0.0576564259827137  \n",
      "Steps : 5020 , Loss : 0.7830862998962402, Weights 1.059523105621338, bias : 0.05765621364116669  \n",
      "Steps : 5021 , Loss : 0.7830862998962402, Weights 1.059523344039917, bias : 0.057656001299619675  \n",
      "Steps : 5022 , Loss : 0.7830864191055298, Weights 1.059523582458496, bias : 0.05765578895807266  \n",
      "Steps : 5023 , Loss : 0.7830864191055298, Weights 1.0595238208770752, bias : 0.05765557661652565  \n",
      "Steps : 5024 , Loss : 0.7830862402915955, Weights 1.0595240592956543, bias : 0.05765536427497864  \n",
      "Steps : 5025 , Loss : 0.7830862998962402, Weights 1.0595242977142334, bias : 0.057655155658721924  \n",
      "Steps : 5026 , Loss : 0.7830864191055298, Weights 1.0595245361328125, bias : 0.05765494704246521  \n",
      "Steps : 5027 , Loss : 0.7830864191055298, Weights 1.0595247745513916, bias : 0.057654738426208496  \n",
      "Steps : 5028 , Loss : 0.7830864191055298, Weights 1.0595250129699707, bias : 0.05765452980995178  \n",
      "Steps : 5029 , Loss : 0.7830862998962402, Weights 1.0595252513885498, bias : 0.05765432119369507  \n",
      "Steps : 5030 , Loss : 0.7830862998962402, Weights 1.059525489807129, bias : 0.057654112577438354  \n",
      "Steps : 5031 , Loss : 0.7830862402915955, Weights 1.059525728225708, bias : 0.05765390396118164  \n",
      "Steps : 5032 , Loss : 0.7830864191055298, Weights 1.059525966644287, bias : 0.05765369534492493  \n",
      "Steps : 5033 , Loss : 0.7830862402915955, Weights 1.0595262050628662, bias : 0.05765348672866821  \n",
      "Steps : 5034 , Loss : 0.7830862998962402, Weights 1.0595264434814453, bias : 0.0576532781124115  \n",
      "Steps : 5035 , Loss : 0.7830864191055298, Weights 1.0595266819000244, bias : 0.057653073221445084  \n",
      "Steps : 5036 , Loss : 0.7830862402915955, Weights 1.0595269203186035, bias : 0.05765286833047867  \n",
      "Steps : 5037 , Loss : 0.7830864191055298, Weights 1.0595271587371826, bias : 0.05765266343951225  \n",
      "Steps : 5038 , Loss : 0.7830862998962402, Weights 1.0595273971557617, bias : 0.05765245854854584  \n",
      "Steps : 5039 , Loss : 0.7830864191055298, Weights 1.0595276355743408, bias : 0.05765225365757942  \n",
      "Steps : 5040 , Loss : 0.7830864191055298, Weights 1.05952787399292, bias : 0.05765204876661301  \n",
      "Steps : 5041 , Loss : 0.7830864191055298, Weights 1.059528112411499, bias : 0.05765184387564659  \n",
      "Steps : 5042 , Loss : 0.7830864191055298, Weights 1.0595283508300781, bias : 0.057651638984680176  \n",
      "Steps : 5043 , Loss : 0.7830864191055298, Weights 1.0595285892486572, bias : 0.05765143409371376  \n",
      "Steps : 5044 , Loss : 0.7830864191055298, Weights 1.0595288276672363, bias : 0.057651229202747345  \n",
      "Steps : 5045 , Loss : 0.7830864191055298, Weights 1.0595290660858154, bias : 0.05765102803707123  \n",
      "Steps : 5046 , Loss : 0.7830864191055298, Weights 1.0595293045043945, bias : 0.05765082687139511  \n",
      "Steps : 5047 , Loss : 0.7830864191055298, Weights 1.0595295429229736, bias : 0.057650625705718994  \n",
      "Steps : 5048 , Loss : 0.7830864191055298, Weights 1.0595297813415527, bias : 0.05765042454004288  \n",
      "Steps : 5049 , Loss : 0.7830862998962402, Weights 1.0595300197601318, bias : 0.05765022337436676  \n",
      "Steps : 5050 , Loss : 0.7830864191055298, Weights 1.059530258178711, bias : 0.05765002220869064  \n",
      "Steps : 5051 , Loss : 0.7830864191055298, Weights 1.05953049659729, bias : 0.057649821043014526  \n",
      "Steps : 5052 , Loss : 0.7830862998962402, Weights 1.0595307350158691, bias : 0.05764961987733841  \n",
      "Steps : 5053 , Loss : 0.7830864191055298, Weights 1.0595309734344482, bias : 0.05764941871166229  \n",
      "Steps : 5054 , Loss : 0.7830864191055298, Weights 1.0595312118530273, bias : 0.057649217545986176  \n",
      "Steps : 5055 , Loss : 0.7830864191055298, Weights 1.0595314502716064, bias : 0.05764902010560036  \n",
      "Steps : 5056 , Loss : 0.7830864191055298, Weights 1.0595316886901855, bias : 0.05764882266521454  \n",
      "Steps : 5057 , Loss : 0.7830864191055298, Weights 1.0595319271087646, bias : 0.05764862522482872  \n",
      "Steps : 5058 , Loss : 0.7830862998962402, Weights 1.0595321655273438, bias : 0.0576484277844429  \n",
      "Steps : 5059 , Loss : 0.7830864191055298, Weights 1.0595324039459229, bias : 0.05764823034405708  \n",
      "Steps : 5060 , Loss : 0.7830862998962402, Weights 1.059532642364502, bias : 0.057648032903671265  \n",
      "Steps : 5061 , Loss : 0.7830864191055298, Weights 1.059532880783081, bias : 0.057647835463285446  \n",
      "Steps : 5062 , Loss : 0.7830862998962402, Weights 1.0595331192016602, bias : 0.05764763802289963  \n",
      "Steps : 5063 , Loss : 0.7830864191055298, Weights 1.0595333576202393, bias : 0.05764744058251381  \n",
      "Steps : 5064 , Loss : 0.7830862998962402, Weights 1.0595335960388184, bias : 0.05764724314212799  \n",
      "Steps : 5065 , Loss : 0.7830862998962402, Weights 1.0595338344573975, bias : 0.05764704942703247  \n",
      "Steps : 5066 , Loss : 0.7830864191055298, Weights 1.0595340728759766, bias : 0.05764685571193695  \n",
      "Steps : 5067 , Loss : 0.7830862998962402, Weights 1.0595343112945557, bias : 0.05764666199684143  \n",
      "Steps : 5068 , Loss : 0.7830862998962402, Weights 1.0595345497131348, bias : 0.05764646828174591  \n",
      "Steps : 5069 , Loss : 0.7830864191055298, Weights 1.0595347881317139, bias : 0.05764627456665039  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5070 , Loss : 0.7830864191055298, Weights 1.059535026550293, bias : 0.05764608085155487  \n",
      "Steps : 5071 , Loss : 0.7830864191055298, Weights 1.059535264968872, bias : 0.05764588713645935  \n",
      "Steps : 5072 , Loss : 0.7830862998962402, Weights 1.0595355033874512, bias : 0.05764569342136383  \n",
      "Steps : 5073 , Loss : 0.7830864191055298, Weights 1.0595357418060303, bias : 0.05764549970626831  \n",
      "Steps : 5074 , Loss : 0.7830864191055298, Weights 1.0595359802246094, bias : 0.05764530599117279  \n",
      "Steps : 5075 , Loss : 0.7830862998962402, Weights 1.0595362186431885, bias : 0.05764511227607727  \n",
      "Steps : 5076 , Loss : 0.7830862998962402, Weights 1.0595364570617676, bias : 0.05764492228627205  \n",
      "Steps : 5077 , Loss : 0.7830862402915955, Weights 1.0595366954803467, bias : 0.05764473229646683  \n",
      "Steps : 5078 , Loss : 0.7830862402915955, Weights 1.0595369338989258, bias : 0.057644542306661606  \n",
      "Steps : 5079 , Loss : 0.7830864191055298, Weights 1.0595371723175049, bias : 0.057644352316856384  \n",
      "Steps : 5080 , Loss : 0.7830862998962402, Weights 1.059537410736084, bias : 0.05764416232705116  \n",
      "Steps : 5081 , Loss : 0.7830862998962402, Weights 1.059537649154663, bias : 0.05764397233724594  \n",
      "Steps : 5082 , Loss : 0.7830864191055298, Weights 1.0595378875732422, bias : 0.05764378234744072  \n",
      "Steps : 5083 , Loss : 0.7830862402915955, Weights 1.0595381259918213, bias : 0.0576435923576355  \n",
      "Steps : 5084 , Loss : 0.7830862402915955, Weights 1.0595383644104004, bias : 0.057643402367830276  \n",
      "Steps : 5085 , Loss : 0.7830862402915955, Weights 1.0595386028289795, bias : 0.057643212378025055  \n",
      "Steps : 5086 , Loss : 0.7830864191055298, Weights 1.0595388412475586, bias : 0.05764302238821983  \n",
      "Steps : 5087 , Loss : 0.7830864191055298, Weights 1.0595390796661377, bias : 0.05764283612370491  \n",
      "Steps : 5088 , Loss : 0.7830864191055298, Weights 1.0595393180847168, bias : 0.05764264985918999  \n",
      "Steps : 5089 , Loss : 0.7830862998962402, Weights 1.059539556503296, bias : 0.057642463594675064  \n",
      "Steps : 5090 , Loss : 0.7830864191055298, Weights 1.059539794921875, bias : 0.05764227733016014  \n",
      "Steps : 5091 , Loss : 0.7830864191055298, Weights 1.059540033340454, bias : 0.05764209106564522  \n",
      "Steps : 5092 , Loss : 0.7830862998962402, Weights 1.0595402717590332, bias : 0.057641904801130295  \n",
      "Steps : 5093 , Loss : 0.7830862402915955, Weights 1.0595405101776123, bias : 0.05764171853661537  \n",
      "Steps : 5094 , Loss : 0.7830862402915955, Weights 1.0595407485961914, bias : 0.05764153227210045  \n",
      "Steps : 5095 , Loss : 0.7830864191055298, Weights 1.0595409870147705, bias : 0.057641346007585526  \n",
      "Steps : 5096 , Loss : 0.7830864191055298, Weights 1.0595412254333496, bias : 0.0576411597430706  \n",
      "Steps : 5097 , Loss : 0.7830862998962402, Weights 1.0595414638519287, bias : 0.05764097347855568  \n",
      "Steps : 5098 , Loss : 0.7830862402915955, Weights 1.0595417022705078, bias : 0.057640790939331055  \n",
      "Steps : 5099 , Loss : 0.7830864191055298, Weights 1.059541940689087, bias : 0.05764060840010643  \n",
      "Steps : 5100 , Loss : 0.7830864191055298, Weights 1.059542179107666, bias : 0.057640425860881805  \n",
      "Steps : 5101 , Loss : 0.7830864191055298, Weights 1.0595424175262451, bias : 0.05764024332165718  \n",
      "Steps : 5102 , Loss : 0.7830864191055298, Weights 1.0595426559448242, bias : 0.057640060782432556  \n",
      "Steps : 5103 , Loss : 0.7830864191055298, Weights 1.0595428943634033, bias : 0.05763987824320793  \n",
      "Steps : 5104 , Loss : 0.7830864191055298, Weights 1.0595431327819824, bias : 0.05763969570398331  \n",
      "Steps : 5105 , Loss : 0.7830864191055298, Weights 1.0595433712005615, bias : 0.05763951316475868  \n",
      "Steps : 5106 , Loss : 0.7830864191055298, Weights 1.0595436096191406, bias : 0.05763933062553406  \n",
      "Steps : 5107 , Loss : 0.7830864191055298, Weights 1.0595438480377197, bias : 0.05763914808630943  \n",
      "Steps : 5108 , Loss : 0.7830864191055298, Weights 1.0595440864562988, bias : 0.05763896554708481  \n",
      "Steps : 5109 , Loss : 0.7830862998962402, Weights 1.059544324874878, bias : 0.05763878673315048  \n",
      "Steps : 5110 , Loss : 0.7830864191055298, Weights 1.059544563293457, bias : 0.057638607919216156  \n",
      "Steps : 5111 , Loss : 0.7830864191055298, Weights 1.0595448017120361, bias : 0.05763842910528183  \n",
      "Steps : 5112 , Loss : 0.7830862402915955, Weights 1.0595450401306152, bias : 0.057638250291347504  \n",
      "Steps : 5113 , Loss : 0.7830862402915955, Weights 1.0595452785491943, bias : 0.05763807147741318  \n",
      "Steps : 5114 , Loss : 0.7830862998962402, Weights 1.0595455169677734, bias : 0.05763789266347885  \n",
      "Steps : 5115 , Loss : 0.7830864191055298, Weights 1.0595457553863525, bias : 0.057637713849544525  \n",
      "Steps : 5116 , Loss : 0.7830862402915955, Weights 1.0595459938049316, bias : 0.0576375350356102  \n",
      "Steps : 5117 , Loss : 0.7830862402915955, Weights 1.0595462322235107, bias : 0.05763735622167587  \n",
      "Steps : 5118 , Loss : 0.7830862402915955, Weights 1.0595464706420898, bias : 0.05763717740774155  \n",
      "Steps : 5119 , Loss : 0.7830862998962402, Weights 1.059546709060669, bias : 0.05763699859380722  \n",
      "Steps : 5120 , Loss : 0.7830862998962402, Weights 1.059546947479248, bias : 0.057636819779872894  \n",
      "Steps : 5121 , Loss : 0.7830864191055298, Weights 1.0595471858978271, bias : 0.05763664469122887  \n",
      "Steps : 5122 , Loss : 0.7830864191055298, Weights 1.0595474243164062, bias : 0.05763646960258484  \n",
      "Steps : 5123 , Loss : 0.7830862998962402, Weights 1.0595476627349854, bias : 0.05763629451394081  \n",
      "Steps : 5124 , Loss : 0.7830862998962402, Weights 1.0595479011535645, bias : 0.05763611942529678  \n",
      "Steps : 5125 , Loss : 0.7830862402915955, Weights 1.0595481395721436, bias : 0.057635944336652756  \n",
      "Steps : 5126 , Loss : 0.7830864191055298, Weights 1.0595483779907227, bias : 0.05763576924800873  \n",
      "Steps : 5127 , Loss : 0.7830864191055298, Weights 1.0595486164093018, bias : 0.0576355941593647  \n",
      "Steps : 5128 , Loss : 0.7830862402915955, Weights 1.0595488548278809, bias : 0.05763541907072067  \n",
      "Steps : 5129 , Loss : 0.7830864191055298, Weights 1.05954909324646, bias : 0.057635243982076645  \n",
      "Steps : 5130 , Loss : 0.7830862998962402, Weights 1.059549331665039, bias : 0.05763506889343262  \n",
      "Steps : 5131 , Loss : 0.7830864191055298, Weights 1.0595495700836182, bias : 0.05763489380478859  \n",
      "Steps : 5132 , Loss : 0.7830864191055298, Weights 1.0595498085021973, bias : 0.05763471871614456  \n",
      "Steps : 5133 , Loss : 0.7830862998962402, Weights 1.0595500469207764, bias : 0.05763454735279083  \n",
      "Steps : 5134 , Loss : 0.7830864191055298, Weights 1.0595502853393555, bias : 0.0576343759894371  \n",
      "Steps : 5135 , Loss : 0.7830862998962402, Weights 1.0595505237579346, bias : 0.057634204626083374  \n",
      "Steps : 5136 , Loss : 0.7830862998962402, Weights 1.0595507621765137, bias : 0.057634033262729645  \n",
      "Steps : 5137 , Loss : 0.7830864191055298, Weights 1.0595510005950928, bias : 0.057633861899375916  \n",
      "Steps : 5138 , Loss : 0.7830862998962402, Weights 1.0595512390136719, bias : 0.057633690536022186  \n",
      "Steps : 5139 , Loss : 0.7830862998962402, Weights 1.059551477432251, bias : 0.05763351917266846  \n",
      "Steps : 5140 , Loss : 0.7830864191055298, Weights 1.05955171585083, bias : 0.05763334780931473  \n",
      "Steps : 5141 , Loss : 0.7830862402915955, Weights 1.0595519542694092, bias : 0.057633176445961  \n",
      "Steps : 5142 , Loss : 0.7830864191055298, Weights 1.0595521926879883, bias : 0.05763300508260727  \n",
      "Steps : 5143 , Loss : 0.7830862402915955, Weights 1.0595524311065674, bias : 0.05763283371925354  \n",
      "Steps : 5144 , Loss : 0.7830862402915955, Weights 1.0595526695251465, bias : 0.05763266235589981  \n",
      "Steps : 5145 , Loss : 0.7830862998962402, Weights 1.0595529079437256, bias : 0.05763249471783638  \n",
      "Steps : 5146 , Loss : 0.7830864191055298, Weights 1.0595531463623047, bias : 0.05763232707977295  \n",
      "Steps : 5147 , Loss : 0.7830864191055298, Weights 1.0595533847808838, bias : 0.05763215944170952  \n",
      "Steps : 5148 , Loss : 0.7830864191055298, Weights 1.059553623199463, bias : 0.05763199180364609  \n",
      "Steps : 5149 , Loss : 0.7830862402915955, Weights 1.059553861618042, bias : 0.05763182416558266  \n",
      "Steps : 5150 , Loss : 0.7830862402915955, Weights 1.059554100036621, bias : 0.057631656527519226  \n",
      "Steps : 5151 , Loss : 0.7830864191055298, Weights 1.0595543384552002, bias : 0.057631488889455795  \n",
      "Steps : 5152 , Loss : 0.7830862402915955, Weights 1.0595545768737793, bias : 0.057631321251392365  \n",
      "Steps : 5153 , Loss : 0.7830862998962402, Weights 1.0595548152923584, bias : 0.057631153613328934  \n",
      "Steps : 5154 , Loss : 0.7830862998962402, Weights 1.0595550537109375, bias : 0.0576309859752655  \n",
      "Steps : 5155 , Loss : 0.7830862402915955, Weights 1.0595552921295166, bias : 0.05763081833720207  \n",
      "Steps : 5156 , Loss : 0.7830862998962402, Weights 1.0595555305480957, bias : 0.05763065069913864  \n",
      "Steps : 5157 , Loss : 0.7830864191055298, Weights 1.0595557689666748, bias : 0.05763048306107521  \n",
      "Steps : 5158 , Loss : 0.7830862402915955, Weights 1.059556007385254, bias : 0.05763031914830208  \n",
      "Steps : 5159 , Loss : 0.7830862402915955, Weights 1.059556245803833, bias : 0.057630155235528946  \n",
      "Steps : 5160 , Loss : 0.7830862998962402, Weights 1.059556484222412, bias : 0.057629991322755814  \n",
      "Steps : 5161 , Loss : 0.7830864191055298, Weights 1.0595567226409912, bias : 0.05762982740998268  \n",
      "Steps : 5162 , Loss : 0.7830862402915955, Weights 1.0595569610595703, bias : 0.05762966349720955  \n",
      "Steps : 5163 , Loss : 0.7830862998962402, Weights 1.0595571994781494, bias : 0.05762949958443642  \n",
      "Steps : 5164 , Loss : 0.7830862998962402, Weights 1.0595574378967285, bias : 0.057629335671663284  \n",
      "Steps : 5165 , Loss : 0.7830862998962402, Weights 1.0595576763153076, bias : 0.05762917175889015  \n",
      "Steps : 5166 , Loss : 0.7830864191055298, Weights 1.0595579147338867, bias : 0.05762900784611702  \n",
      "Steps : 5167 , Loss : 0.7830864191055298, Weights 1.0595581531524658, bias : 0.05762884393334389  \n",
      "Steps : 5168 , Loss : 0.7830862402915955, Weights 1.059558391571045, bias : 0.057628680020570755  \n",
      "Steps : 5169 , Loss : 0.7830862998962402, Weights 1.059558629989624, bias : 0.05762851610779762  \n",
      "Steps : 5170 , Loss : 0.7830862998962402, Weights 1.0595588684082031, bias : 0.05762835592031479  \n",
      "Steps : 5171 , Loss : 0.7830862998962402, Weights 1.0595591068267822, bias : 0.057628195732831955  \n",
      "Steps : 5172 , Loss : 0.7830862402915955, Weights 1.0595593452453613, bias : 0.05762803554534912  \n",
      "Steps : 5173 , Loss : 0.7830862402915955, Weights 1.0595595836639404, bias : 0.05762787535786629  \n",
      "Steps : 5174 , Loss : 0.7830862998962402, Weights 1.0595598220825195, bias : 0.05762771517038345  \n",
      "Steps : 5175 , Loss : 0.7830862998962402, Weights 1.0595600605010986, bias : 0.05762755498290062  \n",
      "Steps : 5176 , Loss : 0.7830862402915955, Weights 1.0595602989196777, bias : 0.057627394795417786  \n",
      "Steps : 5177 , Loss : 0.7830862998962402, Weights 1.0595605373382568, bias : 0.05762723460793495  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5178 , Loss : 0.7830862402915955, Weights 1.059560775756836, bias : 0.05762707442045212  \n",
      "Steps : 5179 , Loss : 0.7830864191055298, Weights 1.059561014175415, bias : 0.057626914232969284  \n",
      "Steps : 5180 , Loss : 0.7830864191055298, Weights 1.0595612525939941, bias : 0.05762675404548645  \n",
      "Steps : 5181 , Loss : 0.7830864191055298, Weights 1.0595614910125732, bias : 0.057626593858003616  \n",
      "Steps : 5182 , Loss : 0.7830864191055298, Weights 1.0595617294311523, bias : 0.05762643367052078  \n",
      "Steps : 5183 , Loss : 0.7830862402915955, Weights 1.0595619678497314, bias : 0.05762627348303795  \n",
      "Steps : 5184 , Loss : 0.7830862998962402, Weights 1.0595622062683105, bias : 0.05762611702084541  \n",
      "Steps : 5185 , Loss : 0.7830862998962402, Weights 1.0595624446868896, bias : 0.05762596055865288  \n",
      "Steps : 5186 , Loss : 0.7830864191055298, Weights 1.0595626831054688, bias : 0.05762580409646034  \n",
      "Steps : 5187 , Loss : 0.7830862998962402, Weights 1.0595629215240479, bias : 0.05762564763426781  \n",
      "Steps : 5188 , Loss : 0.7830862998962402, Weights 1.059563159942627, bias : 0.05762549117207527  \n",
      "Steps : 5189 , Loss : 0.7830862402915955, Weights 1.059563398361206, bias : 0.057625334709882736  \n",
      "Steps : 5190 , Loss : 0.7830862402915955, Weights 1.0595636367797852, bias : 0.0576251782476902  \n",
      "Steps : 5191 , Loss : 0.7830862998962402, Weights 1.0595638751983643, bias : 0.057625021785497665  \n",
      "Steps : 5192 , Loss : 0.7830862998962402, Weights 1.0595641136169434, bias : 0.05762486532330513  \n",
      "Steps : 5193 , Loss : 0.7830862998962402, Weights 1.0595643520355225, bias : 0.057624708861112595  \n",
      "Steps : 5194 , Loss : 0.7830862998962402, Weights 1.0595645904541016, bias : 0.05762455239892006  \n",
      "Steps : 5195 , Loss : 0.7830862998962402, Weights 1.0595648288726807, bias : 0.057624395936727524  \n",
      "Steps : 5196 , Loss : 0.7830864191055298, Weights 1.0595650672912598, bias : 0.05762423947453499  \n",
      "Steps : 5197 , Loss : 0.7830862998962402, Weights 1.0595653057098389, bias : 0.05762408673763275  \n",
      "Steps : 5198 , Loss : 0.7830864191055298, Weights 1.059565544128418, bias : 0.057623934000730515  \n",
      "Steps : 5199 , Loss : 0.7830864191055298, Weights 1.059565782546997, bias : 0.05762378126382828  \n",
      "Steps : 5200 , Loss : 0.7830862998962402, Weights 1.0595660209655762, bias : 0.05762362852692604  \n",
      "Steps : 5201 , Loss : 0.7830864191055298, Weights 1.0595662593841553, bias : 0.057623475790023804  \n",
      "Steps : 5202 , Loss : 0.7830864191055298, Weights 1.0595664978027344, bias : 0.05762332305312157  \n",
      "Steps : 5203 , Loss : 0.7830862998962402, Weights 1.0595667362213135, bias : 0.05762317031621933  \n",
      "Steps : 5204 , Loss : 0.7830864191055298, Weights 1.0595669746398926, bias : 0.05762301757931709  \n",
      "Steps : 5205 , Loss : 0.7830864191055298, Weights 1.0595672130584717, bias : 0.057622864842414856  \n",
      "Steps : 5206 , Loss : 0.7830862998962402, Weights 1.0595674514770508, bias : 0.05762271210551262  \n",
      "Steps : 5207 , Loss : 0.7830862402915955, Weights 1.0595676898956299, bias : 0.05762255936861038  \n",
      "Steps : 5208 , Loss : 0.7830862998962402, Weights 1.059567928314209, bias : 0.057622406631708145  \n",
      "Steps : 5209 , Loss : 0.7830862998962402, Weights 1.059568166732788, bias : 0.05762225389480591  \n",
      "Steps : 5210 , Loss : 0.7830862998962402, Weights 1.0595684051513672, bias : 0.05762210115790367  \n",
      "Steps : 5211 , Loss : 0.7830862402915955, Weights 1.0595686435699463, bias : 0.05762195214629173  \n",
      "Steps : 5212 , Loss : 0.7830864191055298, Weights 1.0595688819885254, bias : 0.057621803134679794  \n",
      "Steps : 5213 , Loss : 0.7830862998962402, Weights 1.0595691204071045, bias : 0.057621654123067856  \n",
      "Steps : 5214 , Loss : 0.7830862998962402, Weights 1.0595693588256836, bias : 0.05762150511145592  \n",
      "Steps : 5215 , Loss : 0.7830864191055298, Weights 1.0595695972442627, bias : 0.05762135609984398  \n",
      "Steps : 5216 , Loss : 0.7830864191055298, Weights 1.0595698356628418, bias : 0.05762120708823204  \n",
      "Steps : 5217 , Loss : 0.7830864191055298, Weights 1.059570074081421, bias : 0.0576210580766201  \n",
      "Steps : 5218 , Loss : 0.7830862998962402, Weights 1.0595703125, bias : 0.05762090906500816  \n",
      "Steps : 5219 , Loss : 0.7830862998962402, Weights 1.059570550918579, bias : 0.057620760053396225  \n",
      "Steps : 5220 , Loss : 0.7830862998962402, Weights 1.0595707893371582, bias : 0.057620611041784286  \n",
      "Steps : 5221 , Loss : 0.7830862998962402, Weights 1.0595710277557373, bias : 0.05762046203017235  \n",
      "Steps : 5222 , Loss : 0.7830862402915955, Weights 1.0595712661743164, bias : 0.05762031301856041  \n",
      "Steps : 5223 , Loss : 0.7830862402915955, Weights 1.0595715045928955, bias : 0.05762016400694847  \n",
      "Steps : 5224 , Loss : 0.7830862998962402, Weights 1.0595717430114746, bias : 0.05762001499533653  \n",
      "Steps : 5225 , Loss : 0.7830864191055298, Weights 1.0595719814300537, bias : 0.05761986970901489  \n",
      "Steps : 5226 , Loss : 0.7830862998962402, Weights 1.0595722198486328, bias : 0.05761972442269325  \n",
      "Steps : 5227 , Loss : 0.7830862998962402, Weights 1.059572458267212, bias : 0.05761957913637161  \n",
      "Steps : 5228 , Loss : 0.7830862998962402, Weights 1.059572696685791, bias : 0.05761943385004997  \n",
      "Steps : 5229 , Loss : 0.7830864191055298, Weights 1.0595729351043701, bias : 0.05761928856372833  \n",
      "Steps : 5230 , Loss : 0.7830862402915955, Weights 1.0595731735229492, bias : 0.05761914327740669  \n",
      "Steps : 5231 , Loss : 0.7830862998962402, Weights 1.0595734119415283, bias : 0.05761899799108505  \n",
      "Steps : 5232 , Loss : 0.7830862998962402, Weights 1.0595736503601074, bias : 0.05761885270476341  \n",
      "Steps : 5233 , Loss : 0.7830862998962402, Weights 1.0595738887786865, bias : 0.05761870741844177  \n",
      "Steps : 5234 , Loss : 0.7830862402915955, Weights 1.0595741271972656, bias : 0.05761856213212013  \n",
      "Steps : 5235 , Loss : 0.7830862998962402, Weights 1.0595743656158447, bias : 0.05761841684579849  \n",
      "Steps : 5236 , Loss : 0.7830864191055298, Weights 1.0595746040344238, bias : 0.05761827155947685  \n",
      "Steps : 5237 , Loss : 0.7830862998962402, Weights 1.059574842453003, bias : 0.05761812627315521  \n",
      "Steps : 5238 , Loss : 0.7830864191055298, Weights 1.0595749616622925, bias : 0.05761798098683357  \n",
      "Steps : 5239 , Loss : 0.7830862402915955, Weights 1.059575080871582, bias : 0.05761783570051193  \n",
      "Steps : 5240 , Loss : 0.7830862402915955, Weights 1.0595752000808716, bias : 0.05761769413948059  \n",
      "Steps : 5241 , Loss : 0.7830862998962402, Weights 1.0595753192901611, bias : 0.05761755257844925  \n",
      "Steps : 5242 , Loss : 0.7830862998962402, Weights 1.0595754384994507, bias : 0.05761741101741791  \n",
      "Steps : 5243 , Loss : 0.7830862998962402, Weights 1.0595755577087402, bias : 0.057617269456386566  \n",
      "Steps : 5244 , Loss : 0.7830864191055298, Weights 1.0595756769180298, bias : 0.057617127895355225  \n",
      "Steps : 5245 , Loss : 0.7830864191055298, Weights 1.0595757961273193, bias : 0.05761698633432388  \n",
      "Steps : 5246 , Loss : 0.7830862402915955, Weights 1.0595759153366089, bias : 0.05761684477329254  \n",
      "Steps : 5247 , Loss : 0.7830862402915955, Weights 1.0595760345458984, bias : 0.0576167032122612  \n",
      "Steps : 5248 , Loss : 0.7830862998962402, Weights 1.059576153755188, bias : 0.05761656165122986  \n",
      "Steps : 5249 , Loss : 0.7830864191055298, Weights 1.0595762729644775, bias : 0.05761642009019852  \n",
      "Steps : 5250 , Loss : 0.7830862998962402, Weights 1.059576392173767, bias : 0.057616278529167175  \n",
      "Steps : 5251 , Loss : 0.7830862998962402, Weights 1.0595765113830566, bias : 0.057616136968135834  \n",
      "Steps : 5252 , Loss : 0.7830862402915955, Weights 1.0595766305923462, bias : 0.05761599540710449  \n",
      "Steps : 5253 , Loss : 0.7830862998962402, Weights 1.0595767498016357, bias : 0.05761585384607315  \n",
      "Steps : 5254 , Loss : 0.7830864191055298, Weights 1.0595768690109253, bias : 0.05761571601033211  \n",
      "Steps : 5255 , Loss : 0.7830862998962402, Weights 1.0595769882202148, bias : 0.057615578174591064  \n",
      "Steps : 5256 , Loss : 0.7830862402915955, Weights 1.0595771074295044, bias : 0.05761544033885002  \n",
      "Steps : 5257 , Loss : 0.7830862998962402, Weights 1.059577226638794, bias : 0.05761530250310898  \n",
      "Steps : 5258 , Loss : 0.7830862402915955, Weights 1.0595773458480835, bias : 0.057615164667367935  \n",
      "Steps : 5259 , Loss : 0.7830862998962402, Weights 1.059577465057373, bias : 0.05761502683162689  \n",
      "Steps : 5260 , Loss : 0.7830864191055298, Weights 1.0595775842666626, bias : 0.05761488899588585  \n",
      "Steps : 5261 , Loss : 0.7830864191055298, Weights 1.0595777034759521, bias : 0.057614751160144806  \n",
      "Steps : 5262 , Loss : 0.7830864191055298, Weights 1.0595778226852417, bias : 0.05761461332440376  \n",
      "Steps : 5263 , Loss : 0.7830862402915955, Weights 1.0595779418945312, bias : 0.05761447548866272  \n",
      "Steps : 5264 , Loss : 0.7830862402915955, Weights 1.0595780611038208, bias : 0.05761433765292168  \n",
      "Steps : 5265 , Loss : 0.7830862402915955, Weights 1.0595781803131104, bias : 0.057614199817180634  \n",
      "Steps : 5266 , Loss : 0.7830862998962402, Weights 1.0595782995224, bias : 0.05761406198143959  \n",
      "Steps : 5267 , Loss : 0.7830864191055298, Weights 1.0595784187316895, bias : 0.05761392414569855  \n",
      "Steps : 5268 , Loss : 0.7830862998962402, Weights 1.059578537940979, bias : 0.0576137900352478  \n",
      "Steps : 5269 , Loss : 0.7830862998962402, Weights 1.0595786571502686, bias : 0.05761365592479706  \n",
      "Steps : 5270 , Loss : 0.7830862998962402, Weights 1.059578776359558, bias : 0.057613521814346313  \n",
      "Steps : 5271 , Loss : 0.7830864191055298, Weights 1.0595788955688477, bias : 0.05761338770389557  \n",
      "Steps : 5272 , Loss : 0.7830862998962402, Weights 1.0595790147781372, bias : 0.057613253593444824  \n",
      "Steps : 5273 , Loss : 0.7830862998962402, Weights 1.0595791339874268, bias : 0.05761311948299408  \n",
      "Steps : 5274 , Loss : 0.7830862402915955, Weights 1.0595792531967163, bias : 0.057612985372543335  \n",
      "Steps : 5275 , Loss : 0.7830862998962402, Weights 1.0595793724060059, bias : 0.05761285126209259  \n",
      "Steps : 5276 , Loss : 0.7830864191055298, Weights 1.0595794916152954, bias : 0.057612717151641846  \n",
      "Steps : 5277 , Loss : 0.7830862998962402, Weights 1.059579610824585, bias : 0.0576125830411911  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5278 , Loss : 0.7830862998962402, Weights 1.0595797300338745, bias : 0.057612448930740356  \n",
      "Steps : 5279 , Loss : 0.7830862998962402, Weights 1.059579849243164, bias : 0.05761231482028961  \n",
      "Steps : 5280 , Loss : 0.7830862998962402, Weights 1.0595799684524536, bias : 0.05761218070983887  \n",
      "Steps : 5281 , Loss : 0.7830862402915955, Weights 1.0595800876617432, bias : 0.05761204659938812  \n",
      "Steps : 5282 , Loss : 0.7830862998962402, Weights 1.0595802068710327, bias : 0.05761191248893738  \n",
      "Steps : 5283 , Loss : 0.7830862998962402, Weights 1.0595803260803223, bias : 0.05761178210377693  \n",
      "Steps : 5284 , Loss : 0.7830862998962402, Weights 1.0595804452896118, bias : 0.057611651718616486  \n",
      "Steps : 5285 , Loss : 0.7830862998962402, Weights 1.0595805644989014, bias : 0.05761152133345604  \n",
      "Steps : 5286 , Loss : 0.7830862998962402, Weights 1.059580683708191, bias : 0.05761139094829559  \n",
      "Steps : 5287 , Loss : 0.7830862998962402, Weights 1.0595808029174805, bias : 0.05761126056313515  \n",
      "Steps : 5288 , Loss : 0.7830862998962402, Weights 1.05958092212677, bias : 0.0576111301779747  \n",
      "Steps : 5289 , Loss : 0.7830862402915955, Weights 1.0595810413360596, bias : 0.057610999792814255  \n",
      "Steps : 5290 , Loss : 0.7830864191055298, Weights 1.0595811605453491, bias : 0.05761086940765381  \n",
      "Steps : 5291 , Loss : 0.7830862402915955, Weights 1.0595812797546387, bias : 0.05761073902249336  \n",
      "Steps : 5292 , Loss : 0.7830862402915955, Weights 1.0595813989639282, bias : 0.057610608637332916  \n",
      "Steps : 5293 , Loss : 0.7830862998962402, Weights 1.0595815181732178, bias : 0.05761047825217247  \n",
      "Steps : 5294 , Loss : 0.7830862402915955, Weights 1.0595816373825073, bias : 0.057610347867012024  \n",
      "Steps : 5295 , Loss : 0.7830862402915955, Weights 1.0595817565917969, bias : 0.05761021748185158  \n",
      "Steps : 5296 , Loss : 0.7830862402915955, Weights 1.0595818758010864, bias : 0.05761008709669113  \n",
      "Steps : 5297 , Loss : 0.7830862402915955, Weights 1.059581995010376, bias : 0.057609956711530685  \n",
      "Steps : 5298 , Loss : 0.7830862998962402, Weights 1.0595821142196655, bias : 0.05760982632637024  \n",
      "Steps : 5299 , Loss : 0.7830864191055298, Weights 1.059582233428955, bias : 0.05760969966650009  \n",
      "Steps : 5300 , Loss : 0.7830864191055298, Weights 1.0595823526382446, bias : 0.057609573006629944  \n",
      "Steps : 5301 , Loss : 0.7830862402915955, Weights 1.0595824718475342, bias : 0.057609446346759796  \n",
      "Steps : 5302 , Loss : 0.7830864191055298, Weights 1.0595825910568237, bias : 0.05760931968688965  \n",
      "Steps : 5303 , Loss : 0.7830862402915955, Weights 1.0595827102661133, bias : 0.0576091930270195  \n",
      "Steps : 5304 , Loss : 0.7830864191055298, Weights 1.0595828294754028, bias : 0.05760906636714935  \n",
      "Steps : 5305 , Loss : 0.7830862998962402, Weights 1.0595829486846924, bias : 0.057608939707279205  \n",
      "Steps : 5306 , Loss : 0.7830862998962402, Weights 1.059583067893982, bias : 0.05760881304740906  \n",
      "Steps : 5307 , Loss : 0.7830864191055298, Weights 1.0595831871032715, bias : 0.05760868638753891  \n",
      "Steps : 5308 , Loss : 0.7830864191055298, Weights 1.059583306312561, bias : 0.05760855972766876  \n",
      "Steps : 5309 , Loss : 0.7830864191055298, Weights 1.0595834255218506, bias : 0.057608433067798615  \n",
      "Steps : 5310 , Loss : 0.7830864191055298, Weights 1.0595835447311401, bias : 0.05760830640792847  \n",
      "Steps : 5311 , Loss : 0.7830862998962402, Weights 1.0595836639404297, bias : 0.05760817974805832  \n",
      "Steps : 5312 , Loss : 0.7830862402915955, Weights 1.0595837831497192, bias : 0.05760805308818817  \n",
      "Steps : 5313 , Loss : 0.7830864191055298, Weights 1.0595839023590088, bias : 0.057607926428318024  \n",
      "Steps : 5314 , Loss : 0.7830862998962402, Weights 1.0595840215682983, bias : 0.057607799768447876  \n",
      "Steps : 5315 , Loss : 0.7830862998962402, Weights 1.059584140777588, bias : 0.05760767683386803  \n",
      "Steps : 5316 , Loss : 0.7830862402915955, Weights 1.0595842599868774, bias : 0.05760755389928818  \n",
      "Steps : 5317 , Loss : 0.7830862402915955, Weights 1.059584379196167, bias : 0.05760743096470833  \n",
      "Steps : 5318 , Loss : 0.7830862998962402, Weights 1.0595844984054565, bias : 0.05760730803012848  \n",
      "Steps : 5319 , Loss : 0.7830862402915955, Weights 1.059584617614746, bias : 0.05760718509554863  \n",
      "Steps : 5320 , Loss : 0.7830862402915955, Weights 1.0595847368240356, bias : 0.05760706216096878  \n",
      "Steps : 5321 , Loss : 0.7830862402915955, Weights 1.0595848560333252, bias : 0.05760693922638893  \n",
      "Steps : 5322 , Loss : 0.7830862402915955, Weights 1.0595849752426147, bias : 0.05760681629180908  \n",
      "Steps : 5323 , Loss : 0.7830862998962402, Weights 1.0595850944519043, bias : 0.05760669335722923  \n",
      "Steps : 5324 , Loss : 0.7830864191055298, Weights 1.0595852136611938, bias : 0.057606570422649384  \n",
      "Steps : 5325 , Loss : 0.7830862998962402, Weights 1.0595853328704834, bias : 0.057606447488069534  \n",
      "Steps : 5326 , Loss : 0.7830862998962402, Weights 1.059585452079773, bias : 0.057606324553489685  \n",
      "Steps : 5327 , Loss : 0.7830862402915955, Weights 1.0595855712890625, bias : 0.057606201618909836  \n",
      "Steps : 5328 , Loss : 0.7830862402915955, Weights 1.059585690498352, bias : 0.05760607868432999  \n",
      "Steps : 5329 , Loss : 0.7830864191055298, Weights 1.0595858097076416, bias : 0.05760595574975014  \n",
      "Steps : 5330 , Loss : 0.7830864191055298, Weights 1.0595859289169312, bias : 0.05760583281517029  \n",
      "Steps : 5331 , Loss : 0.7830862998962402, Weights 1.0595860481262207, bias : 0.05760571360588074  \n",
      "Steps : 5332 , Loss : 0.7830862998962402, Weights 1.0595861673355103, bias : 0.057605594396591187  \n",
      "Steps : 5333 , Loss : 0.7830862402915955, Weights 1.0595862865447998, bias : 0.057605475187301636  \n",
      "Steps : 5334 , Loss : 0.7830862402915955, Weights 1.0595864057540894, bias : 0.057605355978012085  \n",
      "Steps : 5335 , Loss : 0.7830862402915955, Weights 1.059586524963379, bias : 0.057605236768722534  \n",
      "Steps : 5336 , Loss : 0.7830862998962402, Weights 1.0595866441726685, bias : 0.05760511755943298  \n",
      "Steps : 5337 , Loss : 0.7830862998962402, Weights 1.059586763381958, bias : 0.05760499835014343  \n",
      "Steps : 5338 , Loss : 0.7830864191055298, Weights 1.0595868825912476, bias : 0.05760487914085388  \n",
      "Steps : 5339 , Loss : 0.7830862998962402, Weights 1.059587001800537, bias : 0.05760475993156433  \n",
      "Steps : 5340 , Loss : 0.7830862998962402, Weights 1.0595871210098267, bias : 0.05760464072227478  \n",
      "Steps : 5341 , Loss : 0.7830862402915955, Weights 1.0595872402191162, bias : 0.05760452151298523  \n",
      "Steps : 5342 , Loss : 0.7830862998962402, Weights 1.0595873594284058, bias : 0.05760440230369568  \n",
      "Steps : 5343 , Loss : 0.7830864191055298, Weights 1.0595874786376953, bias : 0.05760428309440613  \n",
      "Steps : 5344 , Loss : 0.7830862998962402, Weights 1.0595875978469849, bias : 0.05760416388511658  \n",
      "Steps : 5345 , Loss : 0.7830862998962402, Weights 1.0595877170562744, bias : 0.057604044675827026  \n",
      "Steps : 5346 , Loss : 0.7830862998962402, Weights 1.059587836265564, bias : 0.057603925466537476  \n",
      "Steps : 5347 , Loss : 0.7830864191055298, Weights 1.0595879554748535, bias : 0.057603806257247925  \n",
      "Steps : 5348 , Loss : 0.7830862998962402, Weights 1.059588074684143, bias : 0.05760369077324867  \n",
      "Steps : 5349 , Loss : 0.7830862998962402, Weights 1.0595881938934326, bias : 0.05760357528924942  \n",
      "Steps : 5350 , Loss : 0.7830862402915955, Weights 1.0595883131027222, bias : 0.05760345980525017  \n",
      "Steps : 5351 , Loss : 0.7830862998962402, Weights 1.0595884323120117, bias : 0.057603344321250916  \n",
      "Steps : 5352 , Loss : 0.7830862402915955, Weights 1.0595885515213013, bias : 0.05760322883725166  \n",
      "Steps : 5353 , Loss : 0.7830862998962402, Weights 1.0595886707305908, bias : 0.05760311335325241  \n",
      "Steps : 5354 , Loss : 0.7830862998962402, Weights 1.0595887899398804, bias : 0.05760299786925316  \n",
      "Steps : 5355 , Loss : 0.7830862998962402, Weights 1.05958890914917, bias : 0.057602882385253906  \n",
      "Steps : 5356 , Loss : 0.7830862402915955, Weights 1.0595890283584595, bias : 0.057602766901254654  \n",
      "Steps : 5357 , Loss : 0.7830862402915955, Weights 1.059589147567749, bias : 0.0576026514172554  \n",
      "Steps : 5358 , Loss : 0.7830862402915955, Weights 1.0595892667770386, bias : 0.05760253593325615  \n",
      "Steps : 5359 , Loss : 0.7830862998962402, Weights 1.0595893859863281, bias : 0.0576024204492569  \n",
      "Steps : 5360 , Loss : 0.7830864191055298, Weights 1.0595895051956177, bias : 0.057602304965257645  \n",
      "Steps : 5361 , Loss : 0.7830862998962402, Weights 1.0595896244049072, bias : 0.05760218948125839  \n",
      "Steps : 5362 , Loss : 0.7830864191055298, Weights 1.0595897436141968, bias : 0.05760207399725914  \n",
      "Steps : 5363 , Loss : 0.7830862998962402, Weights 1.0595898628234863, bias : 0.05760195851325989  \n",
      "Steps : 5364 , Loss : 0.7830862402915955, Weights 1.0595899820327759, bias : 0.057601843029260635  \n",
      "Steps : 5365 , Loss : 0.7830862402915955, Weights 1.0595901012420654, bias : 0.05760172754526138  \n",
      "Steps : 5366 , Loss : 0.7830862402915955, Weights 1.059590220451355, bias : 0.05760161578655243  \n",
      "Steps : 5367 , Loss : 0.7830862402915955, Weights 1.0595903396606445, bias : 0.057601504027843475  \n",
      "Steps : 5368 , Loss : 0.7830862402915955, Weights 1.059590458869934, bias : 0.05760139226913452  \n",
      "Steps : 5369 , Loss : 0.7830862402915955, Weights 1.0595905780792236, bias : 0.05760128051042557  \n",
      "Steps : 5370 , Loss : 0.7830862402915955, Weights 1.0595906972885132, bias : 0.057601168751716614  \n",
      "Steps : 5371 , Loss : 0.7830862402915955, Weights 1.0595908164978027, bias : 0.05760105699300766  \n",
      "Steps : 5372 , Loss : 0.7830862402915955, Weights 1.0595909357070923, bias : 0.057600945234298706  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5373 , Loss : 0.7830864191055298, Weights 1.0595910549163818, bias : 0.05760083347558975  \n",
      "Steps : 5374 , Loss : 0.7830864191055298, Weights 1.0595911741256714, bias : 0.0576007217168808  \n",
      "Steps : 5375 , Loss : 0.7830864191055298, Weights 1.059591293334961, bias : 0.057600609958171844  \n",
      "Steps : 5376 , Loss : 0.7830862402915955, Weights 1.0595914125442505, bias : 0.05760049819946289  \n",
      "Steps : 5377 , Loss : 0.7830862402915955, Weights 1.05959153175354, bias : 0.05760038644075394  \n",
      "Steps : 5378 , Loss : 0.7830862402915955, Weights 1.0595916509628296, bias : 0.05760027468204498  \n",
      "Steps : 5379 , Loss : 0.7830862402915955, Weights 1.0595917701721191, bias : 0.05760016292333603  \n",
      "Steps : 5380 , Loss : 0.7830862402915955, Weights 1.0595918893814087, bias : 0.057600051164627075  \n",
      "Steps : 5381 , Loss : 0.7830862998962402, Weights 1.0595920085906982, bias : 0.05759993940591812  \n",
      "Steps : 5382 , Loss : 0.7830862998962402, Weights 1.0595921277999878, bias : 0.05759982764720917  \n",
      "Steps : 5383 , Loss : 0.7830862998962402, Weights 1.0595922470092773, bias : 0.057599715888500214  \n",
      "Steps : 5384 , Loss : 0.7830862402915955, Weights 1.059592366218567, bias : 0.05759960785508156  \n",
      "Steps : 5385 , Loss : 0.7830862402915955, Weights 1.0595924854278564, bias : 0.0575994998216629  \n",
      "Steps : 5386 , Loss : 0.7830862402915955, Weights 1.059592604637146, bias : 0.05759939178824425  \n",
      "Steps : 5387 , Loss : 0.7830862402915955, Weights 1.0595927238464355, bias : 0.05759928375482559  \n",
      "Steps : 5388 , Loss : 0.7830862402915955, Weights 1.059592843055725, bias : 0.05759917572140694  \n",
      "Steps : 5389 , Loss : 0.7830862998962402, Weights 1.0595929622650146, bias : 0.05759906768798828  \n",
      "Steps : 5390 , Loss : 0.7830862998962402, Weights 1.0595930814743042, bias : 0.057598959654569626  \n",
      "Steps : 5391 , Loss : 0.7830862402915955, Weights 1.0595932006835938, bias : 0.05759885162115097  \n",
      "Steps : 5392 , Loss : 0.7830862402915955, Weights 1.0595933198928833, bias : 0.057598743587732315  \n",
      "Steps : 5393 , Loss : 0.7830862402915955, Weights 1.0595934391021729, bias : 0.05759863555431366  \n",
      "Steps : 5394 , Loss : 0.7830864191055298, Weights 1.0595935583114624, bias : 0.057598527520895004  \n",
      "Steps : 5395 , Loss : 0.7830862998962402, Weights 1.059593677520752, bias : 0.05759841948747635  \n",
      "Steps : 5396 , Loss : 0.7830862998962402, Weights 1.0595937967300415, bias : 0.057598311454057693  \n",
      "Steps : 5397 , Loss : 0.7830862402915955, Weights 1.059593915939331, bias : 0.05759820342063904  \n",
      "Steps : 5398 , Loss : 0.7830862998962402, Weights 1.0595940351486206, bias : 0.05759809538722038  \n",
      "Steps : 5399 , Loss : 0.7830864191055298, Weights 1.0595941543579102, bias : 0.05759798735380173  \n",
      "Steps : 5400 , Loss : 0.7830862402915955, Weights 1.0595942735671997, bias : 0.05759787932038307  \n",
      "Steps : 5401 , Loss : 0.7830862998962402, Weights 1.0595943927764893, bias : 0.057597771286964417  \n",
      "Steps : 5402 , Loss : 0.7830862998962402, Weights 1.0595945119857788, bias : 0.05759766325354576  \n",
      "Steps : 5403 , Loss : 0.7830862402915955, Weights 1.0595946311950684, bias : 0.057597558945417404  \n",
      "Steps : 5404 , Loss : 0.7830862998962402, Weights 1.059594750404358, bias : 0.05759745463728905  \n",
      "Steps : 5405 , Loss : 0.7830862998962402, Weights 1.0595948696136475, bias : 0.05759735032916069  \n",
      "Steps : 5406 , Loss : 0.7830862998962402, Weights 1.059594988822937, bias : 0.05759724602103233  \n",
      "Steps : 5407 , Loss : 0.7830862998962402, Weights 1.0595951080322266, bias : 0.057597141712903976  \n",
      "Steps : 5408 , Loss : 0.7830862998962402, Weights 1.0595952272415161, bias : 0.05759703740477562  \n",
      "Steps : 5409 , Loss : 0.7830862998962402, Weights 1.0595953464508057, bias : 0.05759693309664726  \n",
      "Steps : 5410 , Loss : 0.7830862998962402, Weights 1.0595954656600952, bias : 0.057596828788518906  \n",
      "Steps : 5411 , Loss : 0.7830862998962402, Weights 1.0595955848693848, bias : 0.05759672448039055  \n",
      "Steps : 5412 , Loss : 0.7830862402915955, Weights 1.0595957040786743, bias : 0.05759662017226219  \n",
      "Steps : 5413 , Loss : 0.7830862402915955, Weights 1.0595958232879639, bias : 0.057596515864133835  \n",
      "Steps : 5414 , Loss : 0.7830862402915955, Weights 1.0595959424972534, bias : 0.05759641155600548  \n",
      "Steps : 5415 , Loss : 0.7830862402915955, Weights 1.059596061706543, bias : 0.05759630724787712  \n",
      "Steps : 5416 , Loss : 0.7830862402915955, Weights 1.0595961809158325, bias : 0.057596202939748764  \n",
      "Steps : 5417 , Loss : 0.7830862998962402, Weights 1.059596300125122, bias : 0.05759609863162041  \n",
      "Steps : 5418 , Loss : 0.7830862998962402, Weights 1.0595964193344116, bias : 0.05759599432349205  \n",
      "Steps : 5419 , Loss : 0.7830862402915955, Weights 1.0595965385437012, bias : 0.05759589001536369  \n",
      "Steps : 5420 , Loss : 0.7830862402915955, Weights 1.0595966577529907, bias : 0.057595785707235336  \n",
      "Steps : 5421 , Loss : 0.7830862998962402, Weights 1.0595967769622803, bias : 0.05759568139910698  \n",
      "Steps : 5422 , Loss : 0.7830862402915955, Weights 1.0595968961715698, bias : 0.05759558081626892  \n",
      "Steps : 5423 , Loss : 0.7830864191055298, Weights 1.0595970153808594, bias : 0.05759548023343086  \n",
      "Steps : 5424 , Loss : 0.7830862998962402, Weights 1.059597134590149, bias : 0.057595379650592804  \n",
      "Steps : 5425 , Loss : 0.7830862998962402, Weights 1.0595972537994385, bias : 0.057595279067754745  \n",
      "Steps : 5426 , Loss : 0.7830862998962402, Weights 1.059597373008728, bias : 0.05759517848491669  \n",
      "Steps : 5427 , Loss : 0.7830862998962402, Weights 1.0595974922180176, bias : 0.05759507790207863  \n",
      "Steps : 5428 , Loss : 0.7830864191055298, Weights 1.0595976114273071, bias : 0.05759497731924057  \n",
      "Steps : 5429 , Loss : 0.7830862998962402, Weights 1.0595977306365967, bias : 0.05759487673640251  \n",
      "Steps : 5430 , Loss : 0.7830862998962402, Weights 1.0595978498458862, bias : 0.05759477615356445  \n",
      "Steps : 5431 , Loss : 0.7830862998962402, Weights 1.0595979690551758, bias : 0.057594675570726395  \n",
      "Steps : 5432 , Loss : 0.7830864191055298, Weights 1.0595980882644653, bias : 0.057594574987888336  \n",
      "Steps : 5433 , Loss : 0.7830862998962402, Weights 1.0595982074737549, bias : 0.05759447440505028  \n",
      "Steps : 5434 , Loss : 0.7830862998962402, Weights 1.0595983266830444, bias : 0.05759437382221222  \n",
      "Steps : 5435 , Loss : 0.7830862402915955, Weights 1.059598445892334, bias : 0.05759427323937416  \n",
      "Steps : 5436 , Loss : 0.7830862998962402, Weights 1.0595985651016235, bias : 0.0575941726565361  \n",
      "Steps : 5437 , Loss : 0.7830862998962402, Weights 1.059598684310913, bias : 0.057594072073698044  \n",
      "Steps : 5438 , Loss : 0.7830862402915955, Weights 1.0595988035202026, bias : 0.057593971490859985  \n",
      "Steps : 5439 , Loss : 0.7830862998962402, Weights 1.0595989227294922, bias : 0.05759387090802193  \n",
      "Steps : 5440 , Loss : 0.7830862998962402, Weights 1.0595990419387817, bias : 0.05759377032518387  \n",
      "Steps : 5441 , Loss : 0.7830864191055298, Weights 1.0595991611480713, bias : 0.05759366974234581  \n",
      "Steps : 5442 , Loss : 0.7830862998962402, Weights 1.0595992803573608, bias : 0.05759356915950775  \n",
      "Steps : 5443 , Loss : 0.7830864191055298, Weights 1.0595993995666504, bias : 0.05759347230195999  \n",
      "Steps : 5444 , Loss : 0.7830862998962402, Weights 1.05959951877594, bias : 0.05759337544441223  \n",
      "Steps : 5445 , Loss : 0.7830862402915955, Weights 1.0595996379852295, bias : 0.05759327858686447  \n",
      "Steps : 5446 , Loss : 0.7830862998962402, Weights 1.059599757194519, bias : 0.05759318172931671  \n",
      "Steps : 5447 , Loss : 0.7830862998962402, Weights 1.0595998764038086, bias : 0.05759308487176895  \n",
      "Steps : 5448 , Loss : 0.7830862402915955, Weights 1.0595999956130981, bias : 0.05759298801422119  \n",
      "Steps : 5449 , Loss : 0.7830862998962402, Weights 1.0596001148223877, bias : 0.05759289115667343  \n",
      "Steps : 5450 , Loss : 0.7830862998962402, Weights 1.0596002340316772, bias : 0.05759279429912567  \n",
      "Steps : 5451 , Loss : 0.7830864191055298, Weights 1.0596003532409668, bias : 0.05759269744157791  \n",
      "Steps : 5452 , Loss : 0.7830864191055298, Weights 1.0596004724502563, bias : 0.05759260058403015  \n",
      "Steps : 5453 , Loss : 0.7830862998962402, Weights 1.059600591659546, bias : 0.05759250372648239  \n",
      "Steps : 5454 , Loss : 0.7830862998962402, Weights 1.0596007108688354, bias : 0.05759240686893463  \n",
      "Steps : 5455 , Loss : 0.7830862402915955, Weights 1.059600830078125, bias : 0.05759231001138687  \n",
      "Steps : 5456 , Loss : 0.7830862402915955, Weights 1.0596009492874146, bias : 0.05759221315383911  \n",
      "Steps : 5457 , Loss : 0.7830862402915955, Weights 1.059601068496704, bias : 0.05759211629629135  \n",
      "Steps : 5458 , Loss : 0.7830864191055298, Weights 1.0596011877059937, bias : 0.05759201943874359  \n",
      "Steps : 5459 , Loss : 0.7830862998962402, Weights 1.0596013069152832, bias : 0.05759192258119583  \n",
      "Steps : 5460 , Loss : 0.7830862402915955, Weights 1.0596014261245728, bias : 0.05759182572364807  \n",
      "Steps : 5461 , Loss : 0.7830862998962402, Weights 1.0596015453338623, bias : 0.05759172886610031  \n",
      "Steps : 5462 , Loss : 0.7830862998962402, Weights 1.0596016645431519, bias : 0.05759163200855255  \n",
      "Steps : 5463 , Loss : 0.7830864191055298, Weights 1.0596017837524414, bias : 0.05759153515100479  \n",
      "Steps : 5464 , Loss : 0.7830862402915955, Weights 1.059601902961731, bias : 0.05759144201874733  \n",
      "Steps : 5465 , Loss : 0.7830862402915955, Weights 1.0596020221710205, bias : 0.05759134888648987  \n",
      "Steps : 5466 , Loss : 0.7830862402915955, Weights 1.05960214138031, bias : 0.05759125575423241  \n",
      "Steps : 5467 , Loss : 0.7830862998962402, Weights 1.0596022605895996, bias : 0.057591162621974945  \n",
      "Steps : 5468 , Loss : 0.7830862998962402, Weights 1.0596023797988892, bias : 0.057591069489717484  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5469 , Loss : 0.7830862998962402, Weights 1.0596024990081787, bias : 0.05759097635746002  \n",
      "Steps : 5470 , Loss : 0.7830862402915955, Weights 1.0596026182174683, bias : 0.05759088322520256  \n",
      "Steps : 5471 , Loss : 0.7830862402915955, Weights 1.0596027374267578, bias : 0.0575907900929451  \n",
      "Steps : 5472 , Loss : 0.7830862402915955, Weights 1.0596028566360474, bias : 0.05759069696068764  \n",
      "Steps : 5473 , Loss : 0.7830862402915955, Weights 1.059602975845337, bias : 0.057590603828430176  \n",
      "Steps : 5474 , Loss : 0.7830862402915955, Weights 1.0596030950546265, bias : 0.057590510696172714  \n",
      "Steps : 5475 , Loss : 0.7830862402915955, Weights 1.059603214263916, bias : 0.05759041756391525  \n",
      "Steps : 5476 , Loss : 0.7830864191055298, Weights 1.0596033334732056, bias : 0.05759032443165779  \n",
      "Steps : 5477 , Loss : 0.7830864191055298, Weights 1.0596034526824951, bias : 0.05759023129940033  \n",
      "Steps : 5478 , Loss : 0.7830862998962402, Weights 1.0596035718917847, bias : 0.05759013816714287  \n",
      "Steps : 5479 , Loss : 0.7830864191055298, Weights 1.0596036911010742, bias : 0.057590045034885406  \n",
      "Steps : 5480 , Loss : 0.7830862998962402, Weights 1.0596038103103638, bias : 0.057589951902627945  \n",
      "Steps : 5481 , Loss : 0.7830862998962402, Weights 1.0596039295196533, bias : 0.05758985877037048  \n",
      "Steps : 5482 , Loss : 0.7830862402915955, Weights 1.0596040487289429, bias : 0.05758976563811302  \n",
      "Steps : 5483 , Loss : 0.7830862998962402, Weights 1.0596041679382324, bias : 0.05758967250585556  \n",
      "Steps : 5484 , Loss : 0.7830864191055298, Weights 1.059604287147522, bias : 0.0575895793735981  \n",
      "Steps : 5485 , Loss : 0.7830864191055298, Weights 1.0596044063568115, bias : 0.05758948624134064  \n",
      "Steps : 5486 , Loss : 0.7830862998962402, Weights 1.059604525566101, bias : 0.057589396834373474  \n",
      "Steps : 5487 , Loss : 0.7830862402915955, Weights 1.0596046447753906, bias : 0.05758930742740631  \n",
      "Steps : 5488 , Loss : 0.7830862402915955, Weights 1.0596047639846802, bias : 0.05758921802043915  \n",
      "Steps : 5489 , Loss : 0.7830862402915955, Weights 1.0596048831939697, bias : 0.057589128613471985  \n",
      "Steps : 5490 , Loss : 0.7830862998962402, Weights 1.0596050024032593, bias : 0.05758903920650482  \n",
      "Steps : 5491 , Loss : 0.7830862998962402, Weights 1.0596051216125488, bias : 0.05758894979953766  \n",
      "Steps : 5492 , Loss : 0.7830862998962402, Weights 1.0596052408218384, bias : 0.057588860392570496  \n",
      "Steps : 5493 , Loss : 0.7830862402915955, Weights 1.059605360031128, bias : 0.05758877098560333  \n",
      "Steps : 5494 , Loss : 0.7830862402915955, Weights 1.0596054792404175, bias : 0.05758868157863617  \n",
      "Steps : 5495 , Loss : 0.7830862998962402, Weights 1.059605598449707, bias : 0.057588592171669006  \n",
      "Steps : 5496 , Loss : 0.7830864191055298, Weights 1.0596057176589966, bias : 0.05758850276470184  \n",
      "Steps : 5497 , Loss : 0.7830862998962402, Weights 1.0596058368682861, bias : 0.05758841335773468  \n",
      "Steps : 5498 , Loss : 0.7830864191055298, Weights 1.0596059560775757, bias : 0.05758832395076752  \n",
      "Steps : 5499 , Loss : 0.7830862998962402, Weights 1.0596060752868652, bias : 0.057588234543800354  \n",
      "Steps : 5500 , Loss : 0.7830862998962402, Weights 1.0596061944961548, bias : 0.05758814513683319  \n",
      "Steps : 5501 , Loss : 0.7830862998962402, Weights 1.0596063137054443, bias : 0.05758805572986603  \n",
      "Steps : 5502 , Loss : 0.7830862998962402, Weights 1.0596064329147339, bias : 0.057587966322898865  \n",
      "Steps : 5503 , Loss : 0.7830864191055298, Weights 1.0596065521240234, bias : 0.0575878769159317  \n",
      "Steps : 5504 , Loss : 0.7830864191055298, Weights 1.059606671333313, bias : 0.05758778750896454  \n",
      "Steps : 5505 , Loss : 0.7830864191055298, Weights 1.0596067905426025, bias : 0.057587698101997375  \n",
      "Steps : 5506 , Loss : 0.7830862998962402, Weights 1.059606909751892, bias : 0.05758760869503021  \n",
      "Steps : 5507 , Loss : 0.7830862402915955, Weights 1.0596070289611816, bias : 0.05758751928806305  \n",
      "Steps : 5508 , Loss : 0.7830862402915955, Weights 1.0596071481704712, bias : 0.057587429881095886  \n",
      "Steps : 5509 , Loss : 0.7830862998962402, Weights 1.0596072673797607, bias : 0.05758734419941902  \n",
      "Steps : 5510 , Loss : 0.7830862998962402, Weights 1.0596073865890503, bias : 0.05758725851774216  \n",
      "Steps : 5511 , Loss : 0.7830862998962402, Weights 1.0596075057983398, bias : 0.05758717283606529  \n",
      "Steps : 5512 , Loss : 0.7830864191055298, Weights 1.0596076250076294, bias : 0.05758708715438843  \n",
      "Steps : 5513 , Loss : 0.7830862998962402, Weights 1.059607744216919, bias : 0.05758700147271156  \n",
      "Steps : 5514 , Loss : 0.7830862402915955, Weights 1.0596078634262085, bias : 0.0575869157910347  \n",
      "Steps : 5515 , Loss : 0.7830864191055298, Weights 1.059607982635498, bias : 0.057586830109357834  \n",
      "Steps : 5516 , Loss : 0.7830862998962402, Weights 1.0596081018447876, bias : 0.05758674442768097  \n",
      "Steps : 5517 , Loss : 0.7830864191055298, Weights 1.0596082210540771, bias : 0.057586658746004105  \n",
      "Steps : 5518 , Loss : 0.7830862998962402, Weights 1.0596083402633667, bias : 0.05758657306432724  \n",
      "Steps : 5519 , Loss : 0.7830862998962402, Weights 1.0596084594726562, bias : 0.057586487382650375  \n",
      "Steps : 5520 , Loss : 0.7830862402915955, Weights 1.0596085786819458, bias : 0.05758640170097351  \n",
      "Steps : 5521 , Loss : 0.7830862998962402, Weights 1.0596086978912354, bias : 0.057586316019296646  \n",
      "Steps : 5522 , Loss : 0.7830862998962402, Weights 1.059608817100525, bias : 0.05758623033761978  \n",
      "Steps : 5523 , Loss : 0.7830864191055298, Weights 1.0596089363098145, bias : 0.05758614465594292  \n",
      "Steps : 5524 , Loss : 0.7830862998962402, Weights 1.059609055519104, bias : 0.05758605897426605  \n",
      "Steps : 5525 , Loss : 0.7830862402915955, Weights 1.0596091747283936, bias : 0.05758597329258919  \n",
      "Steps : 5526 , Loss : 0.7830862998962402, Weights 1.059609293937683, bias : 0.05758588761091232  \n",
      "Steps : 5527 , Loss : 0.7830862998962402, Weights 1.0596094131469727, bias : 0.05758580192923546  \n",
      "Steps : 5528 , Loss : 0.7830864191055298, Weights 1.0596095323562622, bias : 0.057585716247558594  \n",
      "Steps : 5529 , Loss : 0.7830862998962402, Weights 1.0596096515655518, bias : 0.05758563056588173  \n",
      "Steps : 5530 , Loss : 0.7830862998962402, Weights 1.0596097707748413, bias : 0.057585544884204865  \n",
      "Steps : 5531 , Loss : 0.7830864191055298, Weights 1.0596098899841309, bias : 0.057585459202528  \n",
      "Steps : 5532 , Loss : 0.7830862998962402, Weights 1.0596100091934204, bias : 0.057585373520851135  \n",
      "Steps : 5533 , Loss : 0.7830862402915955, Weights 1.05961012840271, bias : 0.05758528783917427  \n",
      "Steps : 5534 , Loss : 0.7830862402915955, Weights 1.0596102476119995, bias : 0.057585205882787704  \n",
      "Steps : 5535 , Loss : 0.7830862402915955, Weights 1.059610366821289, bias : 0.05758512392640114  \n",
      "Steps : 5536 , Loss : 0.7830862998962402, Weights 1.0596104860305786, bias : 0.05758504197001457  \n",
      "Steps : 5537 , Loss : 0.7830864191055298, Weights 1.0596106052398682, bias : 0.057584960013628006  \n",
      "Steps : 5538 , Loss : 0.7830864191055298, Weights 1.0596107244491577, bias : 0.05758487805724144  \n",
      "Steps : 5539 , Loss : 0.7830862998962402, Weights 1.0596108436584473, bias : 0.057584796100854874  \n",
      "Steps : 5540 , Loss : 0.7830862998962402, Weights 1.0596109628677368, bias : 0.05758471414446831  \n",
      "Steps : 5541 , Loss : 0.7830862402915955, Weights 1.0596110820770264, bias : 0.05758463218808174  \n",
      "Steps : 5542 , Loss : 0.7830862998962402, Weights 1.059611201286316, bias : 0.057584550231695175  \n",
      "Steps : 5543 , Loss : 0.7830862402915955, Weights 1.0596113204956055, bias : 0.05758446827530861  \n",
      "Steps : 5544 , Loss : 0.7830862998962402, Weights 1.059611439704895, bias : 0.05758438631892204  \n",
      "Steps : 5545 , Loss : 0.7830862998962402, Weights 1.0596115589141846, bias : 0.05758430436253548  \n",
      "Steps : 5546 , Loss : 0.7830862998962402, Weights 1.0596116781234741, bias : 0.05758422240614891  \n",
      "Steps : 5547 , Loss : 0.7830864191055298, Weights 1.0596117973327637, bias : 0.057584140449762344  \n",
      "Steps : 5548 , Loss : 0.7830862402915955, Weights 1.0596119165420532, bias : 0.05758405849337578  \n",
      "Steps : 5549 , Loss : 0.7830862998962402, Weights 1.0596120357513428, bias : 0.05758397653698921  \n",
      "Steps : 5550 , Loss : 0.7830862402915955, Weights 1.0596121549606323, bias : 0.057583894580602646  \n",
      "Steps : 5551 , Loss : 0.7830862998962402, Weights 1.0596122741699219, bias : 0.05758381262421608  \n",
      "Steps : 5552 , Loss : 0.7830862402915955, Weights 1.0596123933792114, bias : 0.057583730667829514  \n",
      "Steps : 5553 , Loss : 0.7830862402915955, Weights 1.059612512588501, bias : 0.05758364871144295  \n",
      "Steps : 5554 , Loss : 0.7830862998962402, Weights 1.0596126317977905, bias : 0.05758356675505638  \n",
      "Steps : 5555 , Loss : 0.7830862998962402, Weights 1.05961275100708, bias : 0.057583484798669815  \n",
      "Steps : 5556 , Loss : 0.7830862998962402, Weights 1.0596128702163696, bias : 0.05758340284228325  \n",
      "Steps : 5557 , Loss : 0.7830862998962402, Weights 1.0596129894256592, bias : 0.05758332088589668  \n",
      "Steps : 5558 , Loss : 0.7830862998962402, Weights 1.0596131086349487, bias : 0.05758323892951012  \n",
      "Steps : 5559 , Loss : 0.7830862402915955, Weights 1.0596132278442383, bias : 0.05758316069841385  \n",
      "Steps : 5560 , Loss : 0.7830864191055298, Weights 1.0596133470535278, bias : 0.05758308246731758  \n",
      "Steps : 5561 , Loss : 0.7830862998962402, Weights 1.0596134662628174, bias : 0.057583004236221313  \n",
      "Steps : 5562 , Loss : 0.7830862402915955, Weights 1.059613585472107, bias : 0.057582926005125046  \n",
      "Steps : 5563 , Loss : 0.7830862998962402, Weights 1.0596137046813965, bias : 0.05758284777402878  \n",
      "Steps : 5564 , Loss : 0.7830864191055298, Weights 1.059613823890686, bias : 0.05758276954293251  \n",
      "Steps : 5565 , Loss : 0.7830862998962402, Weights 1.0596139430999756, bias : 0.05758269131183624  \n",
      "Steps : 5566 , Loss : 0.7830864191055298, Weights 1.0596140623092651, bias : 0.057582613080739975  \n",
      "Steps : 5567 , Loss : 0.7830862998962402, Weights 1.0596141815185547, bias : 0.05758253484964371  \n",
      "Steps : 5568 , Loss : 0.7830862998962402, Weights 1.0596143007278442, bias : 0.05758245661854744  \n",
      "Steps : 5569 , Loss : 0.7830862998962402, Weights 1.0596144199371338, bias : 0.05758237838745117  \n",
      "Steps : 5570 , Loss : 0.7830862998962402, Weights 1.0596145391464233, bias : 0.057582300156354904  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5571 , Loss : 0.7830862998962402, Weights 1.059614658355713, bias : 0.057582221925258636  \n",
      "Steps : 5572 , Loss : 0.7830864191055298, Weights 1.0596147775650024, bias : 0.05758214369416237  \n",
      "Steps : 5573 , Loss : 0.7830862402915955, Weights 1.059614896774292, bias : 0.0575820654630661  \n",
      "Steps : 5574 , Loss : 0.7830862998962402, Weights 1.0596150159835815, bias : 0.05758198723196983  \n",
      "Steps : 5575 , Loss : 0.7830862998962402, Weights 1.059615135192871, bias : 0.057581909000873566  \n",
      "Steps : 5576 , Loss : 0.7830862998962402, Weights 1.0596152544021606, bias : 0.0575818307697773  \n",
      "Steps : 5577 , Loss : 0.7830864191055298, Weights 1.0596153736114502, bias : 0.05758175253868103  \n",
      "Steps : 5578 , Loss : 0.7830862998962402, Weights 1.0596154928207397, bias : 0.05758167430758476  \n",
      "Steps : 5579 , Loss : 0.7830862402915955, Weights 1.0596156120300293, bias : 0.057581596076488495  \n",
      "Steps : 5580 , Loss : 0.7830862998962402, Weights 1.0596157312393188, bias : 0.05758151784539223  \n",
      "Steps : 5581 , Loss : 0.7830862402915955, Weights 1.0596158504486084, bias : 0.05758143961429596  \n",
      "Steps : 5582 , Loss : 0.7830862402915955, Weights 1.059615969657898, bias : 0.05758136138319969  \n",
      "Steps : 5583 , Loss : 0.7830862402915955, Weights 1.0596160888671875, bias : 0.057581283152103424  \n",
      "Steps : 5584 , Loss : 0.7830862402915955, Weights 1.059616208076477, bias : 0.057581204921007156  \n",
      "Steps : 5585 , Loss : 0.7830862402915955, Weights 1.0596163272857666, bias : 0.05758112668991089  \n",
      "Steps : 5586 , Loss : 0.7830862402915955, Weights 1.0596164464950562, bias : 0.05758104845881462  \n",
      "Steps : 5587 , Loss : 0.7830862402915955, Weights 1.0596165657043457, bias : 0.05758097395300865  \n",
      "Steps : 5588 , Loss : 0.7830862998962402, Weights 1.0596166849136353, bias : 0.05758089944720268  \n",
      "Steps : 5589 , Loss : 0.7830864191055298, Weights 1.0596168041229248, bias : 0.05758082494139671  \n",
      "Steps : 5590 , Loss : 0.7830862402915955, Weights 1.0596169233322144, bias : 0.057580750435590744  \n",
      "Steps : 5591 , Loss : 0.7830862402915955, Weights 1.059617042541504, bias : 0.057580675929784775  \n",
      "Steps : 5592 , Loss : 0.7830862998962402, Weights 1.0596171617507935, bias : 0.057580601423978806  \n",
      "Steps : 5593 , Loss : 0.7830862998962402, Weights 1.059617280960083, bias : 0.057580526918172836  \n",
      "Steps : 5594 , Loss : 0.7830862998962402, Weights 1.0596174001693726, bias : 0.05758045241236687  \n",
      "Steps : 5595 , Loss : 0.7830862402915955, Weights 1.059617519378662, bias : 0.0575803779065609  \n",
      "Steps : 5596 , Loss : 0.7830862402915955, Weights 1.0596176385879517, bias : 0.05758030340075493  \n",
      "Steps : 5597 , Loss : 0.7830862998962402, Weights 1.0596177577972412, bias : 0.05758022889494896  \n",
      "Steps : 5598 , Loss : 0.7830862998962402, Weights 1.0596178770065308, bias : 0.05758015438914299  \n",
      "Steps : 5599 , Loss : 0.7830864191055298, Weights 1.0596179962158203, bias : 0.05758007988333702  \n",
      "Steps : 5600 , Loss : 0.7830864191055298, Weights 1.0596181154251099, bias : 0.05758000537753105  \n",
      "Steps : 5601 , Loss : 0.7830862402915955, Weights 1.0596182346343994, bias : 0.05757993087172508  \n",
      "Steps : 5602 , Loss : 0.7830864191055298, Weights 1.059618353843689, bias : 0.05757985636591911  \n",
      "Steps : 5603 , Loss : 0.7830862998962402, Weights 1.0596184730529785, bias : 0.057579781860113144  \n",
      "Steps : 5604 , Loss : 0.7830862998962402, Weights 1.059618592262268, bias : 0.057579707354307175  \n",
      "Steps : 5605 , Loss : 0.7830862998962402, Weights 1.0596187114715576, bias : 0.057579632848501205  \n",
      "Steps : 5606 , Loss : 0.7830862402915955, Weights 1.0596188306808472, bias : 0.057579558342695236  \n",
      "Steps : 5607 , Loss : 0.7830862402915955, Weights 1.0596189498901367, bias : 0.05757948383688927  \n",
      "Steps : 5608 , Loss : 0.7830862998962402, Weights 1.0596190690994263, bias : 0.0575794093310833  \n",
      "Steps : 5609 , Loss : 0.7830862402915955, Weights 1.0596191883087158, bias : 0.05757933482527733  \n",
      "Steps : 5610 , Loss : 0.7830862998962402, Weights 1.0596193075180054, bias : 0.05757926031947136  \n",
      "Steps : 5611 , Loss : 0.7830862998962402, Weights 1.059619426727295, bias : 0.05757918581366539  \n",
      "Steps : 5612 , Loss : 0.7830864191055298, Weights 1.0596195459365845, bias : 0.05757911130785942  \n",
      "Steps : 5613 , Loss : 0.7830862402915955, Weights 1.059619665145874, bias : 0.05757903680205345  \n",
      "Steps : 5614 , Loss : 0.7830862402915955, Weights 1.0596197843551636, bias : 0.05757896229624748  \n",
      "Steps : 5615 , Loss : 0.7830862402915955, Weights 1.0596199035644531, bias : 0.05757889151573181  \n",
      "Steps : 5616 , Loss : 0.7830862998962402, Weights 1.0596200227737427, bias : 0.05757882073521614  \n",
      "Steps : 5617 , Loss : 0.7830864191055298, Weights 1.0596201419830322, bias : 0.05757874995470047  \n",
      "Steps : 5618 , Loss : 0.7830864191055298, Weights 1.0596202611923218, bias : 0.0575786791741848  \n",
      "Steps : 5619 , Loss : 0.7830862998962402, Weights 1.0596203804016113, bias : 0.05757860839366913  \n",
      "Steps : 5620 , Loss : 0.7830862998962402, Weights 1.0596204996109009, bias : 0.05757853761315346  \n",
      "Steps : 5621 , Loss : 0.7830862998962402, Weights 1.0596206188201904, bias : 0.05757846683263779  \n",
      "Steps : 5622 , Loss : 0.7830862402915955, Weights 1.05962073802948, bias : 0.057578396052122116  \n",
      "Steps : 5623 , Loss : 0.7830862402915955, Weights 1.0596208572387695, bias : 0.057578325271606445  \n",
      "Steps : 5624 , Loss : 0.7830864191055298, Weights 1.059620976448059, bias : 0.057578254491090775  \n",
      "Steps : 5625 , Loss : 0.7830864191055298, Weights 1.0596210956573486, bias : 0.057578183710575104  \n",
      "Steps : 5626 , Loss : 0.7830862998962402, Weights 1.0596212148666382, bias : 0.05757811293005943  \n",
      "Steps : 5627 , Loss : 0.7830862402915955, Weights 1.0596213340759277, bias : 0.05757804214954376  \n",
      "Steps : 5628 , Loss : 0.7830862402915955, Weights 1.0596214532852173, bias : 0.05757797136902809  \n",
      "Steps : 5629 , Loss : 0.7830862998962402, Weights 1.0596215724945068, bias : 0.05757790058851242  \n",
      "Steps : 5630 , Loss : 0.7830862998962402, Weights 1.0596216917037964, bias : 0.05757782980799675  \n",
      "Steps : 5631 , Loss : 0.7830862402915955, Weights 1.059621810913086, bias : 0.05757775902748108  \n",
      "Steps : 5632 , Loss : 0.7830862998962402, Weights 1.0596219301223755, bias : 0.05757768824696541  \n",
      "Steps : 5633 , Loss : 0.7830862998962402, Weights 1.059622049331665, bias : 0.05757761746644974  \n",
      "Steps : 5634 , Loss : 0.7830862402915955, Weights 1.0596221685409546, bias : 0.05757754668593407  \n",
      "Steps : 5635 , Loss : 0.7830862998962402, Weights 1.0596222877502441, bias : 0.057577475905418396  \n",
      "Steps : 5636 , Loss : 0.7830864191055298, Weights 1.0596224069595337, bias : 0.057577405124902725  \n",
      "Steps : 5637 , Loss : 0.7830862998962402, Weights 1.0596225261688232, bias : 0.057577334344387054  \n",
      "Steps : 5638 , Loss : 0.7830862998962402, Weights 1.0596226453781128, bias : 0.057577263563871384  \n",
      "Steps : 5639 , Loss : 0.7830862998962402, Weights 1.0596227645874023, bias : 0.05757719278335571  \n",
      "Steps : 5640 , Loss : 0.7830862998962402, Weights 1.059622883796692, bias : 0.05757712200284004  \n",
      "Steps : 5641 , Loss : 0.7830862998962402, Weights 1.0596230030059814, bias : 0.05757705122232437  \n",
      "Steps : 5642 , Loss : 0.7830862402915955, Weights 1.059623122215271, bias : 0.0575769804418087  \n",
      "Steps : 5643 , Loss : 0.7830862402915955, Weights 1.0596232414245605, bias : 0.05757690966129303  \n",
      "Steps : 5644 , Loss : 0.7830862402915955, Weights 1.05962336063385, bias : 0.05757683888077736  \n",
      "Steps : 5645 , Loss : 0.7830864191055298, Weights 1.0596234798431396, bias : 0.05757676810026169  \n",
      "Steps : 5646 , Loss : 0.7830864191055298, Weights 1.0596235990524292, bias : 0.057576701045036316  \n",
      "Steps : 5647 , Loss : 0.7830864191055298, Weights 1.0596237182617188, bias : 0.057576633989810944  \n",
      "Steps : 5648 , Loss : 0.7830864191055298, Weights 1.0596238374710083, bias : 0.05757656693458557  \n",
      "Steps : 5649 , Loss : 0.7830864191055298, Weights 1.0596239566802979, bias : 0.0575764998793602  \n",
      "Steps : 5650 , Loss : 0.7830862998962402, Weights 1.0596240758895874, bias : 0.05757643282413483  \n",
      "Steps : 5651 , Loss : 0.7830862402915955, Weights 1.059624195098877, bias : 0.057576365768909454  \n",
      "Steps : 5652 , Loss : 0.7830862402915955, Weights 1.0596243143081665, bias : 0.05757629871368408  \n",
      "Steps : 5653 , Loss : 0.7830862998962402, Weights 1.059624433517456, bias : 0.05757623165845871  \n",
      "Steps : 5654 , Loss : 0.7830864191055298, Weights 1.0596245527267456, bias : 0.05757616460323334  \n",
      "Steps : 5655 , Loss : 0.7830862402915955, Weights 1.0596246719360352, bias : 0.057576097548007965  \n",
      "Steps : 5656 , Loss : 0.7830862998962402, Weights 1.0596247911453247, bias : 0.05757603049278259  \n",
      "Steps : 5657 , Loss : 0.7830862402915955, Weights 1.0596249103546143, bias : 0.05757596343755722  \n",
      "Steps : 5658 , Loss : 0.7830862998962402, Weights 1.0596250295639038, bias : 0.05757589638233185  \n",
      "Steps : 5659 , Loss : 0.7830862998962402, Weights 1.0596251487731934, bias : 0.057575829327106476  \n",
      "Steps : 5660 , Loss : 0.7830862998962402, Weights 1.059625267982483, bias : 0.057575762271881104  \n",
      "Steps : 5661 , Loss : 0.7830862402915955, Weights 1.0596253871917725, bias : 0.05757569521665573  \n",
      "Steps : 5662 , Loss : 0.7830864191055298, Weights 1.059625506401062, bias : 0.05757562816143036  \n",
      "Steps : 5663 , Loss : 0.7830864191055298, Weights 1.0596256256103516, bias : 0.05757556110620499  \n",
      "Steps : 5664 , Loss : 0.7830864191055298, Weights 1.0596257448196411, bias : 0.057575494050979614  \n",
      "Steps : 5665 , Loss : 0.7830864191055298, Weights 1.0596258640289307, bias : 0.05757542699575424  \n",
      "Steps : 5666 , Loss : 0.7830864191055298, Weights 1.0596259832382202, bias : 0.05757535994052887  \n",
      "Steps : 5667 , Loss : 0.7830864191055298, Weights 1.0596261024475098, bias : 0.0575752928853035  \n",
      "Steps : 5668 , Loss : 0.7830862998962402, Weights 1.0596262216567993, bias : 0.057575225830078125  \n",
      "Steps : 5669 , Loss : 0.7830862998962402, Weights 1.0596263408660889, bias : 0.05757515877485275  \n",
      "Steps : 5670 , Loss : 0.7830862998962402, Weights 1.0596264600753784, bias : 0.05757509171962738  \n",
      "Steps : 5671 , Loss : 0.7830862998962402, Weights 1.059626579284668, bias : 0.05757502466440201  \n",
      "Steps : 5672 , Loss : 0.7830862998962402, Weights 1.0596266984939575, bias : 0.057574957609176636  \n",
      "Steps : 5673 , Loss : 0.7830862402915955, Weights 1.059626817703247, bias : 0.05757489055395126  \n",
      "Steps : 5674 , Loss : 0.7830862402915955, Weights 1.0596269369125366, bias : 0.05757482349872589  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5675 , Loss : 0.7830864191055298, Weights 1.0596270561218262, bias : 0.05757475644350052  \n",
      "Steps : 5676 , Loss : 0.7830862998962402, Weights 1.0596271753311157, bias : 0.057574689388275146  \n",
      "Steps : 5677 , Loss : 0.7830862998962402, Weights 1.0596272945404053, bias : 0.057574622333049774  \n",
      "Steps : 5678 , Loss : 0.7830862402915955, Weights 1.0596274137496948, bias : 0.0575745590031147  \n",
      "Steps : 5679 , Loss : 0.7830862998962402, Weights 1.0596275329589844, bias : 0.057574495673179626  \n",
      "Steps : 5680 , Loss : 0.7830862402915955, Weights 1.059627652168274, bias : 0.05757443234324455  \n",
      "Steps : 5681 , Loss : 0.7830864191055298, Weights 1.0596277713775635, bias : 0.05757436901330948  \n",
      "Steps : 5682 , Loss : 0.7830864191055298, Weights 1.059627890586853, bias : 0.057574305683374405  \n",
      "Steps : 5683 , Loss : 0.7830862998962402, Weights 1.0596280097961426, bias : 0.05757424235343933  \n",
      "Steps : 5684 , Loss : 0.7830862402915955, Weights 1.0596281290054321, bias : 0.05757417902350426  \n",
      "Steps : 5685 , Loss : 0.7830862998962402, Weights 1.0596282482147217, bias : 0.05757411569356918  \n",
      "Steps : 5686 , Loss : 0.7830862998962402, Weights 1.0596283674240112, bias : 0.05757405236363411  \n",
      "Steps : 5687 , Loss : 0.7830862402915955, Weights 1.0596284866333008, bias : 0.057573989033699036  \n",
      "Steps : 5688 , Loss : 0.7830862998962402, Weights 1.0596286058425903, bias : 0.05757392570376396  \n",
      "Steps : 5689 , Loss : 0.7830862998962402, Weights 1.0596287250518799, bias : 0.05757386237382889  \n",
      "Steps : 5690 , Loss : 0.7830862402915955, Weights 1.0596288442611694, bias : 0.057573799043893814  \n",
      "Steps : 5691 , Loss : 0.7830862402915955, Weights 1.059628963470459, bias : 0.05757373571395874  \n",
      "Steps : 5692 , Loss : 0.7830862998962402, Weights 1.0596290826797485, bias : 0.057573672384023666  \n",
      "Steps : 5693 , Loss : 0.7830864191055298, Weights 1.059629201889038, bias : 0.05757360905408859  \n",
      "Steps : 5694 , Loss : 0.7830862402915955, Weights 1.0596293210983276, bias : 0.05757354572415352  \n",
      "Steps : 5695 , Loss : 0.7830864191055298, Weights 1.0596294403076172, bias : 0.057573482394218445  \n",
      "Steps : 5696 , Loss : 0.7830862402915955, Weights 1.0596295595169067, bias : 0.05757341906428337  \n",
      "Steps : 5697 , Loss : 0.7830862998962402, Weights 1.0596296787261963, bias : 0.0575733557343483  \n",
      "Steps : 5698 , Loss : 0.7830862998962402, Weights 1.0596297979354858, bias : 0.05757329240441322  \n",
      "Steps : 5699 , Loss : 0.7830862402915955, Weights 1.0596299171447754, bias : 0.05757322907447815  \n",
      "Steps : 5700 , Loss : 0.7830862402915955, Weights 1.059630036354065, bias : 0.057573165744543076  \n",
      "Steps : 5701 , Loss : 0.7830864191055298, Weights 1.0596301555633545, bias : 0.057573102414608  \n",
      "Steps : 5702 , Loss : 0.7830862998962402, Weights 1.059630274772644, bias : 0.05757303908467293  \n",
      "Steps : 5703 , Loss : 0.7830864191055298, Weights 1.0596303939819336, bias : 0.057572975754737854  \n",
      "Steps : 5704 , Loss : 0.7830862998962402, Weights 1.0596305131912231, bias : 0.05757291242480278  \n",
      "Steps : 5705 , Loss : 0.7830862998962402, Weights 1.0596306324005127, bias : 0.057572849094867706  \n",
      "Steps : 5706 , Loss : 0.7830862402915955, Weights 1.0596307516098022, bias : 0.05757278576493263  \n",
      "Steps : 5707 , Loss : 0.7830862402915955, Weights 1.0596308708190918, bias : 0.05757272243499756  \n",
      "Steps : 5708 , Loss : 0.7830862402915955, Weights 1.0596309900283813, bias : 0.057572659105062485  \n",
      "Steps : 5709 , Loss : 0.7830862402915955, Weights 1.059631109237671, bias : 0.05757259577512741  \n",
      "Steps : 5710 , Loss : 0.7830862402915955, Weights 1.0596312284469604, bias : 0.05757253244519234  \n",
      "Steps : 5711 , Loss : 0.7830862998962402, Weights 1.05963134765625, bias : 0.05757246911525726  \n",
      "Steps : 5712 , Loss : 0.7830862998962402, Weights 1.0596314668655396, bias : 0.05757240951061249  \n",
      "Steps : 5713 , Loss : 0.7830862998962402, Weights 1.059631586074829, bias : 0.05757234990596771  \n",
      "Steps : 5714 , Loss : 0.7830864191055298, Weights 1.0596317052841187, bias : 0.05757229030132294  \n",
      "Steps : 5715 , Loss : 0.7830864191055298, Weights 1.0596318244934082, bias : 0.05757223069667816  \n",
      "Steps : 5716 , Loss : 0.7830862998962402, Weights 1.0596319437026978, bias : 0.057572171092033386  \n",
      "Steps : 5717 , Loss : 0.7830864191055298, Weights 1.0596320629119873, bias : 0.05757211148738861  \n",
      "Steps : 5718 , Loss : 0.7830862998962402, Weights 1.0596321821212769, bias : 0.057572051882743835  \n",
      "Steps : 5719 , Loss : 0.7830862998962402, Weights 1.0596323013305664, bias : 0.05757199227809906  \n",
      "Steps : 5720 , Loss : 0.7830862402915955, Weights 1.059632420539856, bias : 0.057571932673454285  \n",
      "Steps : 5721 , Loss : 0.7830862998962402, Weights 1.0596325397491455, bias : 0.05757187306880951  \n",
      "Steps : 5722 , Loss : 0.7830862998962402, Weights 1.059632658958435, bias : 0.057571813464164734  \n",
      "Steps : 5723 , Loss : 0.7830862998962402, Weights 1.0596327781677246, bias : 0.05757175385951996  \n",
      "Steps : 5724 , Loss : 0.7830864191055298, Weights 1.0596328973770142, bias : 0.05757169425487518  \n",
      "Steps : 5725 , Loss : 0.7830862998962402, Weights 1.0596330165863037, bias : 0.05757163465023041  \n",
      "Steps : 5726 , Loss : 0.7830862998962402, Weights 1.0596331357955933, bias : 0.05757157504558563  \n",
      "Steps : 5727 , Loss : 0.7830862402915955, Weights 1.0596332550048828, bias : 0.05757151544094086  \n",
      "Steps : 5728 , Loss : 0.7830862998962402, Weights 1.0596333742141724, bias : 0.05757145583629608  \n",
      "Steps : 5729 , Loss : 0.7830862402915955, Weights 1.059633493423462, bias : 0.057571396231651306  \n",
      "Steps : 5730 , Loss : 0.7830864191055298, Weights 1.0596336126327515, bias : 0.05757133662700653  \n",
      "Steps : 5731 , Loss : 0.7830864191055298, Weights 1.059633731842041, bias : 0.057571277022361755  \n",
      "Steps : 5732 , Loss : 0.7830862998962402, Weights 1.0596338510513306, bias : 0.05757121741771698  \n",
      "Steps : 5733 , Loss : 0.7830862402915955, Weights 1.0596339702606201, bias : 0.057571157813072205  \n",
      "Steps : 5734 , Loss : 0.7830862998962402, Weights 1.0596340894699097, bias : 0.05757109820842743  \n",
      "Steps : 5735 , Loss : 0.7830862402915955, Weights 1.0596342086791992, bias : 0.057571038603782654  \n",
      "Steps : 5736 , Loss : 0.7830862402915955, Weights 1.0596343278884888, bias : 0.05757097899913788  \n",
      "Steps : 5737 , Loss : 0.7830862402915955, Weights 1.0596344470977783, bias : 0.0575709193944931  \n",
      "Steps : 5738 , Loss : 0.7830864191055298, Weights 1.0596345663070679, bias : 0.05757085978984833  \n",
      "Steps : 5739 , Loss : 0.7830862998962402, Weights 1.0596346855163574, bias : 0.05757080018520355  \n",
      "Steps : 5740 , Loss : 0.7830862998962402, Weights 1.059634804725647, bias : 0.05757074058055878  \n",
      "Steps : 5741 , Loss : 0.7830862998962402, Weights 1.0596349239349365, bias : 0.057570680975914  \n",
      "Steps : 5742 , Loss : 0.7830862998962402, Weights 1.059635043144226, bias : 0.057570621371269226  \n",
      "Steps : 5743 , Loss : 0.7830862998962402, Weights 1.0596351623535156, bias : 0.05757056176662445  \n",
      "Steps : 5744 , Loss : 0.7830862402915955, Weights 1.0596352815628052, bias : 0.057570502161979675  \n",
      "Steps : 5745 , Loss : 0.7830862402915955, Weights 1.0596354007720947, bias : 0.0575704425573349  \n",
      "Steps : 5746 , Loss : 0.7830862998962402, Weights 1.0596355199813843, bias : 0.057570382952690125  \n",
      "Steps : 5747 , Loss : 0.7830862998962402, Weights 1.0596356391906738, bias : 0.05757032334804535  \n",
      "Steps : 5748 , Loss : 0.7830864191055298, Weights 1.0596357583999634, bias : 0.057570263743400574  \n",
      "Steps : 5749 , Loss : 0.7830864191055298, Weights 1.059635877609253, bias : 0.0575702078640461  \n",
      "Steps : 5750 , Loss : 0.7830864191055298, Weights 1.0596359968185425, bias : 0.05757015198469162  \n",
      "Steps : 5751 , Loss : 0.7830862998962402, Weights 1.059636116027832, bias : 0.05757009610533714  \n",
      "Steps : 5752 , Loss : 0.7830864191055298, Weights 1.0596362352371216, bias : 0.057570040225982666  \n",
      "Steps : 5753 , Loss : 0.7830864191055298, Weights 1.0596363544464111, bias : 0.05756998434662819  \n",
      "Steps : 5754 , Loss : 0.7830862998962402, Weights 1.0596364736557007, bias : 0.05756992846727371  \n",
      "Steps : 5755 , Loss : 0.7830862998962402, Weights 1.0596365928649902, bias : 0.057569872587919235  \n",
      "Steps : 5756 , Loss : 0.7830862402915955, Weights 1.0596367120742798, bias : 0.05756981670856476  \n",
      "Steps : 5757 , Loss : 0.7830862402915955, Weights 1.0596368312835693, bias : 0.05756976082921028  \n",
      "Steps : 5758 , Loss : 0.7830864191055298, Weights 1.0596369504928589, bias : 0.057569704949855804  \n",
      "Steps : 5759 , Loss : 0.7830862402915955, Weights 1.0596370697021484, bias : 0.05756964907050133  \n",
      "Steps : 5760 , Loss : 0.7830862998962402, Weights 1.059637188911438, bias : 0.05756959319114685  \n",
      "Steps : 5761 , Loss : 0.7830862998962402, Weights 1.0596373081207275, bias : 0.057569537311792374  \n",
      "Steps : 5762 , Loss : 0.7830862998962402, Weights 1.059637427330017, bias : 0.0575694814324379  \n",
      "Steps : 5763 , Loss : 0.7830862998962402, Weights 1.0596375465393066, bias : 0.05756942555308342  \n",
      "Steps : 5764 , Loss : 0.7830862402915955, Weights 1.0596376657485962, bias : 0.05756936967372894  \n",
      "Steps : 5765 , Loss : 0.7830862998962402, Weights 1.0596377849578857, bias : 0.057569313794374466  \n",
      "Steps : 5766 , Loss : 0.7830862998962402, Weights 1.0596379041671753, bias : 0.05756925791501999  \n",
      "Steps : 5767 , Loss : 0.7830864191055298, Weights 1.0596380233764648, bias : 0.05756920203566551  \n",
      "Steps : 5768 , Loss : 0.7830864191055298, Weights 1.0596381425857544, bias : 0.057569146156311035  \n",
      "Steps : 5769 , Loss : 0.7830862998962402, Weights 1.059638261795044, bias : 0.05756909027695656  \n",
      "Steps : 5770 , Loss : 0.7830862998962402, Weights 1.0596383810043335, bias : 0.05756903439760208  \n",
      "Steps : 5771 , Loss : 0.7830862998962402, Weights 1.059638500213623, bias : 0.057568978518247604  \n",
      "Steps : 5772 , Loss : 0.7830862998962402, Weights 1.0596386194229126, bias : 0.05756892263889313  \n",
      "Steps : 5773 , Loss : 0.7830862998962402, Weights 1.0596387386322021, bias : 0.05756886675953865  \n",
      "Steps : 5774 , Loss : 0.7830864191055298, Weights 1.0596388578414917, bias : 0.057568810880184174  \n",
      "Steps : 5775 , Loss : 0.7830864191055298, Weights 1.0596389770507812, bias : 0.0575687550008297  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5776 , Loss : 0.7830862998962402, Weights 1.0596390962600708, bias : 0.05756869912147522  \n",
      "Steps : 5777 , Loss : 0.7830862998962402, Weights 1.0596392154693604, bias : 0.05756864324212074  \n",
      "Steps : 5778 , Loss : 0.7830864191055298, Weights 1.05963933467865, bias : 0.057568587362766266  \n",
      "Steps : 5779 , Loss : 0.7830862998962402, Weights 1.0596394538879395, bias : 0.05756853148341179  \n",
      "Steps : 5780 , Loss : 0.7830862402915955, Weights 1.059639573097229, bias : 0.05756847560405731  \n",
      "Steps : 5781 , Loss : 0.7830862402915955, Weights 1.0596396923065186, bias : 0.057568419724702835  \n",
      "Steps : 5782 , Loss : 0.7830862402915955, Weights 1.059639811515808, bias : 0.05756836384534836  \n",
      "Steps : 5783 , Loss : 0.7830862402915955, Weights 1.0596399307250977, bias : 0.05756830796599388  \n",
      "Steps : 5784 , Loss : 0.7830862998962402, Weights 1.0596400499343872, bias : 0.057568252086639404  \n",
      "Steps : 5785 , Loss : 0.7830862998962402, Weights 1.0596401691436768, bias : 0.05756819620728493  \n",
      "Steps : 5786 , Loss : 0.7830862998962402, Weights 1.0596402883529663, bias : 0.05756814032793045  \n",
      "Steps : 5787 , Loss : 0.7830864191055298, Weights 1.0596404075622559, bias : 0.057568084448575974  \n",
      "Steps : 5788 , Loss : 0.7830862402915955, Weights 1.0596405267715454, bias : 0.0575680285692215  \n",
      "Steps : 5789 , Loss : 0.7830864191055298, Weights 1.059640645980835, bias : 0.05756797268986702  \n",
      "Steps : 5790 , Loss : 0.7830862402915955, Weights 1.0596407651901245, bias : 0.05756792053580284  \n",
      "Steps : 5791 , Loss : 0.7830862998962402, Weights 1.059640884399414, bias : 0.05756786838173866  \n",
      "Steps : 5792 , Loss : 0.7830862402915955, Weights 1.0596410036087036, bias : 0.057567816227674484  \n",
      "Steps : 5793 , Loss : 0.7830864191055298, Weights 1.0596411228179932, bias : 0.057567764073610306  \n",
      "Steps : 5794 , Loss : 0.7830862998962402, Weights 1.0596412420272827, bias : 0.05756771191954613  \n",
      "Steps : 5795 , Loss : 0.7830862998962402, Weights 1.0596413612365723, bias : 0.05756765976548195  \n",
      "Steps : 5796 , Loss : 0.7830862998962402, Weights 1.0596414804458618, bias : 0.05756760761141777  \n",
      "Steps : 5797 , Loss : 0.7830862402915955, Weights 1.0596415996551514, bias : 0.05756755545735359  \n",
      "Steps : 5798 , Loss : 0.7830862402915955, Weights 1.059641718864441, bias : 0.05756750330328941  \n",
      "Steps : 5799 , Loss : 0.7830862402915955, Weights 1.0596418380737305, bias : 0.057567451149225235  \n",
      "Steps : 5800 , Loss : 0.7830862998962402, Weights 1.05964195728302, bias : 0.057567398995161057  \n",
      "Steps : 5801 , Loss : 0.7830862402915955, Weights 1.0596420764923096, bias : 0.05756734684109688  \n",
      "Steps : 5802 , Loss : 0.7830862402915955, Weights 1.0596421957015991, bias : 0.0575672946870327  \n",
      "Steps : 5803 , Loss : 0.7830862998962402, Weights 1.0596423149108887, bias : 0.05756724253296852  \n",
      "Steps : 5804 , Loss : 0.7830862998962402, Weights 1.0596424341201782, bias : 0.05756719037890434  \n",
      "Steps : 5805 , Loss : 0.7830862402915955, Weights 1.0596424341201782, bias : 0.057567138224840164  \n",
      "Steps : 5806 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.057567086070775986  \n",
      "Steps : 5807 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756703391671181  \n",
      "Steps : 5808 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756698176264763  \n",
      "Steps : 5809 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756692960858345  \n",
      "Steps : 5810 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756687745451927  \n",
      "Steps : 5811 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756682530045509  \n",
      "Steps : 5812 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.057566773146390915  \n",
      "Steps : 5813 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.057566720992326736  \n",
      "Steps : 5814 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756666883826256  \n",
      "Steps : 5815 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756661668419838  \n",
      "Steps : 5816 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.0575665645301342  \n",
      "Steps : 5817 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756651237607002  \n",
      "Steps : 5818 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.057566460222005844  \n",
      "Steps : 5819 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.057566408067941666  \n",
      "Steps : 5820 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756635591387749  \n",
      "Steps : 5821 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756630375981331  \n",
      "Steps : 5822 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756625160574913  \n",
      "Steps : 5823 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756619945168495  \n",
      "Steps : 5824 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756614729762077  \n",
      "Steps : 5825 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.057566095143556595  \n",
      "Steps : 5826 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.057566042989492416  \n",
      "Steps : 5827 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756599083542824  \n",
      "Steps : 5828 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756594240665436  \n",
      "Steps : 5829 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756589397788048  \n",
      "Steps : 5830 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.0575658455491066  \n",
      "Steps : 5831 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756579712033272  \n",
      "Steps : 5832 , Loss : 0.7830864191055298, Weights 1.0596425533294678, bias : 0.05756574869155884  \n",
      "Steps : 5833 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756570026278496  \n",
      "Steps : 5834 , Loss : 0.7830862402915955, Weights 1.0596425533294678, bias : 0.05756565183401108  \n",
      "Steps : 5835 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.0575656034052372  \n",
      "Steps : 5836 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756555497646332  \n",
      "Steps : 5837 , Loss : 0.7830862998962402, Weights 1.0596425533294678, bias : 0.05756550654768944  \n",
      "Steps : 5838 , Loss : 0.7830864191055298, Weights 1.0596425533294678, bias : 0.05756545811891556  \n",
      "Steps : 5839 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756540969014168  \n",
      "Steps : 5840 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.0575653612613678  \n",
      "Steps : 5841 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.05756531283259392  \n",
      "Steps : 5842 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756526440382004  \n",
      "Steps : 5843 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756521597504616  \n",
      "Steps : 5844 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756516754627228  \n",
      "Steps : 5845 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.0575651191174984  \n",
      "Steps : 5846 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756507068872452  \n",
      "Steps : 5847 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.05756502225995064  \n",
      "Steps : 5848 , Loss : 0.7830862402915955, Weights 1.0596426725387573, bias : 0.05756497383117676  \n",
      "Steps : 5849 , Loss : 0.7830862402915955, Weights 1.0596426725387573, bias : 0.05756492540240288  \n",
      "Steps : 5850 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.057564876973629  \n",
      "Steps : 5851 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756482854485512  \n",
      "Steps : 5852 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756478011608124  \n",
      "Steps : 5853 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.05756473168730736  \n",
      "Steps : 5854 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.05756468325853348  \n",
      "Steps : 5855 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.0575646348297596  \n",
      "Steps : 5856 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.05756458640098572  \n",
      "Steps : 5857 , Loss : 0.7830864191055298, Weights 1.0596426725387573, bias : 0.05756453797221184  \n",
      "Steps : 5858 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756448954343796  \n",
      "Steps : 5859 , Loss : 0.7830862402915955, Weights 1.0596426725387573, bias : 0.05756444111466408  \n",
      "Steps : 5860 , Loss : 0.7830862402915955, Weights 1.0596426725387573, bias : 0.0575643926858902  \n",
      "Steps : 5861 , Loss : 0.7830862402915955, Weights 1.0596426725387573, bias : 0.05756434425711632  \n",
      "Steps : 5862 , Loss : 0.7830862998962402, Weights 1.0596426725387573, bias : 0.05756429582834244  \n",
      "Steps : 5863 , Loss : 0.7830862402915955, Weights 1.0596426725387573, bias : 0.05756424739956856  \n",
      "Steps : 5864 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756419897079468  \n",
      "Steps : 5865 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.0575641505420208  \n",
      "Steps : 5866 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756410211324692  \n",
      "Steps : 5867 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.057564057409763336  \n",
      "Steps : 5868 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.057564012706279755  \n",
      "Steps : 5869 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756396800279617  \n",
      "Steps : 5870 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756392329931259  \n",
      "Steps : 5871 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756387859582901  \n",
      "Steps : 5872 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756383389234543  \n",
      "Steps : 5873 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756378918886185  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5874 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.057563744485378265  \n",
      "Steps : 5875 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.057563699781894684  \n",
      "Steps : 5876 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.0575636550784111  \n",
      "Steps : 5877 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756361037492752  \n",
      "Steps : 5878 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756356567144394  \n",
      "Steps : 5879 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756352096796036  \n",
      "Steps : 5880 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.057563476264476776  \n",
      "Steps : 5881 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.057563431560993195  \n",
      "Steps : 5882 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756338685750961  \n",
      "Steps : 5883 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756334215402603  \n",
      "Steps : 5884 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756329745054245  \n",
      "Steps : 5885 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756325274705887  \n",
      "Steps : 5886 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756320804357529  \n",
      "Steps : 5887 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.057563163340091705  \n",
      "Steps : 5888 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.057563118636608124  \n",
      "Steps : 5889 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756307393312454  \n",
      "Steps : 5890 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756302922964096  \n",
      "Steps : 5891 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.05756298452615738  \n",
      "Steps : 5892 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.0575629398226738  \n",
      "Steps : 5893 , Loss : 0.7830862402915955, Weights 1.0596427917480469, bias : 0.057562895119190216  \n",
      "Steps : 5894 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.057562850415706635  \n",
      "Steps : 5895 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756280571222305  \n",
      "Steps : 5896 , Loss : 0.7830864191055298, Weights 1.0596427917480469, bias : 0.05756276100873947  \n",
      "Steps : 5897 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756271630525589  \n",
      "Steps : 5898 , Loss : 0.7830864191055298, Weights 1.0596427917480469, bias : 0.05756267160177231  \n",
      "Steps : 5899 , Loss : 0.7830862998962402, Weights 1.0596427917480469, bias : 0.05756262689828873  \n",
      "Steps : 5900 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.057562582194805145  \n",
      "Steps : 5901 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.057562537491321564  \n",
      "Steps : 5902 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756249278783798  \n",
      "Steps : 5903 , Loss : 0.7830862402915955, Weights 1.0596429109573364, bias : 0.0575624480843544  \n",
      "Steps : 5904 , Loss : 0.7830862402915955, Weights 1.0596429109573364, bias : 0.05756240338087082  \n",
      "Steps : 5905 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756235867738724  \n",
      "Steps : 5906 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.057562313973903656  \n",
      "Steps : 5907 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.057562269270420074  \n",
      "Steps : 5908 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756222456693649  \n",
      "Steps : 5909 , Loss : 0.7830862402915955, Weights 1.0596429109573364, bias : 0.05756218358874321  \n",
      "Steps : 5910 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756214261054993  \n",
      "Steps : 5911 , Loss : 0.7830864191055298, Weights 1.0596429109573364, bias : 0.057562101632356644  \n",
      "Steps : 5912 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756206065416336  \n",
      "Steps : 5913 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756201967597008  \n",
      "Steps : 5914 , Loss : 0.7830864191055298, Weights 1.0596429109573364, bias : 0.057561978697776794  \n",
      "Steps : 5915 , Loss : 0.7830864191055298, Weights 1.0596429109573364, bias : 0.05756193771958351  \n",
      "Steps : 5916 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756189674139023  \n",
      "Steps : 5917 , Loss : 0.7830864191055298, Weights 1.0596429109573364, bias : 0.057561855763196945  \n",
      "Steps : 5918 , Loss : 0.7830862402915955, Weights 1.0596429109573364, bias : 0.05756181478500366  \n",
      "Steps : 5919 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756177380681038  \n",
      "Steps : 5920 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.057561732828617096  \n",
      "Steps : 5921 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756169185042381  \n",
      "Steps : 5922 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756165087223053  \n",
      "Steps : 5923 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.05756160989403725  \n",
      "Steps : 5924 , Loss : 0.7830862402915955, Weights 1.0596429109573364, bias : 0.057561568915843964  \n",
      "Steps : 5925 , Loss : 0.7830862402915955, Weights 1.0596429109573364, bias : 0.05756152793765068  \n",
      "Steps : 5926 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.0575614869594574  \n",
      "Steps : 5927 , Loss : 0.7830862998962402, Weights 1.0596429109573364, bias : 0.057561445981264114  \n",
      "Steps : 5928 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756140500307083  \n",
      "Steps : 5929 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756136402487755  \n",
      "Steps : 5930 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.057561323046684265  \n",
      "Steps : 5931 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756128206849098  \n",
      "Steps : 5932 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.0575612410902977  \n",
      "Steps : 5933 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.057561200112104416  \n",
      "Steps : 5934 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756115913391113  \n",
      "Steps : 5935 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.05756111815571785  \n",
      "Steps : 5936 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.05756107717752457  \n",
      "Steps : 5937 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.057561036199331284  \n",
      "Steps : 5938 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.057560995221138  \n",
      "Steps : 5939 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756095424294472  \n",
      "Steps : 5940 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.057560913264751434  \n",
      "Steps : 5941 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756087228655815  \n",
      "Steps : 5942 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756083130836487  \n",
      "Steps : 5943 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.057560790330171585  \n",
      "Steps : 5944 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.0575607493519783  \n",
      "Steps : 5945 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.05756070837378502  \n",
      "Steps : 5946 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.057560667395591736  \n",
      "Steps : 5947 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756062641739845  \n",
      "Steps : 5948 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756058543920517  \n",
      "Steps : 5949 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756054446101189  \n",
      "Steps : 5950 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.057560503482818604  \n",
      "Steps : 5951 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756046250462532  \n",
      "Steps : 5952 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756042152643204  \n",
      "Steps : 5953 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.057560380548238754  \n",
      "Steps : 5954 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756034329533577  \n",
      "Steps : 5955 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.057560306042432785  \n",
      "Steps : 5956 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.0575602687895298  \n",
      "Steps : 5957 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.057560231536626816  \n",
      "Steps : 5958 , Loss : 0.7830862998962402, Weights 1.059643030166626, bias : 0.05756019428372383  \n",
      "Steps : 5959 , Loss : 0.7830864191055298, Weights 1.059643030166626, bias : 0.05756015703082085  \n",
      "Steps : 5960 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756011977791786  \n",
      "Steps : 5961 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756008252501488  \n",
      "Steps : 5962 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756004527211189  \n",
      "Steps : 5963 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05756000801920891  \n",
      "Steps : 5964 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05755997076630592  \n",
      "Steps : 5965 , Loss : 0.7830862402915955, Weights 1.059643030166626, bias : 0.05755993351340294  \n",
      "Steps : 5966 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559896260499954  \n",
      "Steps : 5967 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755985900759697  \n",
      "Steps : 5968 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559821754693985  \n",
      "Steps : 5969 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559784501791  \n",
      "Steps : 5970 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559747248888016  \n",
      "Steps : 5971 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755970999598503  \n",
      "Steps : 5972 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559672743082047  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 5973 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755963549017906  \n",
      "Steps : 5974 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755959823727608  \n",
      "Steps : 5975 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755956098437309  \n",
      "Steps : 5976 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755952373147011  \n",
      "Steps : 5977 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755948647856712  \n",
      "Steps : 5978 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755944922566414  \n",
      "Steps : 5979 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559411972761154  \n",
      "Steps : 5980 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755937471985817  \n",
      "Steps : 5981 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559337466955185  \n",
      "Steps : 5982 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.0575593002140522  \n",
      "Steps : 5983 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.057559262961149216  \n",
      "Steps : 5984 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755922570824623  \n",
      "Steps : 5985 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057559188455343246  \n",
      "Steps : 5986 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755915120244026  \n",
      "Steps : 5987 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755911394953728  \n",
      "Steps : 5988 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.05755907669663429  \n",
      "Steps : 5989 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755903944373131  \n",
      "Steps : 5990 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755900219082832  \n",
      "Steps : 5991 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.05755896493792534  \n",
      "Steps : 5992 , Loss : 0.7830862402915955, Weights 1.0596431493759155, bias : 0.057558927685022354  \n",
      "Steps : 5993 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755889043211937  \n",
      "Steps : 5994 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.057558853179216385  \n",
      "Steps : 5995 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.0575588159263134  \n",
      "Steps : 5996 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.057558778673410416  \n",
      "Steps : 5997 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755874142050743  \n",
      "Steps : 5998 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.057558704167604446  \n",
      "Steps : 5999 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.05755866691470146  \n",
      "Steps : 6000 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.05755862966179848  \n",
      "Steps : 6001 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755859240889549  \n",
      "Steps : 6002 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.05755855515599251  \n",
      "Steps : 6003 , Loss : 0.7830864191055298, Weights 1.0596431493759155, bias : 0.05755851790308952  \n",
      "Steps : 6004 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755848065018654  \n",
      "Steps : 6005 , Loss : 0.7830862998962402, Weights 1.0596431493759155, bias : 0.05755844712257385  \n",
      "Steps : 6006 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.057558413594961166  \n",
      "Steps : 6007 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.05755838006734848  \n",
      "Steps : 6008 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.057558346539735794  \n",
      "Steps : 6009 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755831301212311  \n",
      "Steps : 6010 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755827948451042  \n",
      "Steps : 6011 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.057558245956897736  \n",
      "Steps : 6012 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755821242928505  \n",
      "Steps : 6013 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755817890167236  \n",
      "Steps : 6014 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755814537405968  \n",
      "Steps : 6015 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755811184644699  \n",
      "Steps : 6016 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.057558078318834305  \n",
      "Steps : 6017 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755804479122162  \n",
      "Steps : 6018 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.05755801126360893  \n",
      "Steps : 6019 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.057557977735996246  \n",
      "Steps : 6020 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.05755794420838356  \n",
      "Steps : 6021 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.057557910680770874  \n",
      "Steps : 6022 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755787715315819  \n",
      "Steps : 6023 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.0575578436255455  \n",
      "Steps : 6024 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.057557810097932816  \n",
      "Steps : 6025 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755777657032013  \n",
      "Steps : 6026 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755774304270744  \n",
      "Steps : 6027 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755770951509476  \n",
      "Steps : 6028 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755767598748207  \n",
      "Steps : 6029 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.057557642459869385  \n",
      "Steps : 6030 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.0575576089322567  \n",
      "Steps : 6031 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755757540464401  \n",
      "Steps : 6032 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.057557541877031326  \n",
      "Steps : 6033 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.05755750834941864  \n",
      "Steps : 6034 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.057557474821805954  \n",
      "Steps : 6035 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755744129419327  \n",
      "Steps : 6036 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.05755740776658058  \n",
      "Steps : 6037 , Loss : 0.7830862402915955, Weights 1.059643268585205, bias : 0.057557374238967896  \n",
      "Steps : 6038 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755734071135521  \n",
      "Steps : 6039 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755730718374252  \n",
      "Steps : 6040 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755727365612984  \n",
      "Steps : 6041 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755724012851715  \n",
      "Steps : 6042 , Loss : 0.7830864191055298, Weights 1.059643268585205, bias : 0.057557206600904465  \n",
      "Steps : 6043 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755717307329178  \n",
      "Steps : 6044 , Loss : 0.7830862998962402, Weights 1.059643268585205, bias : 0.05755713954567909  \n",
      "Steps : 6045 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057557106018066406  \n",
      "Steps : 6046 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755707249045372  \n",
      "Steps : 6047 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057557038962841034  \n",
      "Steps : 6048 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755700543522835  \n",
      "Steps : 6049 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755697190761566  \n",
      "Steps : 6050 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057556938380002975  \n",
      "Steps : 6051 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755690485239029  \n",
      "Steps : 6052 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.0575568713247776  \n",
      "Steps : 6053 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755683779716492  \n",
      "Steps : 6054 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755680426955223  \n",
      "Steps : 6055 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057556770741939545  \n",
      "Steps : 6056 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755673721432686  \n",
      "Steps : 6057 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755670368671417  \n",
      "Steps : 6058 , Loss : 0.7830862402915955, Weights 1.0596433877944946, bias : 0.057556670159101486  \n",
      "Steps : 6059 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.0575566366314888  \n",
      "Steps : 6060 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057556603103876114  \n",
      "Steps : 6061 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755656957626343  \n",
      "Steps : 6062 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755653977394104  \n",
      "Steps : 6063 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755650997161865  \n",
      "Steps : 6064 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.057556480169296265  \n",
      "Steps : 6065 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755645036697388  \n",
      "Steps : 6066 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755642056465149  \n",
      "Steps : 6067 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.0575563907623291  \n",
      "Steps : 6068 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.057556360960006714  \n",
      "Steps : 6069 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.057556331157684326  \n",
      "Steps : 6070 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755630135536194  \n",
      "Steps : 6071 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755627155303955  \n",
      "Steps : 6072 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755624175071716  \n",
      "Steps : 6073 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057556211948394775  \n",
      "Steps : 6074 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755618214607239  \n",
      "Steps : 6075 , Loss : 0.7830862402915955, Weights 1.0596433877944946, bias : 0.05755615234375  \n",
      "Steps : 6076 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755612254142761  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6077 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057556092739105225  \n",
      "Steps : 6078 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755606293678284  \n",
      "Steps : 6079 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755603313446045  \n",
      "Steps : 6080 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755600333213806  \n",
      "Steps : 6081 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057555973529815674  \n",
      "Steps : 6082 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057555943727493286  \n",
      "Steps : 6083 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.0575559139251709  \n",
      "Steps : 6084 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755588412284851  \n",
      "Steps : 6085 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755585432052612  \n",
      "Steps : 6086 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.057555824518203735  \n",
      "Steps : 6087 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755579471588135  \n",
      "Steps : 6088 , Loss : 0.7830864191055298, Weights 1.0596433877944946, bias : 0.05755576491355896  \n",
      "Steps : 6089 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.05755573511123657  \n",
      "Steps : 6090 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.057555705308914185  \n",
      "Steps : 6091 , Loss : 0.7830862998962402, Weights 1.0596433877944946, bias : 0.0575556755065918  \n",
      "Steps : 6092 , Loss : 0.7830864191055298, Weights 1.0596435070037842, bias : 0.05755564570426941  \n",
      "Steps : 6093 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755561590194702  \n",
      "Steps : 6094 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.057555586099624634  \n",
      "Steps : 6095 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.057555556297302246  \n",
      "Steps : 6096 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755552649497986  \n",
      "Steps : 6097 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755549669265747  \n",
      "Steps : 6098 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755546689033508  \n",
      "Steps : 6099 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.057555437088012695  \n",
      "Steps : 6100 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755540728569031  \n",
      "Steps : 6101 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755537748336792  \n",
      "Steps : 6102 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755534768104553  \n",
      "Steps : 6103 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057555317878723145  \n",
      "Steps : 6104 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755528807640076  \n",
      "Steps : 6105 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755525827407837  \n",
      "Steps : 6106 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755522847175598  \n",
      "Steps : 6107 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057555198669433594  \n",
      "Steps : 6108 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057555168867111206  \n",
      "Steps : 6109 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755513906478882  \n",
      "Steps : 6110 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755510926246643  \n",
      "Steps : 6111 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755507946014404  \n",
      "Steps : 6112 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057555049657821655  \n",
      "Steps : 6113 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755501985549927  \n",
      "Steps : 6114 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755499005317688  \n",
      "Steps : 6115 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755496025085449  \n",
      "Steps : 6116 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057554930448532104  \n",
      "Steps : 6117 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755490064620972  \n",
      "Steps : 6118 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755487084388733  \n",
      "Steps : 6119 , Loss : 0.7830864191055298, Weights 1.0596435070037842, bias : 0.05755484104156494  \n",
      "Steps : 6120 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.057554811239242554  \n",
      "Steps : 6121 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.057554781436920166  \n",
      "Steps : 6122 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755475163459778  \n",
      "Steps : 6123 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755472555756569  \n",
      "Steps : 6124 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.0575546994805336  \n",
      "Steps : 6125 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755467340350151  \n",
      "Steps : 6126 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755464732646942  \n",
      "Steps : 6127 , Loss : 0.7830864191055298, Weights 1.0596435070037842, bias : 0.05755462124943733  \n",
      "Steps : 6128 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755459517240524  \n",
      "Steps : 6129 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057554569095373154  \n",
      "Steps : 6130 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.057554543018341064  \n",
      "Steps : 6131 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.057554516941308975  \n",
      "Steps : 6132 , Loss : 0.7830864191055298, Weights 1.0596435070037842, bias : 0.057554490864276886  \n",
      "Steps : 6133 , Loss : 0.7830864191055298, Weights 1.0596435070037842, bias : 0.0575544647872448  \n",
      "Steps : 6134 , Loss : 0.7830864191055298, Weights 1.0596435070037842, bias : 0.05755443871021271  \n",
      "Steps : 6135 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755441263318062  \n",
      "Steps : 6136 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755438655614853  \n",
      "Steps : 6137 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755436047911644  \n",
      "Steps : 6138 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755433440208435  \n",
      "Steps : 6139 , Loss : 0.7830862402915955, Weights 1.0596435070037842, bias : 0.05755430832505226  \n",
      "Steps : 6140 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755428224802017  \n",
      "Steps : 6141 , Loss : 0.7830862998962402, Weights 1.0596435070037842, bias : 0.05755425617098808  \n",
      "Steps : 6142 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057554230093955994  \n",
      "Steps : 6143 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057554204016923904  \n",
      "Steps : 6144 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.057554177939891815  \n",
      "Steps : 6145 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.057554151862859726  \n",
      "Steps : 6146 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755412578582764  \n",
      "Steps : 6147 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755409970879555  \n",
      "Steps : 6148 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755407363176346  \n",
      "Steps : 6149 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755404755473137  \n",
      "Steps : 6150 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755402147769928  \n",
      "Steps : 6151 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755399540066719  \n",
      "Steps : 6152 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.0575539693236351  \n",
      "Steps : 6153 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755394324660301  \n",
      "Steps : 6154 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755391716957092  \n",
      "Steps : 6155 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.057553891092538834  \n",
      "Steps : 6156 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.057553865015506744  \n",
      "Steps : 6157 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.057553838938474655  \n",
      "Steps : 6158 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.057553812861442566  \n",
      "Steps : 6159 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755378678441048  \n",
      "Steps : 6160 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755376070737839  \n",
      "Steps : 6161 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.0575537346303463  \n",
      "Steps : 6162 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755370855331421  \n",
      "Steps : 6163 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755368247628212  \n",
      "Steps : 6164 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755365639925003  \n",
      "Steps : 6165 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755363032221794  \n",
      "Steps : 6166 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755360424518585  \n",
      "Steps : 6167 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755357816815376  \n",
      "Steps : 6168 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.057553552091121674  \n",
      "Steps : 6169 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.057553526014089584  \n",
      "Steps : 6170 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.057553499937057495  \n",
      "Steps : 6171 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057553473860025406  \n",
      "Steps : 6172 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755344778299332  \n",
      "Steps : 6173 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755342170596123  \n",
      "Steps : 6174 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755339562892914  \n",
      "Steps : 6175 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755336955189705  \n",
      "Steps : 6176 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755334347486496  \n",
      "Steps : 6177 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755331739783287  \n",
      "Steps : 6178 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755329132080078  \n",
      "Steps : 6179 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.05755326524376869  \n",
      "Steps : 6180 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.0575532391667366  \n",
      "Steps : 6181 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057553213089704514  \n",
      "Steps : 6182 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057553187012672424  \n",
      "Steps : 6183 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057553160935640335  \n",
      "Steps : 6184 , Loss : 0.7830864191055298, Weights 1.0596436262130737, bias : 0.057553134858608246  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6185 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755310878157616  \n",
      "Steps : 6186 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755308270454407  \n",
      "Steps : 6187 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755305662751198  \n",
      "Steps : 6188 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755303055047989  \n",
      "Steps : 6189 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.0575530044734478  \n",
      "Steps : 6190 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755297839641571  \n",
      "Steps : 6191 , Loss : 0.7830862998962402, Weights 1.0596436262130737, bias : 0.05755295231938362  \n",
      "Steps : 6192 , Loss : 0.7830862402915955, Weights 1.0596436262130737, bias : 0.05755292624235153  \n",
      "Steps : 6193 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755290016531944  \n",
      "Steps : 6194 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057552874088287354  \n",
      "Steps : 6195 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755285173654556  \n",
      "Steps : 6196 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755282938480377  \n",
      "Steps : 6197 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755280703306198  \n",
      "Steps : 6198 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755278468132019  \n",
      "Steps : 6199 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.0575527623295784  \n",
      "Steps : 6200 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755273997783661  \n",
      "Steps : 6201 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755271762609482  \n",
      "Steps : 6202 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755269527435303  \n",
      "Steps : 6203 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755267292261124  \n",
      "Steps : 6204 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.057552650570869446  \n",
      "Steps : 6205 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.057552628219127655  \n",
      "Steps : 6206 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057552605867385864  \n",
      "Steps : 6207 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.057552583515644073  \n",
      "Steps : 6208 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755256116390228  \n",
      "Steps : 6209 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755253881216049  \n",
      "Steps : 6210 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.0575525164604187  \n",
      "Steps : 6211 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755249410867691  \n",
      "Steps : 6212 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755247175693512  \n",
      "Steps : 6213 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755244940519333  \n",
      "Steps : 6214 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755242705345154  \n",
      "Steps : 6215 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755240470170975  \n",
      "Steps : 6216 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755238234996796  \n",
      "Steps : 6217 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057552359998226166  \n",
      "Steps : 6218 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057552337646484375  \n",
      "Steps : 6219 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057552315294742584  \n",
      "Steps : 6220 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755229294300079  \n",
      "Steps : 6221 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057552270591259  \n",
      "Steps : 6222 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755224823951721  \n",
      "Steps : 6223 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755222588777542  \n",
      "Steps : 6224 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755220353603363  \n",
      "Steps : 6225 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755218118429184  \n",
      "Steps : 6226 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755215883255005  \n",
      "Steps : 6227 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755213648080826  \n",
      "Steps : 6228 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755211412906647  \n",
      "Steps : 6229 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057552091777324677  \n",
      "Steps : 6230 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057552069425582886  \n",
      "Steps : 6231 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057552047073841095  \n",
      "Steps : 6232 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057552024722099304  \n",
      "Steps : 6233 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755200237035751  \n",
      "Steps : 6234 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755198001861572  \n",
      "Steps : 6235 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755195766687393  \n",
      "Steps : 6236 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755193531513214  \n",
      "Steps : 6237 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755191296339035  \n",
      "Steps : 6238 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755189061164856  \n",
      "Steps : 6239 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755186825990677  \n",
      "Steps : 6240 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755184590816498  \n",
      "Steps : 6241 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755182355642319  \n",
      "Steps : 6242 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057551801204681396  \n",
      "Steps : 6243 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057551778852939606  \n",
      "Steps : 6244 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057551756501197815  \n",
      "Steps : 6245 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057551734149456024  \n",
      "Steps : 6246 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755171179771423  \n",
      "Steps : 6247 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755168944597244  \n",
      "Steps : 6248 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755166709423065  \n",
      "Steps : 6249 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755164474248886  \n",
      "Steps : 6250 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755162239074707  \n",
      "Steps : 6251 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755160003900528  \n",
      "Steps : 6252 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755157768726349  \n",
      "Steps : 6253 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.0575515553355217  \n",
      "Steps : 6254 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755153298377991  \n",
      "Steps : 6255 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057551510632038116  \n",
      "Steps : 6256 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057551488280296326  \n",
      "Steps : 6257 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.057551465928554535  \n",
      "Steps : 6258 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057551443576812744  \n",
      "Steps : 6259 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755142122507095  \n",
      "Steps : 6260 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755139887332916  \n",
      "Steps : 6261 , Loss : 0.7830864191055298, Weights 1.0596437454223633, bias : 0.05755137652158737  \n",
      "Steps : 6262 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.05755135416984558  \n",
      "Steps : 6263 , Loss : 0.7830862402915955, Weights 1.0596437454223633, bias : 0.05755133181810379  \n",
      "Steps : 6264 , Loss : 0.7830862998962402, Weights 1.0596437454223633, bias : 0.057551309466362  \n",
      "Steps : 6265 , Loss : 0.7830864191055298, Weights 1.0596438646316528, bias : 0.05755128711462021  \n",
      "Steps : 6266 , Loss : 0.7830864191055298, Weights 1.0596438646316528, bias : 0.05755126476287842  \n",
      "Steps : 6267 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755124241113663  \n",
      "Steps : 6268 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057551220059394836  \n",
      "Steps : 6269 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057551197707653046  \n",
      "Steps : 6270 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057551175355911255  \n",
      "Steps : 6271 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057551153004169464  \n",
      "Steps : 6272 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755113065242767  \n",
      "Steps : 6273 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755110830068588  \n",
      "Steps : 6274 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755108594894409  \n",
      "Steps : 6275 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.0575510635972023  \n",
      "Steps : 6276 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755104124546051  \n",
      "Steps : 6277 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755101889371872  \n",
      "Steps : 6278 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755099654197693  \n",
      "Steps : 6279 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755097419023514  \n",
      "Steps : 6280 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550955563783646  \n",
      "Steps : 6281 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755093693733215  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6282 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755091831088066  \n",
      "Steps : 6283 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755089968442917  \n",
      "Steps : 6284 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550881057977676  \n",
      "Steps : 6285 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550862431526184  \n",
      "Steps : 6286 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755084380507469  \n",
      "Steps : 6287 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.0575508251786232  \n",
      "Steps : 6288 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755080655217171  \n",
      "Steps : 6289 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550787925720215  \n",
      "Steps : 6290 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755076929926872  \n",
      "Steps : 6291 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755075067281723  \n",
      "Steps : 6292 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755073204636574  \n",
      "Steps : 6293 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550713419914246  \n",
      "Steps : 6294 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755069479346275  \n",
      "Steps : 6295 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755067616701126  \n",
      "Steps : 6296 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755065754055977  \n",
      "Steps : 6297 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550638914108276  \n",
      "Steps : 6298 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550620287656784  \n",
      "Steps : 6299 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755060166120529  \n",
      "Steps : 6300 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.0575505830347538  \n",
      "Steps : 6301 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755056440830231  \n",
      "Steps : 6302 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550545781850815  \n",
      "Steps : 6303 , Loss : 0.7830864191055298, Weights 1.0596438646316528, bias : 0.05755052715539932  \n",
      "Steps : 6304 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755050852894783  \n",
      "Steps : 6305 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755048990249634  \n",
      "Steps : 6306 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550471276044846  \n",
      "Steps : 6307 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755045264959335  \n",
      "Steps : 6308 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755043402314186  \n",
      "Steps : 6309 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05755041539669037  \n",
      "Steps : 6310 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550396770238876  \n",
      "Steps : 6311 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550378143787384  \n",
      "Steps : 6312 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755035951733589  \n",
      "Steps : 6313 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.0575503408908844  \n",
      "Steps : 6314 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755032226443291  \n",
      "Steps : 6315 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550303637981415  \n",
      "Steps : 6316 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755028501152992  \n",
      "Steps : 6317 , Loss : 0.7830864191055298, Weights 1.0596438646316528, bias : 0.05755026638507843  \n",
      "Steps : 6318 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755024775862694  \n",
      "Steps : 6319 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.057550229132175446  \n",
      "Steps : 6320 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755021050572395  \n",
      "Steps : 6321 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755019187927246  \n",
      "Steps : 6322 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755017325282097  \n",
      "Steps : 6323 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550154626369476  \n",
      "Steps : 6324 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550135999917984  \n",
      "Steps : 6325 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755011737346649  \n",
      "Steps : 6326 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550098747015  \n",
      "Steps : 6327 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755008012056351  \n",
      "Steps : 6328 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057550061494112015  \n",
      "Steps : 6329 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755004286766052  \n",
      "Steps : 6330 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755002424120903  \n",
      "Steps : 6331 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05755000561475754  \n",
      "Steps : 6332 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057549986988306046  \n",
      "Steps : 6333 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05754996836185455  \n",
      "Steps : 6334 , Loss : 0.7830862402915955, Weights 1.0596438646316528, bias : 0.05754994973540306  \n",
      "Steps : 6335 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.05754993110895157  \n",
      "Steps : 6336 , Loss : 0.7830864191055298, Weights 1.0596438646316528, bias : 0.057549912482500076  \n",
      "Steps : 6337 , Loss : 0.7830862998962402, Weights 1.0596438646316528, bias : 0.057549893856048584  \n",
      "Steps : 6338 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754987522959709  \n",
      "Steps : 6339 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.0575498566031456  \n",
      "Steps : 6340 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754983797669411  \n",
      "Steps : 6341 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549819350242615  \n",
      "Steps : 6342 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754980072379112  \n",
      "Steps : 6343 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754978209733963  \n",
      "Steps : 6344 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754976347088814  \n",
      "Steps : 6345 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549744844436646  \n",
      "Steps : 6346 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.05754972621798515  \n",
      "Steps : 6347 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754970759153366  \n",
      "Steps : 6348 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754968896508217  \n",
      "Steps : 6349 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057549670338630676  \n",
      "Steps : 6350 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057549651712179184  \n",
      "Steps : 6351 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754963308572769  \n",
      "Steps : 6352 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.0575496144592762  \n",
      "Steps : 6353 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754959583282471  \n",
      "Steps : 6354 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057549577206373215  \n",
      "Steps : 6355 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754955857992172  \n",
      "Steps : 6356 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754953995347023  \n",
      "Steps : 6357 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754952132701874  \n",
      "Steps : 6358 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549502700567245  \n",
      "Steps : 6359 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754948407411575  \n",
      "Steps : 6360 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754946544766426  \n",
      "Steps : 6361 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754944682121277  \n",
      "Steps : 6362 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057549428194761276  \n",
      "Steps : 6363 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057549409568309784  \n",
      "Steps : 6364 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754939094185829  \n",
      "Steps : 6365 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.0575493723154068  \n",
      "Steps : 6366 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754935368895531  \n",
      "Steps : 6367 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549335062503815  \n",
      "Steps : 6368 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754931643605232  \n",
      "Steps : 6369 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754929780960083  \n",
      "Steps : 6370 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754927918314934  \n",
      "Steps : 6371 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549260556697845  \n",
      "Steps : 6372 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754924193024635  \n",
      "Steps : 6373 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754922330379486  \n",
      "Steps : 6374 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.05754920467734337  \n",
      "Steps : 6375 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549186050891876  \n",
      "Steps : 6376 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549167424440384  \n",
      "Steps : 6377 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754914879798889  \n",
      "Steps : 6378 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.0575491301715374  \n",
      "Steps : 6379 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549115270376205  \n",
      "Steps : 6380 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754910036921501  \n",
      "Steps : 6381 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754908546805382  \n",
      "Steps : 6382 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549070566892624  \n",
      "Steps : 6383 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754905566573143  \n",
      "Steps : 6384 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057549040764570236  \n",
      "Steps : 6385 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754902586340904  \n",
      "Steps : 6386 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754901096224785  \n",
      "Steps : 6387 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057548996061086655  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6388 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754898115992546  \n",
      "Steps : 6389 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754896625876427  \n",
      "Steps : 6390 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754895135760307  \n",
      "Steps : 6391 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754893645644188  \n",
      "Steps : 6392 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057548921555280685  \n",
      "Steps : 6393 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754890665411949  \n",
      "Steps : 6394 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.0575488917529583  \n",
      "Steps : 6395 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.057548876851797104  \n",
      "Steps : 6396 , Loss : 0.7830864191055298, Weights 1.0596439838409424, bias : 0.05754886195063591  \n",
      "Steps : 6397 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057548847049474716  \n",
      "Steps : 6398 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.05754883214831352  \n",
      "Steps : 6399 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.05754881724715233  \n",
      "Steps : 6400 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.057548802345991135  \n",
      "Steps : 6401 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.05754878744482994  \n",
      "Steps : 6402 , Loss : 0.7830862402915955, Weights 1.0596439838409424, bias : 0.05754877254366875  \n",
      "Steps : 6403 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754875764250755  \n",
      "Steps : 6404 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754874274134636  \n",
      "Steps : 6405 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057548727840185165  \n",
      "Steps : 6406 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754871293902397  \n",
      "Steps : 6407 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754869803786278  \n",
      "Steps : 6408 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057548683136701584  \n",
      "Steps : 6409 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.05754866823554039  \n",
      "Steps : 6410 , Loss : 0.7830862998962402, Weights 1.0596439838409424, bias : 0.057548653334379196  \n",
      "Steps : 6411 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548638433218  \n",
      "Steps : 6412 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754862353205681  \n",
      "Steps : 6413 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057548608630895615  \n",
      "Steps : 6414 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754859372973442  \n",
      "Steps : 6415 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754857882857323  \n",
      "Steps : 6416 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754856392741203  \n",
      "Steps : 6417 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754854902625084  \n",
      "Steps : 6418 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548534125089645  \n",
      "Steps : 6419 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754851922392845  \n",
      "Steps : 6420 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754850432276726  \n",
      "Steps : 6421 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548489421606064  \n",
      "Steps : 6422 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754847452044487  \n",
      "Steps : 6423 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548459619283676  \n",
      "Steps : 6424 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754844471812248  \n",
      "Steps : 6425 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754842981696129  \n",
      "Steps : 6426 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548414915800095  \n",
      "Steps : 6427 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.0575484000146389  \n",
      "Steps : 6428 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754838511347771  \n",
      "Steps : 6429 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754837021231651  \n",
      "Steps : 6430 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754835531115532  \n",
      "Steps : 6431 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548340409994125  \n",
      "Steps : 6432 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754832550883293  \n",
      "Steps : 6433 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754831060767174  \n",
      "Steps : 6434 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548295706510544  \n",
      "Steps : 6435 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754828080534935  \n",
      "Steps : 6436 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548265904188156  \n",
      "Steps : 6437 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754825100302696  \n",
      "Steps : 6438 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754823610186577  \n",
      "Steps : 6439 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057548221200704575  \n",
      "Steps : 6440 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754820629954338  \n",
      "Steps : 6441 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754819139838219  \n",
      "Steps : 6442 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754817649722099  \n",
      "Steps : 6443 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.0575481615960598  \n",
      "Steps : 6444 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548146694898605  \n",
      "Steps : 6445 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754813179373741  \n",
      "Steps : 6446 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754811689257622  \n",
      "Steps : 6447 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057548101991415024  \n",
      "Steps : 6448 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754808709025383  \n",
      "Steps : 6449 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057548072189092636  \n",
      "Steps : 6450 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754805728793144  \n",
      "Steps : 6451 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754804238677025  \n",
      "Steps : 6452 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057548027485609055  \n",
      "Steps : 6453 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754801258444786  \n",
      "Steps : 6454 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754799768328667  \n",
      "Steps : 6455 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754798278212547  \n",
      "Steps : 6456 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754796788096428  \n",
      "Steps : 6457 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547952979803085  \n",
      "Steps : 6458 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754793807864189  \n",
      "Steps : 6459 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.0575479231774807  \n",
      "Steps : 6460 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057547908276319504  \n",
      "Steps : 6461 , Loss : 0.7830864191055298, Weights 1.059644103050232, bias : 0.05754789337515831  \n",
      "Steps : 6462 , Loss : 0.7830864191055298, Weights 1.059644103050232, bias : 0.057547878473997116  \n",
      "Steps : 6463 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754786357283592  \n",
      "Steps : 6464 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754784867167473  \n",
      "Steps : 6465 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547833770513535  \n",
      "Steps : 6466 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754781886935234  \n",
      "Steps : 6467 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754780396819115  \n",
      "Steps : 6468 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754778906702995  \n",
      "Steps : 6469 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754777416586876  \n",
      "Steps : 6470 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547759264707565  \n",
      "Steps : 6471 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754774436354637  \n",
      "Steps : 6472 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754772946238518  \n",
      "Steps : 6473 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547714561223984  \n",
      "Steps : 6474 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754769966006279  \n",
      "Steps : 6475 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057547684758901596  \n",
      "Steps : 6476 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.0575476698577404  \n",
      "Steps : 6477 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754765495657921  \n",
      "Steps : 6478 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547640055418015  \n",
      "Steps : 6479 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754762515425682  \n",
      "Steps : 6480 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754761025309563  \n",
      "Steps : 6481 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754759535193443  \n",
      "Steps : 6482 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754758045077324  \n",
      "Steps : 6483 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057547565549612045  \n",
      "Steps : 6484 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754755064845085  \n",
      "Steps : 6485 , Loss : 0.7830864191055298, Weights 1.059644103050232, bias : 0.05754753574728966  \n",
      "Steps : 6486 , Loss : 0.7830864191055298, Weights 1.059644103050232, bias : 0.057547520846128464  \n",
      "Steps : 6487 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754750594496727  \n",
      "Steps : 6488 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.057547491043806076  \n",
      "Steps : 6489 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754747614264488  \n",
      "Steps : 6490 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754746124148369  \n",
      "Steps : 6491 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547446340322495  \n",
      "Steps : 6492 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.0575474314391613  \n",
      "Steps : 6493 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754741653800011  \n",
      "Steps : 6494 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754740163683891  \n",
      "Steps : 6495 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754738673567772  \n",
      "Steps : 6496 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547371834516525  \n",
      "Steps : 6497 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754735693335533  \n",
      "Steps : 6498 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754734203219414  \n",
      "Steps : 6499 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.057547327131032944  \n",
      "Steps : 6500 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754731222987175  \n",
      "Steps : 6501 , Loss : 0.7830864191055298, Weights 1.059644103050232, bias : 0.057547297328710556  \n",
      "Steps : 6502 , Loss : 0.7830862998962402, Weights 1.059644103050232, bias : 0.05754728242754936  \n",
      "Steps : 6503 , Loss : 0.7830862402915955, Weights 1.059644103050232, bias : 0.05754726752638817  \n",
      "Steps : 6504 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057547252625226974  \n",
      "Steps : 6505 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754723772406578  \n",
      "Steps : 6506 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057547226548194885  \n",
      "Steps : 6507 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754721537232399  \n",
      "Steps : 6508 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057547204196453094  \n",
      "Steps : 6509 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.0575471930205822  \n",
      "Steps : 6510 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057547181844711304  \n",
      "Steps : 6511 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754717066884041  \n",
      "Steps : 6512 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754715949296951  \n",
      "Steps : 6513 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754714831709862  \n",
      "Steps : 6514 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754713714122772  \n",
      "Steps : 6515 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754712596535683  \n",
      "Steps : 6516 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754711478948593  \n",
      "Steps : 6517 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057547103613615036  \n",
      "Steps : 6518 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754709243774414  \n",
      "Steps : 6519 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057547081261873245  \n",
      "Steps : 6520 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754707008600235  \n",
      "Steps : 6521 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057547058910131454  \n",
      "Steps : 6522 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754704773426056  \n",
      "Steps : 6523 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057547036558389664  \n",
      "Steps : 6524 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754702538251877  \n",
      "Steps : 6525 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754701420664787  \n",
      "Steps : 6526 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754700303077698  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6527 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754699185490608  \n",
      "Steps : 6528 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754698067903519  \n",
      "Steps : 6529 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754696950316429  \n",
      "Steps : 6530 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546958327293396  \n",
      "Steps : 6531 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.0575469471514225  \n",
      "Steps : 6532 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.057546935975551605  \n",
      "Steps : 6533 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754692479968071  \n",
      "Steps : 6534 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.057546913623809814  \n",
      "Steps : 6535 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754690244793892  \n",
      "Steps : 6536 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.057546891272068024  \n",
      "Steps : 6537 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754688009619713  \n",
      "Steps : 6538 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754686892032623  \n",
      "Steps : 6539 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754685774445534  \n",
      "Steps : 6540 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754684656858444  \n",
      "Steps : 6541 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754683539271355  \n",
      "Steps : 6542 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754682421684265  \n",
      "Steps : 6543 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546813040971756  \n",
      "Steps : 6544 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754680186510086  \n",
      "Steps : 6545 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546790689229965  \n",
      "Steps : 6546 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754677951335907  \n",
      "Steps : 6547 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546768337488174  \n",
      "Steps : 6548 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754675716161728  \n",
      "Steps : 6549 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546745985746384  \n",
      "Steps : 6550 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754673480987549  \n",
      "Steps : 6551 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754672363400459  \n",
      "Steps : 6552 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.0575467124581337  \n",
      "Steps : 6553 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.0575467012822628  \n",
      "Steps : 6554 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754669010639191  \n",
      "Steps : 6555 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754667893052101  \n",
      "Steps : 6556 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546667754650116  \n",
      "Steps : 6557 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754665657877922  \n",
      "Steps : 6558 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546645402908325  \n",
      "Steps : 6559 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754663422703743  \n",
      "Steps : 6560 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546623051166534  \n",
      "Steps : 6561 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754661187529564  \n",
      "Steps : 6562 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546600699424744  \n",
      "Steps : 6563 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754658952355385  \n",
      "Steps : 6564 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754657834768295  \n",
      "Steps : 6565 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754656717181206  \n",
      "Steps : 6566 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754655599594116  \n",
      "Steps : 6567 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754654482007027  \n",
      "Steps : 6568 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754653364419937  \n",
      "Steps : 6569 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546522468328476  \n",
      "Steps : 6570 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754651129245758  \n",
      "Steps : 6571 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546500116586685  \n",
      "Steps : 6572 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754648894071579  \n",
      "Steps : 6573 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546477764844894  \n",
      "Steps : 6574 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546466588974  \n",
      "Steps : 6575 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546455413103104  \n",
      "Steps : 6576 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754644423723221  \n",
      "Steps : 6577 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754643306136131  \n",
      "Steps : 6578 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754642188549042  \n",
      "Steps : 6579 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754641070961952  \n",
      "Steps : 6580 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754639953374863  \n",
      "Steps : 6581 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754638835787773  \n",
      "Steps : 6582 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546377182006836  \n",
      "Steps : 6583 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754636600613594  \n",
      "Steps : 6584 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546354830265045  \n",
      "Steps : 6585 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754634365439415  \n",
      "Steps : 6586 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546332478523254  \n",
      "Steps : 6587 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754632130265236  \n",
      "Steps : 6588 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.057546310126781464  \n",
      "Steps : 6589 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754629895091057  \n",
      "Steps : 6590 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754628777503967  \n",
      "Steps : 6591 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754627659916878  \n",
      "Steps : 6592 , Loss : 0.7830862402915955, Weights 1.0596442222595215, bias : 0.05754626542329788  \n",
      "Steps : 6593 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754625424742699  \n",
      "Steps : 6594 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754624307155609  \n",
      "Steps : 6595 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546231895685196  \n",
      "Steps : 6596 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.0575462207198143  \n",
      "Steps : 6597 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546209543943405  \n",
      "Steps : 6598 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754619836807251  \n",
      "Steps : 6599 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546187192201614  \n",
      "Steps : 6600 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754617601633072  \n",
      "Steps : 6601 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546164840459824  \n",
      "Steps : 6602 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754615366458893  \n",
      "Steps : 6603 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754614248871803  \n",
      "Steps : 6604 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754613131284714  \n",
      "Steps : 6605 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754612013697624  \n",
      "Steps : 6606 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754610896110535  \n",
      "Steps : 6607 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754609778523445  \n",
      "Steps : 6608 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546086609363556  \n",
      "Steps : 6609 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754607543349266  \n",
      "Steps : 6610 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.057546064257621765  \n",
      "Steps : 6611 , Loss : 0.7830862998962402, Weights 1.0596442222595215, bias : 0.05754605308175087  \n",
      "Steps : 6612 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.057546041905879974  \n",
      "Steps : 6613 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.05754603073000908  \n",
      "Steps : 6614 , Loss : 0.7830864191055298, Weights 1.0596442222595215, bias : 0.057546019554138184  \n",
      "Steps : 6615 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754600837826729  \n",
      "Steps : 6616 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754599720239639  \n",
      "Steps : 6617 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575459860265255  \n",
      "Steps : 6618 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575459748506546  \n",
      "Steps : 6619 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754596367478371  \n",
      "Steps : 6620 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754595249891281  \n",
      "Steps : 6621 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545941323041916  \n",
      "Steps : 6622 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754593014717102  \n",
      "Steps : 6623 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545918971300125  \n",
      "Steps : 6624 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754590779542923  \n",
      "Steps : 6625 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545896619558334  \n",
      "Steps : 6626 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754588544368744  \n",
      "Steps : 6627 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545874267816544  \n",
      "Steps : 6628 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754586309194565  \n",
      "Steps : 6629 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754585191607475  \n",
      "Steps : 6630 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754584074020386  \n",
      "Steps : 6631 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754582956433296  \n",
      "Steps : 6632 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754581838846207  \n",
      "Steps : 6633 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754580721259117  \n",
      "Steps : 6634 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545796036720276  \n",
      "Steps : 6635 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754578486084938  \n",
      "Steps : 6636 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545773684978485  \n",
      "Steps : 6637 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754576250910759  \n",
      "Steps : 6638 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545751333236694  \n",
      "Steps : 6639 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575457401573658  \n",
      "Steps : 6640 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545728981494904  \n",
      "Steps : 6641 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754571780562401  \n",
      "Steps : 6642 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754570662975311  \n",
      "Steps : 6643 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754569545388222  \n",
      "Steps : 6644 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754568427801132  \n",
      "Steps : 6645 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754567310214043  \n",
      "Steps : 6646 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754566192626953  \n",
      "Steps : 6647 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545650750398636  \n",
      "Steps : 6648 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754563957452774  \n",
      "Steps : 6649 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545628398656845  \n",
      "Steps : 6650 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754561722278595  \n",
      "Steps : 6651 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545606046915054  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6652 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754559487104416  \n",
      "Steps : 6653 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545583695173264  \n",
      "Steps : 6654 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754557251930237  \n",
      "Steps : 6655 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754556134343147  \n",
      "Steps : 6656 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754555016756058  \n",
      "Steps : 6657 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754553899168968  \n",
      "Steps : 6658 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754552781581879  \n",
      "Steps : 6659 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754551663994789  \n",
      "Steps : 6660 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545505464076996  \n",
      "Steps : 6661 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575454942882061  \n",
      "Steps : 6662 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545483112335205  \n",
      "Steps : 6663 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754547193646431  \n",
      "Steps : 6664 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545460760593414  \n",
      "Steps : 6665 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754544958472252  \n",
      "Steps : 6666 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545438408851624  \n",
      "Steps : 6667 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754542723298073  \n",
      "Steps : 6668 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754541605710983  \n",
      "Steps : 6669 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754540488123894  \n",
      "Steps : 6670 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754539370536804  \n",
      "Steps : 6671 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754538252949715  \n",
      "Steps : 6672 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754537135362625  \n",
      "Steps : 6673 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545363903045654  \n",
      "Steps : 6674 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754535645246506  \n",
      "Steps : 6675 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754534900188446  \n",
      "Steps : 6676 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545341551303864  \n",
      "Steps : 6677 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754533410072327  \n",
      "Steps : 6678 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754532665014267  \n",
      "Steps : 6679 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754531919956207  \n",
      "Steps : 6680 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545311748981476  \n",
      "Steps : 6681 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754530429840088  \n",
      "Steps : 6682 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754529684782028  \n",
      "Steps : 6683 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545289397239685  \n",
      "Steps : 6684 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754528194665909  \n",
      "Steps : 6685 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754527449607849  \n",
      "Steps : 6686 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545267045497894  \n",
      "Steps : 6687 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575452595949173  \n",
      "Steps : 6688 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575452521443367  \n",
      "Steps : 6689 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057545244693756104  \n",
      "Steps : 6690 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754523724317551  \n",
      "Steps : 6691 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754522979259491  \n",
      "Steps : 6692 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754522234201431  \n",
      "Steps : 6693 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057545214891433716  \n",
      "Steps : 6694 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754520744085312  \n",
      "Steps : 6695 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754519999027252  \n",
      "Steps : 6696 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545192539691925  \n",
      "Steps : 6697 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754518508911133  \n",
      "Steps : 6698 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754517763853073  \n",
      "Steps : 6699 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545170187950134  \n",
      "Steps : 6700 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754516273736954  \n",
      "Steps : 6701 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754515528678894  \n",
      "Steps : 6702 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545147836208344  \n",
      "Steps : 6703 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754514038562775  \n",
      "Steps : 6704 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754513293504715  \n",
      "Steps : 6705 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754512548446655  \n",
      "Steps : 6706 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545118033885956  \n",
      "Steps : 6707 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754511058330536  \n",
      "Steps : 6708 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754510313272476  \n",
      "Steps : 6709 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545095682144165  \n",
      "Steps : 6710 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754508823156357  \n",
      "Steps : 6711 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754508078098297  \n",
      "Steps : 6712 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545073330402374  \n",
      "Steps : 6713 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754506587982178  \n",
      "Steps : 6714 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754505842924118  \n",
      "Steps : 6715 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545050978660583  \n",
      "Steps : 6716 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754504352807999  \n",
      "Steps : 6717 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754503607749939  \n",
      "Steps : 6718 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754502862691879  \n",
      "Steps : 6719 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545021176338196  \n",
      "Steps : 6720 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575450137257576  \n",
      "Steps : 6721 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057545006275177  \n",
      "Steps : 6722 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544998824596405  \n",
      "Steps : 6723 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754499137401581  \n",
      "Steps : 6724 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754498392343521  \n",
      "Steps : 6725 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544976472854614  \n",
      "Steps : 6726 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754496902227402  \n",
      "Steps : 6727 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754496157169342  \n",
      "Steps : 6728 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544954121112823  \n",
      "Steps : 6729 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754494667053223  \n",
      "Steps : 6730 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754493921995163  \n",
      "Steps : 6731 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754493176937103  \n",
      "Steps : 6732 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544924318790436  \n",
      "Steps : 6733 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754491686820984  \n",
      "Steps : 6734 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754490941762924  \n",
      "Steps : 6735 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544901967048645  \n",
      "Steps : 6736 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754489451646805  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6737 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754488706588745  \n",
      "Steps : 6738 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544879615306854  \n",
      "Steps : 6739 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754487216472626  \n",
      "Steps : 6740 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754486471414566  \n",
      "Steps : 6741 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544857263565063  \n",
      "Steps : 6742 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754484981298447  \n",
      "Steps : 6743 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754484236240387  \n",
      "Steps : 6744 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754483491182327  \n",
      "Steps : 6745 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544827461242676  \n",
      "Steps : 6746 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754482001066208  \n",
      "Steps : 6747 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754481256008148  \n",
      "Steps : 6748 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544805109500885  \n",
      "Steps : 6749 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754479765892029  \n",
      "Steps : 6750 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754479020833969  \n",
      "Steps : 6751 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544782757759094  \n",
      "Steps : 6752 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575447753071785  \n",
      "Steps : 6753 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575447678565979  \n",
      "Steps : 6754 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575447604060173  \n",
      "Steps : 6755 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754475295543671  \n",
      "Steps : 6756 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754474550485611  \n",
      "Steps : 6757 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754473805427551  \n",
      "Steps : 6758 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544730603694916  \n",
      "Steps : 6759 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754472315311432  \n",
      "Steps : 6760 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754471570253372  \n",
      "Steps : 6761 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057544708251953125  \n",
      "Steps : 6762 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754470080137253  \n",
      "Steps : 6763 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754469335079193  \n",
      "Steps : 6764 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057544685900211334  \n",
      "Steps : 6765 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754467844963074  \n",
      "Steps : 6766 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754467099905014  \n",
      "Steps : 6767 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754466354846954  \n",
      "Steps : 6768 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754465609788895  \n",
      "Steps : 6769 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754464864730835  \n",
      "Steps : 6770 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754464119672775  \n",
      "Steps : 6771 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544633746147156  \n",
      "Steps : 6772 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754462629556656  \n",
      "Steps : 6773 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754461884498596  \n",
      "Steps : 6774 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544611394405365  \n",
      "Steps : 6775 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754460394382477  \n",
      "Steps : 6776 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754459649324417  \n",
      "Steps : 6777 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544589042663574  \n",
      "Steps : 6778 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754458159208298  \n",
      "Steps : 6779 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754457414150238  \n",
      "Steps : 6780 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754456669092178  \n",
      "Steps : 6781 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544559240341187  \n",
      "Steps : 6782 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754455178976059  \n",
      "Steps : 6783 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754454433917999  \n",
      "Steps : 6784 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544536888599396  \n",
      "Steps : 6785 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575445294380188  \n",
      "Steps : 6786 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.0575445219874382  \n",
      "Steps : 6787 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544514536857605  \n",
      "Steps : 6788 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754450708627701  \n",
      "Steps : 6789 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754449963569641  \n",
      "Steps : 6790 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.057544492185115814  \n",
      "Steps : 6791 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754448473453522  \n",
      "Steps : 6792 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754447728395462  \n",
      "Steps : 6793 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754446983337402  \n",
      "Steps : 6794 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544462382793427  \n",
      "Steps : 6795 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754445493221283  \n",
      "Steps : 6796 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754444748163223  \n",
      "Steps : 6797 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544440031051636  \n",
      "Steps : 6798 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754443258047104  \n",
      "Steps : 6799 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754442512989044  \n",
      "Steps : 6800 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544417679309845  \n",
      "Steps : 6801 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754441022872925  \n",
      "Steps : 6802 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754440277814865  \n",
      "Steps : 6803 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544395327568054  \n",
      "Steps : 6804 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754438787698746  \n",
      "Steps : 6805 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754438042640686  \n",
      "Steps : 6806 , Loss : 0.7830862402915955, Weights 1.059644341468811, bias : 0.05754437297582626  \n",
      "Steps : 6807 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544365525245667  \n",
      "Steps : 6808 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754435807466507  \n",
      "Steps : 6809 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754435062408447  \n",
      "Steps : 6810 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544343173503876  \n",
      "Steps : 6811 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754433572292328  \n",
      "Steps : 6812 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754432827234268  \n",
      "Steps : 6813 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544320821762085  \n",
      "Steps : 6814 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754431337118149  \n",
      "Steps : 6815 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754430592060089  \n",
      "Steps : 6816 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544298470020294  \n",
      "Steps : 6817 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575442910194397  \n",
      "Steps : 6818 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575442835688591  \n",
      "Steps : 6819 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.0575442761182785  \n",
      "Steps : 6820 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544268667697906  \n",
      "Steps : 6821 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754426121711731  \n",
      "Steps : 6822 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754425376653671  \n",
      "Steps : 6823 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544246315956116  \n",
      "Steps : 6824 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754423886537552  \n",
      "Steps : 6825 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754423141479492  \n",
      "Steps : 6826 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544223964214325  \n",
      "Steps : 6827 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754421651363373  \n",
      "Steps : 6828 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.05754420906305313  \n",
      "Steps : 6829 , Loss : 0.7830862998962402, Weights 1.059644341468811, bias : 0.057544201612472534  \n",
      "Steps : 6830 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754419416189194  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6831 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754418671131134  \n",
      "Steps : 6832 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754417926073074  \n",
      "Steps : 6833 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544171810150146  \n",
      "Steps : 6834 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754416435956955  \n",
      "Steps : 6835 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.05754415690898895  \n",
      "Steps : 6836 , Loss : 0.7830864191055298, Weights 1.059644341468811, bias : 0.057544149458408356  \n",
      "Steps : 6837 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754414200782776  \n",
      "Steps : 6838 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754413455724716  \n",
      "Steps : 6839 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057544127106666565  \n",
      "Steps : 6840 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754411965608597  \n",
      "Steps : 6841 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754411220550537  \n",
      "Steps : 6842 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057544104754924774  \n",
      "Steps : 6843 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754409730434418  \n",
      "Steps : 6844 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754408985376358  \n",
      "Steps : 6845 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754408240318298  \n",
      "Steps : 6846 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057544074952602386  \n",
      "Steps : 6847 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754406750202179  \n",
      "Steps : 6848 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754406005144119  \n",
      "Steps : 6849 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057544052600860596  \n",
      "Steps : 6850 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754404515028  \n",
      "Steps : 6851 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575440376996994  \n",
      "Steps : 6852 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057544030249118805  \n",
      "Steps : 6853 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754402279853821  \n",
      "Steps : 6854 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754401534795761  \n",
      "Steps : 6855 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057544007897377014  \n",
      "Steps : 6856 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754400044679642  \n",
      "Steps : 6857 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754399299621582  \n",
      "Steps : 6858 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754398554563522  \n",
      "Steps : 6859 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057543978095054626  \n",
      "Steps : 6860 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754397064447403  \n",
      "Steps : 6861 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754396319389343  \n",
      "Steps : 6862 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057543955743312836  \n",
      "Steps : 6863 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754394829273224  \n",
      "Steps : 6864 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754394084215164  \n",
      "Steps : 6865 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057543933391571045  \n",
      "Steps : 6866 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754392594099045  \n",
      "Steps : 6867 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754391849040985  \n",
      "Steps : 6868 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543911039829254  \n",
      "Steps : 6869 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754390358924866  \n",
      "Steps : 6870 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754389613866806  \n",
      "Steps : 6871 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754388868808746  \n",
      "Steps : 6872 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543881237506866  \n",
      "Steps : 6873 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754387378692627  \n",
      "Steps : 6874 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754386633634567  \n",
      "Steps : 6875 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543858885765076  \n",
      "Steps : 6876 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754385143518448  \n",
      "Steps : 6877 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754384398460388  \n",
      "Steps : 6878 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543836534023285  \n",
      "Steps : 6879 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754382908344269  \n",
      "Steps : 6880 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754382163286209  \n",
      "Steps : 6881 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543814182281494  \n",
      "Steps : 6882 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575438067317009  \n",
      "Steps : 6883 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575437992811203  \n",
      "Steps : 6884 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575437918305397  \n",
      "Steps : 6885 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543784379959106  \n",
      "Steps : 6886 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754377692937851  \n",
      "Steps : 6887 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754376947879791  \n",
      "Steps : 6888 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543762028217316  \n",
      "Steps : 6889 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754375457763672  \n",
      "Steps : 6890 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754374712705612  \n",
      "Steps : 6891 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543739676475525  \n",
      "Steps : 6892 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754373222589493  \n",
      "Steps : 6893 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754372477531433  \n",
      "Steps : 6894 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543717324733734  \n",
      "Steps : 6895 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754370987415314  \n",
      "Steps : 6896 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754370242357254  \n",
      "Steps : 6897 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754369497299194  \n",
      "Steps : 6898 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543687522411346  \n",
      "Steps : 6899 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754368007183075  \n",
      "Steps : 6900 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754367262125015  \n",
      "Steps : 6901 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543665170669556  \n",
      "Steps : 6902 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754365772008896  \n",
      "Steps : 6903 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754365026950836  \n",
      "Steps : 6904 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543642818927765  \n",
      "Steps : 6905 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754363536834717  \n",
      "Steps : 6906 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754362791776657  \n",
      "Steps : 6907 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543620467185974  \n",
      "Steps : 6908 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754361301660538  \n",
      "Steps : 6909 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754360556602478  \n",
      "Steps : 6910 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754359811544418  \n",
      "Steps : 6911 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543590664863586  \n",
      "Steps : 6912 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754358321428299  \n",
      "Steps : 6913 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754357576370239  \n",
      "Steps : 6914 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543568313121796  \n",
      "Steps : 6915 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575435608625412  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 6916 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575435534119606  \n",
      "Steps : 6917 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543545961380005  \n",
      "Steps : 6918 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754353851079941  \n",
      "Steps : 6919 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754353106021881  \n",
      "Steps : 6920 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543523609638214  \n",
      "Steps : 6921 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754351615905762  \n",
      "Steps : 6922 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754350870847702  \n",
      "Steps : 6923 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754350125789642  \n",
      "Steps : 6924 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543493807315826  \n",
      "Steps : 6925 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754348635673523  \n",
      "Steps : 6926 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754348263144493  \n",
      "Steps : 6927 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754347890615463  \n",
      "Steps : 6928 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543475180864334  \n",
      "Steps : 6929 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543471455574036  \n",
      "Steps : 6930 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754346773028374  \n",
      "Steps : 6931 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754346400499344  \n",
      "Steps : 6932 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754346027970314  \n",
      "Steps : 6933 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754345655441284  \n",
      "Steps : 6934 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754345282912254  \n",
      "Steps : 6935 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543449103832245  \n",
      "Steps : 6936 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543445378541946  \n",
      "Steps : 6937 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754344165325165  \n",
      "Steps : 6938 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754343792796135  \n",
      "Steps : 6939 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754343420267105  \n",
      "Steps : 6940 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754343047738075  \n",
      "Steps : 6941 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543426752090454  \n",
      "Steps : 6942 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543423026800156  \n",
      "Steps : 6943 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754341930150986  \n",
      "Steps : 6944 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754341557621956  \n",
      "Steps : 6945 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754341185092926  \n",
      "Steps : 6946 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754340812563896  \n",
      "Steps : 6947 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754340440034866  \n",
      "Steps : 6948 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543400675058365  \n",
      "Steps : 6949 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543396949768066  \n",
      "Steps : 6950 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754339322447777  \n",
      "Steps : 6951 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754338949918747  \n",
      "Steps : 6952 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754338577389717  \n",
      "Steps : 6953 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754338204860687  \n",
      "Steps : 6954 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543378323316574  \n",
      "Steps : 6955 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543374598026276  \n",
      "Steps : 6956 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754337087273598  \n",
      "Steps : 6957 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754336714744568  \n",
      "Steps : 6958 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754336342215538  \n",
      "Steps : 6959 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754335969686508  \n",
      "Steps : 6960 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754335597157478  \n",
      "Steps : 6961 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543352246284485  \n",
      "Steps : 6962 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543348520994186  \n",
      "Steps : 6963 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754334479570389  \n",
      "Steps : 6964 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754334107041359  \n",
      "Steps : 6965 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754333734512329  \n",
      "Steps : 6966 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754333361983299  \n",
      "Steps : 6967 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543329894542694  \n",
      "Steps : 6968 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543326169252396  \n",
      "Steps : 6969 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575433224439621  \n",
      "Steps : 6970 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575433187186718  \n",
      "Steps : 6971 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575433149933815  \n",
      "Steps : 6972 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575433112680912  \n",
      "Steps : 6973 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.0575433075428009  \n",
      "Steps : 6974 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543303817510605  \n",
      "Steps : 6975 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543300092220306  \n",
      "Steps : 6976 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754329636693001  \n",
      "Steps : 6977 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754329264163971  \n",
      "Steps : 6978 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754328891634941  \n",
      "Steps : 6979 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.05754328519105911  \n",
      "Steps : 6980 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543281465768814  \n",
      "Steps : 6981 , Loss : 0.7830862402915955, Weights 1.0596444606781006, bias : 0.057543277740478516  \n",
      "Steps : 6982 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754327401518822  \n",
      "Steps : 6983 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754327028989792  \n",
      "Steps : 6984 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754326656460762  \n",
      "Steps : 6985 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754326283931732  \n",
      "Steps : 6986 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754325911402702  \n",
      "Steps : 6987 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543255388736725  \n",
      "Steps : 6988 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543251663446426  \n",
      "Steps : 6989 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754324793815613  \n",
      "Steps : 6990 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754324421286583  \n",
      "Steps : 6991 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754324048757553  \n",
      "Steps : 6992 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754323676228523  \n",
      "Steps : 6993 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543233036994934  \n",
      "Steps : 6994 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543229311704636  \n",
      "Steps : 6995 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754322558641434  \n",
      "Steps : 6996 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754322186112404  \n",
      "Steps : 6997 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754321813583374  \n",
      "Steps : 6998 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754321441054344  \n",
      "Steps : 6999 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754321068525314  \n",
      "Steps : 7000 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543206959962845  \n",
      "Steps : 7001 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543203234672546  \n",
      "Steps : 7002 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754319950938225  \n",
      "Steps : 7003 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754319578409195  \n",
      "Steps : 7004 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754319205880165  \n",
      "Steps : 7005 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754318833351135  \n",
      "Steps : 7006 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543184608221054  \n",
      "Steps : 7007 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543180882930756  \n",
      "Steps : 7008 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754317715764046  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7009 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754317343235016  \n",
      "Steps : 7010 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754316970705986  \n",
      "Steps : 7011 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754316598176956  \n",
      "Steps : 7012 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754316225647926  \n",
      "Steps : 7013 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543158531188965  \n",
      "Steps : 7014 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543154805898666  \n",
      "Steps : 7015 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754315108060837  \n",
      "Steps : 7016 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754314735531807  \n",
      "Steps : 7017 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754314363002777  \n",
      "Steps : 7018 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754313990473747  \n",
      "Steps : 7019 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543136179447174  \n",
      "Steps : 7020 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543132454156876  \n",
      "Steps : 7021 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754312872886658  \n",
      "Steps : 7022 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754312500357628  \n",
      "Steps : 7023 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754312127828598  \n",
      "Steps : 7024 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754311755299568  \n",
      "Steps : 7025 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754311382770538  \n",
      "Steps : 7026 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543110102415085  \n",
      "Steps : 7027 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543106377124786  \n",
      "Steps : 7028 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754310265183449  \n",
      "Steps : 7029 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754309892654419  \n",
      "Steps : 7030 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754309520125389  \n",
      "Steps : 7031 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754309147596359  \n",
      "Steps : 7032 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543087750673294  \n",
      "Steps : 7033 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543084025382996  \n",
      "Steps : 7034 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575430803000927  \n",
      "Steps : 7035 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575430765748024  \n",
      "Steps : 7036 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575430728495121  \n",
      "Steps : 7037 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575430691242218  \n",
      "Steps : 7038 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575430653989315  \n",
      "Steps : 7039 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543061673641205  \n",
      "Steps : 7040 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543057948350906  \n",
      "Steps : 7041 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754305422306061  \n",
      "Steps : 7042 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754305049777031  \n",
      "Steps : 7043 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754304677248001  \n",
      "Steps : 7044 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754304304718971  \n",
      "Steps : 7045 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543039321899414  \n",
      "Steps : 7046 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543035596609116  \n",
      "Steps : 7047 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754303187131882  \n",
      "Steps : 7048 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754302814602852  \n",
      "Steps : 7049 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754302442073822  \n",
      "Steps : 7050 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754302069544792  \n",
      "Steps : 7051 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754301697015762  \n",
      "Steps : 7052 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543013244867325  \n",
      "Steps : 7053 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057543009519577026  \n",
      "Steps : 7054 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754300579428673  \n",
      "Steps : 7055 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754300206899643  \n",
      "Steps : 7056 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754299834370613  \n",
      "Steps : 7057 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754299461841583  \n",
      "Steps : 7058 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542990893125534  \n",
      "Steps : 7059 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542987167835236  \n",
      "Steps : 7060 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754298344254494  \n",
      "Steps : 7061 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754297971725464  \n",
      "Steps : 7062 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754297599196434  \n",
      "Steps : 7063 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754297226667404  \n",
      "Steps : 7064 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754296854138374  \n",
      "Steps : 7065 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057542964816093445  \n",
      "Steps : 7066 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057542961090803146  \n",
      "Steps : 7067 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754295736551285  \n",
      "Steps : 7068 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754295364022255  \n",
      "Steps : 7069 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754294991493225  \n",
      "Steps : 7070 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754294618964195  \n",
      "Steps : 7071 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057542942464351654  \n",
      "Steps : 7072 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.057542938739061356  \n",
      "Steps : 7073 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754293501377106  \n",
      "Steps : 7074 , Loss : 0.7830862998962402, Weights 1.0596444606781006, bias : 0.05754293128848076  \n",
      "Steps : 7075 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754292756319046  \n",
      "Steps : 7076 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754292383790016  \n",
      "Steps : 7077 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754292011260986  \n",
      "Steps : 7078 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542916387319565  \n",
      "Steps : 7079 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542912662029266  \n",
      "Steps : 7080 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754290893673897  \n",
      "Steps : 7081 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754290521144867  \n",
      "Steps : 7082 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754290148615837  \n",
      "Steps : 7083 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754289776086807  \n",
      "Steps : 7084 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542894035577774  \n",
      "Steps : 7085 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542890310287476  \n",
      "Steps : 7086 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754288658499718  \n",
      "Steps : 7087 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754288285970688  \n",
      "Steps : 7088 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754287913441658  \n",
      "Steps : 7089 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754287540912628  \n",
      "Steps : 7090 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754287168383598  \n",
      "Steps : 7091 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542867958545685  \n",
      "Steps : 7092 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542864233255386  \n",
      "Steps : 7093 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754286050796509  \n",
      "Steps : 7094 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754285678267479  \n",
      "Steps : 7095 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754285305738449  \n",
      "Steps : 7096 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754284933209419  \n",
      "Steps : 7097 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542845606803894  \n",
      "Steps : 7098 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542841881513596  \n",
      "Steps : 7099 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575428381562233  \n",
      "Steps : 7100 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542834430933  \n",
      "Steps : 7101 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575428307056427  \n",
      "Steps : 7102 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575428269803524  \n",
      "Steps : 7103 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.0575428232550621  \n",
      "Steps : 7104 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542819529771805  \n",
      "Steps : 7105 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542815804481506  \n",
      "Steps : 7106 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754281207919121  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7107 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754280835390091  \n",
      "Steps : 7108 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754280462861061  \n",
      "Steps : 7109 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754280090332031  \n",
      "Steps : 7110 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542797178030014  \n",
      "Steps : 7111 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.057542793452739716  \n",
      "Steps : 7112 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754278972744942  \n",
      "Steps : 7113 , Loss : 0.7830864191055298, Weights 1.0596444606781006, bias : 0.05754278600215912  \n",
      "Steps : 7114 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754278227686882  \n",
      "Steps : 7115 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754277855157852  \n",
      "Steps : 7116 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754277482628822  \n",
      "Steps : 7117 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542771100997925  \n",
      "Steps : 7118 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542767375707626  \n",
      "Steps : 7119 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754276365041733  \n",
      "Steps : 7120 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754275992512703  \n",
      "Steps : 7121 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754275619983673  \n",
      "Steps : 7122 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754275247454643  \n",
      "Steps : 7123 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542748749256134  \n",
      "Steps : 7124 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542745023965836  \n",
      "Steps : 7125 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754274129867554  \n",
      "Steps : 7126 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754273757338524  \n",
      "Steps : 7127 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754273384809494  \n",
      "Steps : 7128 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754273012280464  \n",
      "Steps : 7129 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754272639751434  \n",
      "Steps : 7130 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542722672224045  \n",
      "Steps : 7131 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542718946933746  \n",
      "Steps : 7132 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754271522164345  \n",
      "Steps : 7133 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754271149635315  \n",
      "Steps : 7134 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754270777106285  \n",
      "Steps : 7135 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754270404577255  \n",
      "Steps : 7136 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542700320482254  \n",
      "Steps : 7137 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542696595191956  \n",
      "Steps : 7138 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754269286990166  \n",
      "Steps : 7139 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754268914461136  \n",
      "Steps : 7140 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754268541932106  \n",
      "Steps : 7141 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754268169403076  \n",
      "Steps : 7142 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754267796874046  \n",
      "Steps : 7143 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542674243450165  \n",
      "Steps : 7144 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542670518159866  \n",
      "Steps : 7145 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754266679286957  \n",
      "Steps : 7146 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754266306757927  \n",
      "Steps : 7147 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754265934228897  \n",
      "Steps : 7148 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754265561699867  \n",
      "Steps : 7149 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542651891708374  \n",
      "Steps : 7150 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542648166418076  \n",
      "Steps : 7151 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754264444112778  \n",
      "Steps : 7152 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754264071583748  \n",
      "Steps : 7153 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754263699054718  \n",
      "Steps : 7154 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754263326525688  \n",
      "Steps : 7155 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754262953996658  \n",
      "Steps : 7156 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542625814676285  \n",
      "Steps : 7157 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542622089385986  \n",
      "Steps : 7158 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754261836409569  \n",
      "Steps : 7159 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754261463880539  \n",
      "Steps : 7160 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754261091351509  \n",
      "Steps : 7161 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754260718822479  \n",
      "Steps : 7162 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542603462934494  \n",
      "Steps : 7163 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542599737644196  \n",
      "Steps : 7164 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575425960123539  \n",
      "Steps : 7165 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575425922870636  \n",
      "Steps : 7166 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575425885617733  \n",
      "Steps : 7167 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542584836483  \n",
      "Steps : 7168 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575425811111927  \n",
      "Steps : 7169 , Loss : 0.7830862402915955, Weights 1.0596445798873901, bias : 0.057542577385902405  \n",
      "Steps : 7170 , Loss : 0.7830862402915955, Weights 1.0596445798873901, bias : 0.057542573660612106  \n",
      "Steps : 7171 , Loss : 0.7830862402915955, Weights 1.0596445798873901, bias : 0.05754256993532181  \n",
      "Steps : 7172 , Loss : 0.7830862402915955, Weights 1.0596445798873901, bias : 0.05754256621003151  \n",
      "Steps : 7173 , Loss : 0.7830862402915955, Weights 1.0596445798873901, bias : 0.05754256248474121  \n",
      "Steps : 7174 , Loss : 0.7830862402915955, Weights 1.0596445798873901, bias : 0.05754255875945091  \n",
      "Steps : 7175 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542555034160614  \n",
      "Steps : 7176 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542551308870316  \n",
      "Steps : 7177 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754254758358002  \n",
      "Steps : 7178 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754254385828972  \n",
      "Steps : 7179 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754254013299942  \n",
      "Steps : 7180 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754253640770912  \n",
      "Steps : 7181 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754253268241882  \n",
      "Steps : 7182 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542528957128525  \n",
      "Steps : 7183 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542525231838226  \n",
      "Steps : 7184 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754252150654793  \n",
      "Steps : 7185 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754251778125763  \n",
      "Steps : 7186 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754251405596733  \n",
      "Steps : 7187 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754251033067703  \n",
      "Steps : 7188 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542506605386734  \n",
      "Steps : 7189 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542502880096436  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7190 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754249915480614  \n",
      "Steps : 7191 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754249542951584  \n",
      "Steps : 7192 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754249170422554  \n",
      "Steps : 7193 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754248797893524  \n",
      "Steps : 7194 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754248425364494  \n",
      "Steps : 7195 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542480528354645  \n",
      "Steps : 7196 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542476803064346  \n",
      "Steps : 7197 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754247307777405  \n",
      "Steps : 7198 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754246935248375  \n",
      "Steps : 7199 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754246562719345  \n",
      "Steps : 7200 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754246190190315  \n",
      "Steps : 7201 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542458176612854  \n",
      "Steps : 7202 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542454451322556  \n",
      "Steps : 7203 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754245072603226  \n",
      "Steps : 7204 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754244700074196  \n",
      "Steps : 7205 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754244327545166  \n",
      "Steps : 7206 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754243955016136  \n",
      "Steps : 7207 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754243582487106  \n",
      "Steps : 7208 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542432099580765  \n",
      "Steps : 7209 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542428374290466  \n",
      "Steps : 7210 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754242464900017  \n",
      "Steps : 7211 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754242092370987  \n",
      "Steps : 7212 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754241719841957  \n",
      "Steps : 7213 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754241347312927  \n",
      "Steps : 7214 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542409747838974  \n",
      "Steps : 7215 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542406022548676  \n",
      "Steps : 7216 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754240229725838  \n",
      "Steps : 7217 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754239857196808  \n",
      "Steps : 7218 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754239484667778  \n",
      "Steps : 7219 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754239112138748  \n",
      "Steps : 7220 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754238739609718  \n",
      "Steps : 7221 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542383670806885  \n",
      "Steps : 7222 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542379945516586  \n",
      "Steps : 7223 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754237622022629  \n",
      "Steps : 7224 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754237249493599  \n",
      "Steps : 7225 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754236876964569  \n",
      "Steps : 7226 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754236504435539  \n",
      "Steps : 7227 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542361319065094  \n",
      "Steps : 7228 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542357593774796  \n",
      "Steps : 7229 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575423538684845  \n",
      "Steps : 7230 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575423501431942  \n",
      "Steps : 7231 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575423464179039  \n",
      "Steps : 7232 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575423426926136  \n",
      "Steps : 7233 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575423389673233  \n",
      "Steps : 7234 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542335242033005  \n",
      "Steps : 7235 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542331516742706  \n",
      "Steps : 7236 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754232779145241  \n",
      "Steps : 7237 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754232406616211  \n",
      "Steps : 7238 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754232034087181  \n",
      "Steps : 7239 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754231661558151  \n",
      "Steps : 7240 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542312890291214  \n",
      "Steps : 7241 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542309165000916  \n",
      "Steps : 7242 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754230543971062  \n",
      "Steps : 7243 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754230171442032  \n",
      "Steps : 7244 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754229798913002  \n",
      "Steps : 7245 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754229426383972  \n",
      "Steps : 7246 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754229053854942  \n",
      "Steps : 7247 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542286813259125  \n",
      "Steps : 7248 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542283087968826  \n",
      "Steps : 7249 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754227936267853  \n",
      "Steps : 7250 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754227563738823  \n",
      "Steps : 7251 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754227191209793  \n",
      "Steps : 7252 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754226818680763  \n",
      "Steps : 7253 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542264461517334  \n",
      "Steps : 7254 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542260736227036  \n",
      "Steps : 7255 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754225701093674  \n",
      "Steps : 7256 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754225328564644  \n",
      "Steps : 7257 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754224956035614  \n",
      "Steps : 7258 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754224583506584  \n",
      "Steps : 7259 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754224210977554  \n",
      "Steps : 7260 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542238384485245  \n",
      "Steps : 7261 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542234659194946  \n",
      "Steps : 7262 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754223093390465  \n",
      "Steps : 7263 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754222720861435  \n",
      "Steps : 7264 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754222348332405  \n",
      "Steps : 7265 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754221975803375  \n",
      "Steps : 7266 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542216032743454  \n",
      "Steps : 7267 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542212307453156  \n",
      "Steps : 7268 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754220858216286  \n",
      "Steps : 7269 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754220485687256  \n",
      "Steps : 7270 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754220113158226  \n",
      "Steps : 7271 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754219740629196  \n",
      "Steps : 7272 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754219368100166  \n",
      "Steps : 7273 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542189955711365  \n",
      "Steps : 7274 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542186230421066  \n",
      "Steps : 7275 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754218250513077  \n",
      "Steps : 7276 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754217877984047  \n",
      "Steps : 7277 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754217505455017  \n",
      "Steps : 7278 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754217132925987  \n",
      "Steps : 7279 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542167603969574  \n",
      "Steps : 7280 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542163878679276  \n",
      "Steps : 7281 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754216015338898  \n",
      "Steps : 7282 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754215642809868  \n",
      "Steps : 7283 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754215270280838  \n",
      "Steps : 7284 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754214897751808  \n",
      "Steps : 7285 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754214525222778  \n",
      "Steps : 7286 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542141526937485  \n",
      "Steps : 7287 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542137801647186  \n",
      "Steps : 7288 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754213407635689  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7289 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754213035106659  \n",
      "Steps : 7290 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754212662577629  \n",
      "Steps : 7291 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754212290048599  \n",
      "Steps : 7292 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542119175195694  \n",
      "Steps : 7293 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057542115449905396  \n",
      "Steps : 7294 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575421117246151  \n",
      "Steps : 7295 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575421079993248  \n",
      "Steps : 7296 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575421042740345  \n",
      "Steps : 7297 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575421005487442  \n",
      "Steps : 7298 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575420968234539  \n",
      "Steps : 7299 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542093098163605  \n",
      "Steps : 7300 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542089372873306  \n",
      "Steps : 7301 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754208564758301  \n",
      "Steps : 7302 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754208192229271  \n",
      "Steps : 7303 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754207819700241  \n",
      "Steps : 7304 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754207447171211  \n",
      "Steps : 7305 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542070746421814  \n",
      "Steps : 7306 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542067021131516  \n",
      "Steps : 7307 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754206329584122  \n",
      "Steps : 7308 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754205957055092  \n",
      "Steps : 7309 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754205584526062  \n",
      "Steps : 7310 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754205211997032  \n",
      "Steps : 7311 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754204839468002  \n",
      "Steps : 7312 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542044669389725  \n",
      "Steps : 7313 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542040944099426  \n",
      "Steps : 7314 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754203721880913  \n",
      "Steps : 7315 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754203349351883  \n",
      "Steps : 7316 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754202976822853  \n",
      "Steps : 7317 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754202604293823  \n",
      "Steps : 7318 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542022317647934  \n",
      "Steps : 7319 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057542018592357635  \n",
      "Steps : 7320 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754201486706734  \n",
      "Steps : 7321 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754201114177704  \n",
      "Steps : 7322 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754200741648674  \n",
      "Steps : 7323 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754200369119644  \n",
      "Steps : 7324 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754199996590614  \n",
      "Steps : 7325 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541996240615845  \n",
      "Steps : 7326 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541992515325546  \n",
      "Steps : 7327 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754198879003525  \n",
      "Steps : 7328 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754198506474495  \n",
      "Steps : 7329 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754198133945465  \n",
      "Steps : 7330 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754197761416435  \n",
      "Steps : 7331 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541973888874054  \n",
      "Steps : 7332 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541970163583755  \n",
      "Steps : 7333 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754196643829346  \n",
      "Steps : 7334 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754196271300316  \n",
      "Steps : 7335 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754195898771286  \n",
      "Steps : 7336 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754195526242256  \n",
      "Steps : 7337 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754195153713226  \n",
      "Steps : 7338 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541947811841965  \n",
      "Steps : 7339 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541944086551666  \n",
      "Steps : 7340 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754194036126137  \n",
      "Steps : 7341 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754193663597107  \n",
      "Steps : 7342 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754193291068077  \n",
      "Steps : 7343 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754192918539047  \n",
      "Steps : 7344 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541925460100174  \n",
      "Steps : 7345 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541921734809875  \n",
      "Steps : 7346 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754191800951958  \n",
      "Steps : 7347 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754191428422928  \n",
      "Steps : 7348 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754191055893898  \n",
      "Steps : 7349 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754190683364868  \n",
      "Steps : 7350 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754190310835838  \n",
      "Steps : 7351 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541899383068085  \n",
      "Steps : 7352 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541895657777786  \n",
      "Steps : 7353 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754189193248749  \n",
      "Steps : 7354 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754188820719719  \n",
      "Steps : 7355 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754188448190689  \n",
      "Steps : 7356 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754188075661659  \n",
      "Steps : 7357 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541877031326294  \n",
      "Steps : 7358 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541873306035995  \n",
      "Steps : 7359 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575418695807457  \n",
      "Steps : 7360 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575418658554554  \n",
      "Steps : 7361 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.0575418621301651  \n",
      "Steps : 7362 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575418584048748  \n",
      "Steps : 7363 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.0575418546795845  \n",
      "Steps : 7364 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541850954294205  \n",
      "Steps : 7365 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541847229003906  \n",
      "Steps : 7366 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754184350371361  \n",
      "Steps : 7367 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754183977842331  \n",
      "Steps : 7368 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754183605313301  \n",
      "Steps : 7369 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754183232784271  \n",
      "Steps : 7370 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541828602552414  \n",
      "Steps : 7371 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.057541824877262115  \n",
      "Steps : 7372 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754182115197182  \n",
      "Steps : 7373 , Loss : 0.7830862998962402, Weights 1.0596445798873901, bias : 0.05754181742668152  \n",
      "Steps : 7374 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754181370139122  \n",
      "Steps : 7375 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754180997610092  \n",
      "Steps : 7376 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754180625081062  \n",
      "Steps : 7377 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541802525520325  \n",
      "Steps : 7378 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.057541798800230026  \n",
      "Steps : 7379 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754179507493973  \n",
      "Steps : 7380 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754179134964943  \n",
      "Steps : 7381 , Loss : 0.7830864191055298, Weights 1.0596445798873901, bias : 0.05754178762435913  \n",
      "Steps : 7382 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754178389906883  \n",
      "Steps : 7383 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.057541780173778534  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7384 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.057541776448488235  \n",
      "Steps : 7385 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754177272319794  \n",
      "Steps : 7386 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754176899790764  \n",
      "Steps : 7387 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754176527261734  \n",
      "Steps : 7388 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754176154732704  \n",
      "Steps : 7389 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754175782203674  \n",
      "Steps : 7390 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.057541754096746445  \n",
      "Steps : 7391 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.057541750371456146  \n",
      "Steps : 7392 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754174664616585  \n",
      "Steps : 7393 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754174292087555  \n",
      "Steps : 7394 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754173919558525  \n",
      "Steps : 7395 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754173547029495  \n",
      "Steps : 7396 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.057541731745004654  \n",
      "Steps : 7397 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.057541728019714355  \n",
      "Steps : 7398 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754172429442406  \n",
      "Steps : 7399 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754172056913376  \n",
      "Steps : 7400 , Loss : 0.7830864191055298, Weights 1.0596446990966797, bias : 0.05754171684384346  \n",
      "Steps : 7401 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754171311855316  \n",
      "Steps : 7402 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754170939326286  \n",
      "Steps : 7403 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.057541705667972565  \n",
      "Steps : 7404 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.057541701942682266  \n",
      "Steps : 7405 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754169821739197  \n",
      "Steps : 7406 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754169449210167  \n",
      "Steps : 7407 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754169076681137  \n",
      "Steps : 7408 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754168704152107  \n",
      "Steps : 7409 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.057541683316230774  \n",
      "Steps : 7410 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.057541679590940475  \n",
      "Steps : 7411 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754167586565018  \n",
      "Steps : 7412 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754167214035988  \n",
      "Steps : 7413 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754166841506958  \n",
      "Steps : 7414 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754166468977928  \n",
      "Steps : 7415 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754166096448898  \n",
      "Steps : 7416 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.057541657239198685  \n",
      "Steps : 7417 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.057541653513908386  \n",
      "Steps : 7418 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754164978861809  \n",
      "Steps : 7419 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754164606332779  \n",
      "Steps : 7420 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754164233803749  \n",
      "Steps : 7421 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7422 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7423 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7424 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7425 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7426 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7427 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7428 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7429 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7430 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7431 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7432 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7433 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7434 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7435 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7436 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7437 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7438 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7439 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7440 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7441 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7442 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7443 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7444 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7445 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7446 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7447 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7448 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7449 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7450 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7451 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7452 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7453 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7454 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7455 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7456 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7457 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7458 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7459 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7460 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7461 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7462 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7463 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7464 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7465 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7466 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7467 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7468 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7469 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7470 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7471 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7472 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7473 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7474 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7475 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7476 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7477 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7478 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7479 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7480 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7481 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7482 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7483 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7484 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7485 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7486 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7487 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7488 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7489 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7490 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7491 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7492 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7493 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7494 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7495 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7496 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7497 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7498 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7499 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7500 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7501 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7502 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7503 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7504 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7505 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7506 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7507 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7508 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7509 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7510 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7511 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7512 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7513 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7514 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7515 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7516 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7517 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7518 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7519 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7520 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7521 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7522 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7523 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7524 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7525 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7526 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7527 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7528 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7529 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7530 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7531 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7532 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7533 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7534 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7535 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7536 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7537 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7538 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7539 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7540 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7541 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7542 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7543 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7544 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7545 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7546 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7547 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7548 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7549 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7550 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7551 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7552 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7553 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7554 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7555 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7556 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7557 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7558 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7559 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7560 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7561 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7562 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7563 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7564 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7565 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7566 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7567 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7568 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7569 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7570 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7571 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7572 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7573 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7574 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7575 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7576 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7577 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7578 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7579 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7580 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7581 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7582 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7583 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7584 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7585 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7586 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7587 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7588 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7589 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7590 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7591 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7592 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7593 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7594 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7595 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7596 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7597 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7598 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7599 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7600 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7601 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7602 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7603 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7604 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7605 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7606 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7607 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7608 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7609 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7610 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7611 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7612 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7613 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7614 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7615 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7616 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7617 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7618 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7619 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7620 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7621 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7622 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7623 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7624 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7625 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7626 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7627 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7628 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7629 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7630 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7631 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7632 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7633 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7634 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7635 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7636 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7637 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7638 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7639 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7640 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7641 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7642 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7643 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7644 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7645 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7646 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7647 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7648 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7649 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7650 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7651 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7652 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7653 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7654 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7655 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7656 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7657 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7658 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7659 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7660 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7661 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7662 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7663 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7664 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7665 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7666 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7667 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7668 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7669 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7670 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7671 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7672 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7673 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7674 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7675 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7676 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7677 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7678 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7679 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7680 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7681 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7682 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7683 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7684 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7685 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7686 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7687 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7688 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7689 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7690 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7691 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7692 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7693 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7694 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7695 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7696 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7697 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7698 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7699 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7700 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7701 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7702 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7703 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7704 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7705 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7706 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7707 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7708 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7709 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7710 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7711 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7712 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7713 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7714 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7715 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7716 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7717 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7718 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7719 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7720 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7721 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7722 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7723 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7724 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7725 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7726 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7727 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7728 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7729 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7730 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7731 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7732 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7733 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7734 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7735 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7736 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7737 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7738 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7739 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7740 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7741 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7742 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7743 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7744 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7745 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7746 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7747 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7748 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7749 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7750 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7751 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7752 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7753 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7754 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7755 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7756 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7757 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7758 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7759 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7760 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7761 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7762 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7763 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7764 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7765 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7766 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7767 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7768 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7769 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7770 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7771 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7772 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7773 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7774 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7775 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7776 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7777 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7778 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7779 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7780 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7781 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7782 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7783 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7784 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7785 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7786 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7787 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7788 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7789 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7790 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7791 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7792 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7793 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7794 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7795 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7796 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7797 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7798 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7799 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7800 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7801 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7802 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7803 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7804 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7805 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7806 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7807 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7808 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7809 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7810 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7811 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7812 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7813 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7814 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7815 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7816 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7817 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7818 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7819 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7820 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7821 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7822 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7823 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7824 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7825 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7826 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7827 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7828 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7829 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7830 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7831 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7832 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7833 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7834 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7835 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7836 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7837 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7838 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7839 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7840 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7841 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7842 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7843 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7844 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7845 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7846 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7847 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7848 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7849 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7850 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7851 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7852 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7853 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7854 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7855 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7856 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7857 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7858 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7859 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7860 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7861 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7862 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7863 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7864 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7865 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7866 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7867 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7868 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7869 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7870 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7871 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7872 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7873 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7874 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7875 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7876 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7877 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7878 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7879 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7880 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7881 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7882 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7883 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7884 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7885 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7886 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7887 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7888 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7889 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7890 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7891 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7892 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7893 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7894 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7895 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7896 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7897 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7898 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7899 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7900 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7901 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7902 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7903 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7904 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7905 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7906 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7907 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7908 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7909 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7910 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7911 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7912 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7913 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7914 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7915 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7916 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7917 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7918 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7919 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7920 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7921 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7922 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7923 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7924 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7925 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7926 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7927 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7928 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7929 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7930 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7931 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7932 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7933 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7934 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7935 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7936 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7937 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7938 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7939 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7940 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7941 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7942 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7943 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7944 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7945 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7946 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7947 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7948 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7949 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7950 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7951 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7952 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7953 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7954 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7955 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7956 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7957 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7958 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7959 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7960 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7961 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7962 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7963 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7964 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7965 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7966 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7967 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7968 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7969 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7970 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7971 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7972 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7973 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7974 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7975 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7976 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7977 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7978 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7979 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7980 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7981 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7982 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7983 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7984 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7985 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 7986 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7987 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7988 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7989 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7990 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7991 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7992 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7993 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7994 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7995 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7996 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7997 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7998 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 7999 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8000 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8001 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8002 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8003 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8004 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8005 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8006 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8007 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8008 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8009 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8010 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8011 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8012 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8013 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8014 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8015 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8016 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8017 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8018 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8019 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8020 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8021 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8022 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8023 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8024 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8025 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8026 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8027 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8028 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8029 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8030 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8031 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8032 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8033 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8034 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8035 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8036 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8037 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8038 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8039 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8040 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8041 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8042 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8043 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8044 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8045 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8046 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8047 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8048 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8049 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8050 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8051 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8052 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8053 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8054 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8055 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8056 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8057 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8058 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8059 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8060 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8061 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8062 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8063 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8064 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8065 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8066 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8067 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8068 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8069 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8070 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8071 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8072 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8073 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8074 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8075 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8076 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8077 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8078 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8079 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8080 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8081 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8082 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8083 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8084 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8085 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8086 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8087 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8088 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8089 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8090 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8091 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8092 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8093 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8094 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8095 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8096 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8097 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8098 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8099 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8100 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8101 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8102 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8103 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8104 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8105 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8106 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8107 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8108 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8109 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8110 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8111 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8112 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8113 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8114 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8115 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8116 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8117 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8118 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8119 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8120 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8121 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8122 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8123 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8124 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8125 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8126 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8127 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8128 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8129 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8130 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8131 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8132 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8133 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8134 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8135 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8136 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8137 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8138 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8139 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8140 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8141 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8142 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8143 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8144 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8145 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8146 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8147 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8148 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8149 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8150 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8151 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8152 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8153 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8154 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8155 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8156 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8157 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8158 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8159 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8160 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8161 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8162 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8163 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8164 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8165 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8166 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8167 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8168 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8169 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8170 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8171 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8172 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8173 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8174 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8175 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8176 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8177 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8178 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8179 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8180 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8181 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8182 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8183 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8184 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8185 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8186 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8187 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8188 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8189 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8190 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8191 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8192 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8193 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8194 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8195 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8196 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8197 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8198 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8199 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8200 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8201 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8202 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8203 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8204 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8205 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8206 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8207 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8208 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8209 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8210 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8211 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8212 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8213 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8214 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8215 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8216 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8217 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8218 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8219 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8220 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8221 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8222 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8223 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8224 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8225 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8226 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8227 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8228 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8229 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8230 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8231 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8232 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8233 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8234 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8235 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8236 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8237 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8238 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8239 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8240 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8241 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8242 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8243 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8244 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8245 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8246 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8247 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8248 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8249 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8250 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8251 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8252 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8253 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8254 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8255 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8256 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8257 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8258 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8259 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8260 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8261 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8262 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8263 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8264 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8265 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8266 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8267 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8268 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8269 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8270 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8271 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8272 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8273 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8274 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8275 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8276 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8277 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8278 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8279 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8280 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8281 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8282 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8283 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8284 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8285 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8286 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8287 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8288 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8289 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8290 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8291 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8292 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8293 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8294 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8295 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8296 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8297 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8298 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8299 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8300 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8301 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8302 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8303 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8304 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8305 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8306 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8307 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8308 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8309 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8310 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8311 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8312 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8313 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8314 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8315 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8316 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8317 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8318 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8319 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8320 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8321 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8322 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8323 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8324 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8325 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8326 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8327 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8328 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8329 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8330 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8331 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8332 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8333 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8334 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8335 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8336 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8337 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8338 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8339 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8340 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8341 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8342 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8343 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8344 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8345 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8346 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8347 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8348 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8349 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8350 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8351 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8352 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8353 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8354 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8355 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8356 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8357 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8358 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8359 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8360 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8361 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8362 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8363 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8364 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8365 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8366 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8367 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8368 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8369 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8370 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8371 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8372 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8373 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8374 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8375 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8376 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8377 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8378 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8379 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8380 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8381 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8382 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8383 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8384 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8385 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8386 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8387 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8388 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8389 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8390 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8391 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8392 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8393 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8394 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8395 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8396 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8397 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8398 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8399 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8400 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8401 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8402 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8403 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8404 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8405 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8406 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8407 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8408 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8409 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8410 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8411 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8412 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8413 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8414 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8415 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8416 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8417 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8418 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8419 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8420 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8421 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8422 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8423 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8424 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8425 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8426 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8427 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8428 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8429 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8430 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8431 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8432 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8433 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8434 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8435 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8436 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8437 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8438 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8439 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8440 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8441 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8442 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8443 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8444 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8445 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8446 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8447 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8448 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8449 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8450 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8451 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8452 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8453 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8454 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8455 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8456 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8457 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8458 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8459 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8460 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8461 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8462 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8463 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8464 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8465 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8466 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8467 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8468 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8469 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8470 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8471 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8472 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8473 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8474 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8475 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8476 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8477 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8478 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8479 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8480 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8481 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8482 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8483 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8484 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8485 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8486 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8487 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8488 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8489 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8490 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8491 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8492 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8493 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8494 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8495 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8496 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8497 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8498 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8499 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8500 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8501 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8502 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8503 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8504 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8505 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8506 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8507 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8508 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8509 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8510 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8511 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8512 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8513 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8514 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8515 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8516 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8517 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8518 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8519 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8520 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8521 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8522 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8523 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8524 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8525 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8526 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8527 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8528 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8529 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8530 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8531 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8532 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8533 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8534 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8535 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8536 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8537 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8538 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8539 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8540 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8541 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8542 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8543 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8544 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8545 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8546 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8547 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8548 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8549 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8550 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8551 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8552 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8553 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8554 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8555 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8556 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8557 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8558 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8559 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8560 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8561 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8562 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8563 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8564 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8565 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8566 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8567 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8568 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8569 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8570 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8571 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8572 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8573 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8574 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8575 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8576 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8577 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8578 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8579 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8580 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8581 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8582 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8583 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8584 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8585 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8586 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8587 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8588 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8589 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8590 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8591 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8592 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8593 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8594 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8595 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8596 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8597 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8598 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8599 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8600 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8601 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8602 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8603 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8604 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8605 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8606 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8607 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8608 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8609 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8610 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8611 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8612 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8613 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8614 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8615 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8616 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8617 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8618 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8619 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8620 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8621 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8622 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8623 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8624 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8625 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8626 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8627 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8628 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8629 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8630 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8631 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8632 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8633 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8634 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8635 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8636 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8637 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8638 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8639 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8640 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8641 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8642 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8643 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8644 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8645 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8646 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8647 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8648 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8649 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8650 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8651 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8652 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8653 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8654 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8655 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8656 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8657 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8658 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8659 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8660 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8661 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8662 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8663 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8664 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8665 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8666 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8667 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8668 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8669 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8670 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8671 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8672 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8673 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8674 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8675 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8676 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8677 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8678 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8679 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8680 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8681 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8682 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8683 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8684 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8685 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8686 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8687 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8688 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8689 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8690 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8691 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8692 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8693 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8694 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8695 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8696 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8697 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8698 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8699 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8700 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8701 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8702 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8703 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8704 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8705 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8706 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8707 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8708 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8709 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8710 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8711 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8712 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8713 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8714 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8715 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8716 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8717 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8718 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8719 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8720 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8721 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8722 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8723 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8724 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8725 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8726 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8727 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8728 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8729 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8730 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8731 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8732 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8733 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8734 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8735 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8736 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8737 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8738 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8739 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8740 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8741 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8742 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8743 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8744 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8745 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8746 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8747 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8748 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8749 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8750 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8751 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8752 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8753 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8754 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8755 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8756 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8757 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8758 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8759 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8760 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8761 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8762 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8763 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8764 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8765 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8766 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8767 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8768 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8769 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8770 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8771 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8772 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8773 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8774 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8775 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8776 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8777 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8778 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8779 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8780 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8781 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8782 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8783 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8784 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8785 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8786 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8787 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8788 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8789 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8790 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8791 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8792 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8793 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8794 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8795 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8796 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8797 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8798 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8799 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8800 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8801 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8802 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8803 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8804 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8805 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8806 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8807 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8808 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8809 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8810 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8811 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8812 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8813 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8814 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8815 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8816 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8817 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8818 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8819 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8820 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8821 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8822 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8823 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8824 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8825 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8826 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8827 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8828 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8829 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8830 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8831 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8832 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8833 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8834 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8835 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8836 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8837 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8838 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8839 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8840 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8841 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8842 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8843 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8844 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8845 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8846 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8847 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8848 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8849 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8850 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8851 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8852 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8853 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8854 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8855 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8856 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8857 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8858 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8859 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8860 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8861 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8862 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8863 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8864 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8865 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8866 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8867 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8868 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8869 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8870 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8871 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8872 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8873 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8874 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8875 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8876 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8877 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8878 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8879 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8880 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8881 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8882 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8883 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8884 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8885 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8886 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8887 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8888 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8889 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8890 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8891 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8892 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8893 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8894 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8895 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8896 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8897 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8898 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8899 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8900 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8901 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8902 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8903 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8904 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8905 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8906 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8907 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8908 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8909 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8910 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8911 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8912 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8913 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8914 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8915 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8916 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8917 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8918 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8919 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8920 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8921 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8922 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8923 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8924 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8925 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8926 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8927 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8928 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8929 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8930 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8931 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8932 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8933 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8934 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8935 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8936 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8937 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8938 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8939 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8940 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8941 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8942 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8943 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8944 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8945 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8946 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8947 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8948 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8949 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8950 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8951 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8952 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8953 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8954 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8955 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8956 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8957 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8958 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8959 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8960 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8961 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8962 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8963 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8964 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8965 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8966 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8967 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8968 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8969 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8970 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8971 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8972 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8973 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8974 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 8975 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8976 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8977 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8978 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8979 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8980 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8981 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8982 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8983 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8984 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8985 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8986 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8987 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8988 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8989 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8990 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8991 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8992 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8993 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8994 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8995 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8996 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8997 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8998 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 8999 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9000 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9001 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9002 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9003 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9004 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9005 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9006 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9007 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9008 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9009 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9010 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9011 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9012 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9013 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9014 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9015 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9016 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9017 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9018 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9019 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9020 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9021 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9022 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9023 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9024 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9025 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9026 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9027 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9028 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9029 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9030 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9031 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9032 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9033 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9034 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9035 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9036 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9037 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9038 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9039 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9040 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9041 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9042 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9043 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9044 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9045 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9046 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9047 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9048 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9049 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9050 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9051 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9052 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9053 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9054 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9055 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9056 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9057 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9058 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9059 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9060 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9061 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9062 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9063 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9064 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9065 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9066 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9067 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9068 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9069 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9070 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9071 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9072 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9073 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9074 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9075 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9076 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9077 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9078 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9079 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9080 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9081 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9082 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9083 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9084 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9085 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9086 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9087 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9088 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9089 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9090 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9091 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9092 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9093 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9094 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9095 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9096 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9097 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9098 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9099 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9100 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9101 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9102 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9103 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9104 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9105 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9106 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9107 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9108 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9109 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9110 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9111 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9112 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9113 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9114 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9115 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9116 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9117 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9118 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9119 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9120 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9121 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9122 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9123 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9124 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9125 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9126 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9127 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9128 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9129 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9130 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9131 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9132 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9133 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9134 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9135 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9136 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9137 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9138 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9139 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9140 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9141 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9142 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9143 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9144 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9145 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9146 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9147 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9148 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9149 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9150 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9151 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9152 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9153 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9154 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9155 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9156 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9157 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9158 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9159 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9160 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9161 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9162 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9163 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9164 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9165 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9166 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9167 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9168 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9169 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9170 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9171 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9172 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9173 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9174 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9175 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9176 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9177 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9178 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9179 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9180 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9181 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9182 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9183 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9184 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9185 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9186 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9187 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9188 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9189 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9190 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9191 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9192 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9193 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9194 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9195 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9196 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9197 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9198 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9199 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9200 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9201 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9202 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9203 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9204 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9205 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9206 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9207 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9208 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9209 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9210 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9211 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9212 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9213 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9214 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9215 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9216 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9217 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9218 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9219 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9220 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9221 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9222 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9223 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9224 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9225 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9226 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9227 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9228 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9229 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9230 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9231 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9232 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9233 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9234 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9235 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9236 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9237 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9238 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9239 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9240 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9241 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9242 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9243 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9244 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9245 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9246 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9247 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9248 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9249 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9250 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9251 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9252 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9253 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9254 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9255 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9256 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9257 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9258 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9259 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9260 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9261 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9262 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9263 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9264 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9265 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9266 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9267 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9268 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9269 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9270 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9271 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9272 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9273 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9274 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9275 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9276 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9277 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9278 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9279 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9280 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9281 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9282 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9283 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9284 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9285 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9286 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9287 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9288 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9289 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9290 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9291 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9292 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9293 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9294 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9295 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9296 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9297 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9298 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9299 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9300 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9301 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9302 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9303 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9304 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9305 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9306 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9307 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9308 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9309 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9310 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9311 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9312 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9313 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9314 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9315 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9316 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9317 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9318 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9319 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9320 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9321 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9322 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9323 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9324 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9325 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9326 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9327 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9328 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9329 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9330 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9331 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9332 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9333 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9334 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9335 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9336 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9337 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9338 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9339 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9340 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9341 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9342 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9343 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9344 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9345 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9346 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9347 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9348 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9349 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9350 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9351 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9352 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9353 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9354 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9355 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9356 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9357 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9358 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9359 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9360 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9361 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9362 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9363 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9364 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9365 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9366 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9367 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9368 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9369 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9370 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9371 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9372 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9373 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9374 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9375 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9376 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9377 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9378 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9379 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9380 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9381 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9382 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9383 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9384 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9385 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9386 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9387 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9388 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9389 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9390 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9391 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9392 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9393 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9394 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9395 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9396 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9397 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9398 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9399 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9400 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9401 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9402 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9403 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9404 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9405 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9406 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9407 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9408 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9409 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9410 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9411 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9412 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9413 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9414 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9415 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9416 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9417 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9418 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9419 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9420 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9421 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9422 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9423 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9424 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9425 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9426 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9427 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9428 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9429 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9430 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9431 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9432 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9433 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9434 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9435 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9436 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9437 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9438 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9439 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9440 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9441 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9442 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9443 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9444 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9445 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9446 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9447 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9448 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9449 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9450 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9451 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9452 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9453 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9454 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9455 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9456 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9457 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9458 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9459 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9460 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9461 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9462 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9463 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9464 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9465 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9466 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9467 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9468 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9469 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9470 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9471 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9472 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9473 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9474 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9475 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9476 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9477 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9478 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9479 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9480 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9481 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9482 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9483 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9484 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9485 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9486 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9487 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9488 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9489 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9490 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9491 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9492 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9493 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9494 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9495 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9496 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9497 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9498 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9499 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9500 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9501 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9502 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9503 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9504 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9505 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9506 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9507 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9508 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9509 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9510 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9511 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9512 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9513 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9514 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9515 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9516 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9517 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9518 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9519 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9520 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9521 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9522 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9523 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9524 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9525 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9526 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9527 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9528 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9529 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9530 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9531 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9532 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9533 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9534 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9535 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9536 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9537 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9538 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9539 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9540 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9541 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9542 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9543 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9544 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9545 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9546 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9547 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9548 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9549 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9550 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9551 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9552 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9553 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9554 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9555 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9556 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9557 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9558 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9559 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9560 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9561 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9562 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9563 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9564 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9565 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9566 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9567 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9568 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9569 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9570 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9571 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9572 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9573 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9574 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9575 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9576 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9577 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9578 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9579 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9580 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9581 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9582 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9583 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9584 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9585 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9586 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9587 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9588 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9589 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9590 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9591 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9592 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9593 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9594 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9595 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9596 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9597 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9598 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9599 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9600 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9601 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9602 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9603 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9604 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9605 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9606 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9607 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9608 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9609 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9610 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9611 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9612 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9613 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9614 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9615 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9616 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9617 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9618 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9619 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9620 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9621 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9622 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9623 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9624 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9625 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9626 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9627 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9628 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9629 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9630 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9631 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9632 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9633 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9634 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9635 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9636 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9637 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9638 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9639 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9640 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9641 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9642 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9643 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9644 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9645 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9646 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9647 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9648 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9649 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9650 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9651 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9652 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9653 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9654 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9655 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9656 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9657 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9658 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9659 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9660 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9661 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9662 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9663 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9664 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9665 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9666 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9667 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9668 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9669 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9670 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9671 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9672 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9673 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9674 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9675 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9676 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9677 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9678 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9679 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9680 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9681 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9682 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9683 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9684 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9685 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9686 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9687 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9688 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9689 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9690 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9691 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9692 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9693 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9694 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9695 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9696 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9697 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9698 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9699 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9700 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9701 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9702 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9703 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9704 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9705 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9706 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9707 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9708 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9709 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9710 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9711 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9712 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9713 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9714 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9715 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9716 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9717 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9718 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9719 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9720 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9721 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9722 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9723 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9724 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9725 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9726 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9727 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9728 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9729 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9730 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9731 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9732 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9733 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9734 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9735 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9736 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9737 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9738 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9739 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9740 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9741 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9742 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9743 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9744 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9745 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9746 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9747 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9748 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9749 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9750 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9751 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9752 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9753 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9754 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9755 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9756 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9757 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9758 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9759 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9760 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9761 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9762 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9763 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9764 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9765 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9766 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9767 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9768 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9769 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9770 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9771 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9772 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9773 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9774 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9775 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9776 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9777 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9778 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9779 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9780 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9781 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9782 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9783 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9784 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9785 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9786 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9787 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9788 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9789 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9790 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9791 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9792 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9793 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9794 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9795 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9796 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9797 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9798 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9799 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9800 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9801 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9802 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9803 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9804 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9805 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9806 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9807 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9808 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9809 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9810 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9811 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9812 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9813 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9814 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9815 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9816 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9817 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9818 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9819 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9820 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9821 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9822 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9823 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9824 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9825 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9826 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9827 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9828 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9829 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9830 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9831 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9832 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9833 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9834 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9835 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9836 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9837 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9838 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9839 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9840 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9841 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9842 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9843 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9844 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9845 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9846 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9847 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9848 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9849 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9850 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9851 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9852 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9853 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9854 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9855 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9856 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9857 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9858 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9859 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9860 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9861 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9862 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9863 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9864 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9865 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9866 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9867 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9868 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9869 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9870 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9871 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9872 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9873 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9874 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9875 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9876 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9877 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9878 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9879 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9880 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9881 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9882 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9883 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9884 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9885 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9886 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9887 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9888 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9889 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9890 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9891 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9892 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9893 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9894 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9895 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9896 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9897 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9898 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9899 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9900 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9901 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9902 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9903 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9904 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9905 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9906 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9907 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9908 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9909 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9910 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9911 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9912 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9913 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9914 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9915 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9916 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9917 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9918 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9919 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9920 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9921 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9922 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9923 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9924 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9925 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9926 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9927 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9928 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9929 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9930 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9931 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9932 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9933 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9934 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9935 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9936 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9937 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9938 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9939 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9940 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9941 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9942 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9943 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9944 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9945 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9946 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9947 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9948 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9949 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9950 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9951 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9952 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9953 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9954 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9955 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9956 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9957 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9958 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9959 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9960 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9961 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9962 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9963 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9964 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9965 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9966 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9967 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9968 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9969 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9970 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9971 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9972 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9973 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9974 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9975 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9976 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9977 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9978 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9979 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9980 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9981 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9982 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9983 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9984 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9985 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9986 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9987 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 9988 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9989 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9990 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9991 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9992 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9993 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9994 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9995 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9996 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9997 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9998 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n",
      "Steps : 9999 , Loss : 0.7830862998962402, Weights 1.0596446990966797, bias : 0.05754163861274719  \n"
     ]
    }
   ],
   "source": [
    "for i in range(steps):\n",
    "    run_optimizer()\n",
    "    pred = lin_reg(X)\n",
    "    loss = cost_mean_square(Y,pred)\n",
    "    print(f\"Steps : {i} , Loss : {loss.numpy()}, Weights {weight.numpy()}, bias : {bias.numpy()}  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "atlantic-immigration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1eed3488f60>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X,Y,\"bo\")\n",
    "plt.plot(X,np.array(weight * X +bias),\"r\",label=\"fitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-milan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-cattle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
